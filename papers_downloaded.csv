Title,ArXiv Link,Paper Date,Date Added
Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts,https://arxiv.org/abs/2406.12845,2024-06-18,2024-06-19
Synergizing Foundation Models and Federated Learning - A Survey,https://arxiv.org/abs/2406.12844,2024-06-18,2024-06-19
LayerMerge - Neural Network Depth Compression through Layer Pruning and Merging,https://arxiv.org/abs/2406.12837,2024-06-18,2024-06-19
LaMDA - Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation,https://arxiv.org/abs/2406.12832,2024-06-18,2024-06-19
What Are the Odds? Language Models Are Capable of Probabilistic Reasoning,https://arxiv.org/abs/2406.12830,2024-06-18,2024-06-19
Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?,https://arxiv.org/abs/2406.12809,2024-06-18,2024-06-19
ChatGLM - A Family of Large Language Models from GLM-130B to GLM-4 All Tools,https://arxiv.org/abs/2406.12793,2024-06-18,2024-06-19
Jailbreak Paradox - The Achilles' Heel of LLMs,https://arxiv.org/abs/2406.12702,2024-06-18,2024-06-19
Estimating Knowledge in Large Language Models Without Generating a Single Token,https://arxiv.org/abs/2406.12673,2024-06-18,2024-06-19
From Insights to Actions - The Impact of Interpretability and Analysis Research on NLP,https://arxiv.org/abs/2406.12618,2024-06-18,2024-06-19
What makes two models think alike?,https://arxiv.org/abs/2406.12620,2024-06-18,2024-06-19
Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling,https://arxiv.org/abs/2406.12585,2024-06-18,2024-06-19
P-Tailor - Customizing Personality Traits for Language Models via Mixture of Specialized LoRA Experts,https://arxiv.org/abs/2406.12548,2024-06-18,2024-06-19
Adaptive Token Biaser - Knowledge Editing via Biasing Key Entities,https://arxiv.org/abs/2406.12468,2024-06-18,2024-06-19
Abstraction-of-Thought Makes Language Models Better Reasoners,https://arxiv.org/abs/2406.12442,2024-06-18,2024-06-19
Translation Equivariant Transformer Neural Processes,https://arxiv.org/abs/2406.12409,2024-06-18,2024-06-19
Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction - Value Also Matters,https://arxiv.org/abs/2406.12335,2024-06-18,2024-06-19
Mixture of Scales - Memory-Efficient Token-Adaptive Binarization for Large Language Models,https://arxiv.org/abs/2406.12311,2024-06-18,2024-06-19
Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions,https://arxiv.org/abs/2406.12216,2024-06-18,2024-06-19
Time Series Modeling for Heart Rate Prediction - From ARIMA to Transformers,https://arxiv.org/abs/2406.12199,2024-06-18,2024-06-19
LLMs Are Prone to Fallacies in Causal Inference,https://arxiv.org/abs/2406.12158,2024-06-18,2024-06-19
Can LLMs Learn Macroeconomic Narratives from Social Media?,https://arxiv.org/abs/2406.12109,2024-06-18,2024-06-19
MEDeA - Multi-view Efficient Depth Adjustment,https://arxiv.org/abs/2406.12048,2024-06-18,2024-06-19
Iterative Length-Regularized Direct Preference Optimization - A Case Study on Improving 7B Language Models to GPT-4 Level,https://arxiv.org/abs/2406.11817,2024-06-18,2024-06-19
How Do Large Language Models Acquire Factual Knowledge During Pretraining?,https://arxiv.org/abs/2406.11813,2024-06-18,2024-06-19
Provable Guarantees for Model Performance via Mechanistic Interpretability,https://arxiv.org/abs/2406.11779,2024-06-18,2024-06-19
Transcendence - Generative Models Can Outperform The Experts That Train Them,https://arxiv.org/abs/2406.11741,2024-06-18,2024-06-19
A Clipped Trip - the Dynamics of SGD with Gradient Clipping in High-Dimensions,https://arxiv.org/abs/2406.11733,2024-06-18,2024-06-19
Meta Reasoning for Large Language Models,https://arxiv.org/abs/2406.11698,2024-06-18,2024-06-19
Tokenization Falling Short - The Curse of Tokenization,https://arxiv.org/abs/2406.11687,2024-06-18,2024-06-19
See It from My Perspective - Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding,https://arxiv.org/abs/2406.11665,2024-06-18,2024-06-19
Can LLM be a Personalized Judge?,https://arxiv.org/abs/2406.11657,2024-06-18,2024-06-19
"Understanding ""Democratization"" in NLP and ML Research",https://arxiv.org/abs/2406.11598,2024-06-18,2024-06-19
Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models,https://arxiv.org/abs/2406.11568,2024-06-18,2024-06-19
DeepSeek-Coder-V2 - Breaking the Barrier of Closed-Source Models in Code Intelligence,https://arxiv.org/abs/2406.11931,2024-06-18,2024-06-19
Do Parameters Reveal More than Loss for Membership Inference?,https://arxiv.org/abs/2406.11544,2024-06-18,2024-06-19
A Critical Study of What Code-LLMs (Do Not) Learn,https://arxiv.org/abs/2406.11930,2024-06-18,2024-06-19
"Promises, Outlooks and Challenges of Diffusion Language Modeling",https://arxiv.org/abs/2406.11473,2024-06-18,2024-06-19
"HARE - HumAn pRiors, a key to small language model Efficiency",https://arxiv.org/abs/2406.11410,2024-06-18,2024-06-19
CodeGemma - Open Code Models Based on Gemma,https://arxiv.org/abs/2406.11409,2024-06-18,2024-06-19
MetaGPT - Merging Large Language Models Using Model Exclusive Task Arithmetic,https://arxiv.org/abs/2406.11385,2024-06-18,2024-06-19
Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts,https://arxiv.org/abs/2406.11256,2024-06-18,2024-06-19
What Kinds of Tokens Benefit from Distant Text? An Analysis on Long Context Language Modeling,https://arxiv.org/abs/2406.11238,2024-06-18,2024-06-19
A Survey on Human Preference Learning for Large Language Models,https://arxiv.org/abs/2406.11191,2024-06-18,2024-06-19
A Peek into Token Bias - Large Language Models Are Not Yet Genuine Reasoners,https://arxiv.org/abs/2406.11050,2024-06-18,2024-06-19
Latent Communication in Artificial Neural Networks,https://arxiv.org/abs/2406.11014,2024-06-18,2024-06-19
Effective Generative AI - The Human-Algorithm Centaur,https://arxiv.org/abs/2406.10942,2024-06-18,2024-06-19
Understanding Understanding - A Pragmatic Framework Motivated by Large Language Models,https://arxiv.org/abs/2406.10937,2024-06-18,2024-06-19
Breaking the Attention Bottleneck,https://arxiv.org/abs/2406.10906,2024-06-18,2024-06-19
Evaluating LLMs with Multiple Problems at once - A New Paradigm for Probing LLM Capabilities,https://arxiv.org/abs/2406.10786,2024-06-18,2024-06-19
Multilingual Large Language Models and Curse of Multilinguality,https://arxiv.org/abs/2406.10602,2024-06-18,2024-06-19
Concentrate Attention - Towards Domain-Generalizable Prompt Optimization for Language Models,https://arxiv.org/abs/2406.10584,2024-06-18,2024-06-19
Explain the Black Box for the Sake of Science - Revisiting the Scientific Method in the Era of Generative Artificial Intelligence,https://arxiv.org/abs/2406.10557,2024-06-18,2024-06-19
A Theory of Interpretable Approximations,https://arxiv.org/abs/2406.10529,2024-06-18,2024-06-19
Personalized Pieces - Efficient Personalized Large Language Models through Collaborative Efforts,https://arxiv.org/abs/2406.10471,2024-06-18,2024-06-19
Byzantine-Robust Decentralized Federated Learning,https://arxiv.org/abs/2406.10416,2024-06-18,2024-06-19
"Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",https://arxiv.org/abs/2406.10209,2024-06-18,2024-06-19
LieRE - Generalizing Rotary Position Encodings,https://arxiv.org/abs/2406.10322,2024-06-18,2024-06-19
Towards Effective and Efficient Non-autoregressive Decoding Using Block-based Attention Mask,https://arxiv.org/abs/2406.10034,2024-06-18,2024-06-19
An elementary proof of a universal approximation theorem,https://arxiv.org/abs/2406.10002,2024-06-18,2024-06-19
Towards Scalable and Versatile Weight Space Learning,https://arxiv.org/abs/2406.09997,2024-06-18,2024-06-19
Neural Concept Binder,https://arxiv.org/abs/2406.09949,2024-06-18,2024-06-19
Forgetting Order of Continual Learning - Examples That are Learned First are Forgotten Last,https://arxiv.org/abs/2406.09935,2024-06-18,2024-06-19
GEB-1.3B - Open Lightweight Large Language Model,https://arxiv.org/abs/2406.09900,2024-06-18,2024-06-19
3D-RPE - Enhancing Long-Context Modeling Through 3D Rotary Position Encoding,https://arxiv.org/abs/2406.09897,2024-06-18,2024-06-19
Federated Learning driven Large Language Models for Swarm Intelligence - A Survey,https://arxiv.org/abs/2406.09831,2024-06-18,2024-06-19
When Will Gradient Regularization Be Harmful?,https://arxiv.org/abs/2406.09723,2024-06-18,2024-06-19
Meta-Learning Loss Functions for Deep Neural Networks,https://arxiv.org/abs/2406.09713,2024-06-18,2024-06-19
Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,https://arxiv.org/abs/2406.07394,2024-06-11,2024-06-19
Beyond Scaling Laws - Understanding Transformer Performance with Associative Memory,https://arxiv.org/abs/2405.08707,2024-05-14,2024-06-20
Mixture-of-Agents Enhances Large Language Model Capabilities,https://arxiv.org/abs/2406.04692,2024-06-07,2024-06-20
Whiteboard-of-Thought - Thinking Step-by-Step Across Modalities,https://arxiv.org/abs/2406.14562,2024-06-20,2024-06-21
Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases,https://arxiv.org/abs/2406.14462,2024-06-20,2024-06-21
The Impact of AI on Perceived Job Decency and Meaningfulness - A Case Study,https://arxiv.org/abs/2406.14273,2024-06-20,2024-06-21
In Tree Structure Should Sentence Be Generated,https://arxiv.org/abs/2406.14189,2024-06-20,2024-06-21
Ranking LLMs by compression,https://arxiv.org/abs/2406.14171,2024-06-20,2024-06-21
Understanding Different Design Choices in Training Large Time Series Models,https://arxiv.org/abs/2406.14045,2024-06-20,2024-06-21
Toward Infinite-Long Prefix in Transformer,https://arxiv.org/abs/2406.14036,2024-06-20,2024-06-21
Complex fractal trainability boundary can arise from trivial non-convexity,https://arxiv.org/abs/2406.13971,2024-06-20,2024-06-21
Evolving to be Your Soulmate - Personalized Dialogue Agents with Dynamically Adapted Personas,https://arxiv.org/abs/2406.13960,2024-06-20,2024-06-21
SPL - A Socratic Playground for Learning Powered by Large Language Mode,https://arxiv.org/abs/2406.13919,2024-06-20,2024-06-21
Distributional reasoning in LLMs - Parallel reasoning processes in multi-hop reasoning,https://arxiv.org/abs/2406.13858,2024-06-20,2024-06-21
A Primal-Dual Framework for Transformers and Neural Networks,https://arxiv.org/abs/2406.13781,2024-06-20,2024-06-21
Elliptical Attention,https://arxiv.org/abs/2406.13770,2024-06-20,2024-06-21
Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis,https://arxiv.org/abs/2406.13762,2024-06-20,2024-06-21
In-Context Former - Lightning-fast Compressing Context for Large Language Model,https://arxiv.org/abs/2406.13618,2024-06-20,2024-06-21
Understanding the RoPE Extensions of Long-Context LLMs - An Attention Perspective,https://arxiv.org/abs/2406.13282,2024-06-20,2024-06-21
AdaMoE - Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models,https://arxiv.org/abs/2406.13233,2024-06-20,2024-06-21
Locating and Extracting Relational Concepts in Large Language Models,https://arxiv.org/abs/2406.13184,2024-06-20,2024-06-21
Amphista - Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style,https://arxiv.org/abs/2406.13170,2024-06-20,2024-06-21
Large Language Models are Biased Because They Are Large Language Models,https://arxiv.org/abs/2406.13138,2024-06-20,2024-06-21
Understanding and Mitigating Tokenization Bias in Language Models,https://arxiv.org/abs/2406.16829,2024-06-24,2024-06-25
Lottery Ticket Adaptation - Mitigating Destructive Interference in LLMs,https://arxiv.org/abs/2406.16797,2024-06-24,2024-06-25
Adam-mini - Use Fewer Learning Rates To Gain More,https://arxiv.org/abs/2406.16793,2024-06-24,2024-06-25
Finding Transformer Circuits with Edge Pruning,https://arxiv.org/abs/2406.16778,2024-06-24,2024-06-25
The Responsible Foundation Model Development Cheatsheet - A Review of Tools & Resources,https://arxiv.org/abs/2406.16746,2024-06-24,2024-06-25
Scaling Laws for Linear Complexity Language Models,https://arxiv.org/abs/2406.16690,2024-06-24,2024-06-25
Forecasting with Deep Learning - Beyond Average of Average of Average Performance,https://arxiv.org/abs/2406.16590,2024-06-24,2024-06-25
LLaMA-MoE - Building Mixture-of-Experts from LLaMA with Continual Pre-training,https://arxiv.org/abs/2406.16554,2024-06-24,2024-06-25
Large Vocabulary Size Improves Large Language Models,https://arxiv.org/abs/2406.16508,2024-06-24,2024-06-25
OTCE - Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser,https://arxiv.org/abs/2406.16495,2024-06-24,2024-06-25
The Hidden Pitfalls of the Cosine Similarity Loss,https://arxiv.org/abs/2406.16468,2024-06-24,2024-06-25
Building on Efficient Foundations - Effectively Training LLMs with Structured Feedforward Layers,https://arxiv.org/abs/2406.16450,2024-06-24,2024-06-25
Theory on Mixture-of-Experts in Continual Learning,https://arxiv.org/abs/2406.16437,2024-06-24,2024-06-25
Pruning via Merging - Compressing LLMs via Manifold Alignment Based Layer Merging,https://arxiv.org/abs/2406.16330,2024-06-24,2024-06-25
What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation,https://arxiv.org/abs/2406.16320,2024-06-24,2024-06-25
Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?,https://arxiv.org/abs/2406.16316,2024-06-24,2024-06-25
"One Thousand and One Pairs - A ""novel"" challenge for long-context language models",https://arxiv.org/abs/2406.16264,2024-06-24,2024-06-25
Video-Infinity - Distributed Long Video Generation,https://arxiv.org/abs/2406.16260,2024-06-24,2024-06-25
LLMs' Classification Performance is Overclaimed,https://arxiv.org/abs/2406.16203,2024-06-24,2024-06-25
FastMem - Fast Memorization of Prompt Improves Context Awareness of Large Language Models,https://arxiv.org/abs/2406.16069,2024-06-24,2024-06-25
Combine and Conquer - A Meta-Analysis on Data Shift and Out-of-Distribution Detection,https://arxiv.org/abs/2406.16045,2024-06-24,2024-06-25
Unlocking the Future - Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models,https://arxiv.org/abs/2406.16033,2024-06-24,2024-06-25
Effect of Random Learning Rate - Theoretical Analysis of SGD Dynamics in Non-Convex Optimization via Stationary Distribution,https://arxiv.org/abs/2406.16032,2024-06-24,2024-06-25
Distributed Rule Vectors is A Key Mechanism in Large Language Models' In-Context Learning,https://arxiv.org/abs/2406.16007,2024-06-24,2024-06-25
SimSMoE - Solving Representational Collapse via Similarity Measure,https://arxiv.org/abs/2406.15883,2024-06-24,2024-06-25
What Matters in Transformers? Not All Attention is Needed,https://arxiv.org/abs/2406.15786,2024-06-24,2024-06-25
Unveiling and Harnessing Hidden Attention Sinks - Enhancing Large Language Models without Training through Attention Calibration,https://arxiv.org/abs/2406.15765,2024-06-24,2024-06-25
Scaling Laws for Fact Memorization of Large Language Models,https://arxiv.org/abs/2406.15720,2024-06-24,2024-06-25
Large Language Models have Intrinsic Self-Correction Ability,https://arxiv.org/abs/2406.15673,2024-06-24,2024-06-25
Hybrid Alignment Training for Large Language Models,https://arxiv.org/abs/2406.15178,2024-06-24,2024-06-25
Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network,https://arxiv.org/abs/2406.15109,2024-06-24,2024-06-25
HLQ - Fast and Efficient Backpropagation via Hadamard Low-rank Quantization,https://arxiv.org/abs/2406.15102,2024-06-24,2024-06-25
Optimised Grouped-Query Attention Mechanism for Transformers,https://arxiv.org/abs/2406.14963,2024-06-24,2024-06-25
MoA - Mixture of Sparse Attention for Automatic Large Language Model Compression,https://arxiv.org/abs/2406.14909,2024-06-24,2024-06-25
Do LLMs Have Distinct and Consistent Personality? TRAIT - Personality Testset designed for LLMs with Psychometrics,https://arxiv.org/abs/2406.14703,2024-06-24,2024-06-25
Can LLMs Learn by Teaching? A Preliminary Study,https://arxiv.org/abs/2406.14629,2024-06-24,2024-06-25
DataComp-LM - In search of the next generation of training sets for language models,https://arxiv.org/abs/2406.11794,2024-06-17,2024-06-25

Interpreting Attention Layer Outputs with Sparse Autoencoders,https://arxiv.org/abs/2406.17759,2024-06-25,2024-06-27
"Recite, Reconstruct, Recollect - Memorization in LMs as a Multifaceted Phenomenon",https://arxiv.org/abs/2406.17746,2024-06-25,2024-06-27
Grass - Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients,https://arxiv.org/abs/2406.17660,2024-06-25,2024-06-27
Banishing LLM Hallucinations Requires Rethinking Generalization,https://arxiv.org/abs/2406.17642,2024-06-25,2024-06-27
Benchmarking Mental State Representations in Language Models,https://arxiv.org/abs/2406.17513,2024-06-25,2024-06-27
The Tree of Diffusion Life - Evolutionary Embeddings to Understand the Generation Process of Diffusion Models,https://arxiv.org/abs/2406.17462,2024-06-25,2024-06-27
A Text is Worth Several Tokens - Text Embedding from LLMs Secretly Aligns Well with The Key Tokens,https://arxiv.org/abs/2406.17378,2024-06-25,2024-06-27
Native Design Bias - Studying the Impact of English Nativeness on Language Model Performance,https://arxiv.org/abs/2406.17385,2024-06-25,2024-06-27
Peirce in the Machine - How Mixture of Experts Models Perform Hypothesis Construction,https://arxiv.org/abs/2406.17150,2024-06-25,2024-06-27
Large Language Models Assume People are More Rational than We Really are,https://arxiv.org/abs/2406.17055,2024-06-25,2024-06-27
Mental Modeling of Reinforcement Learning Agents by Language Models,https://arxiv.org/abs/2406.18505,2024-06-26,2024-06-27
Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers,https://arxiv.org/abs/2406.18400,2024-06-26,2024-06-27
Adversarial Search Engine Optimization for Large Language Models,https://arxiv.org/abs/2406.18382,2024-06-26,2024-06-27
A Closer Look into Mixture-of-Experts in Large Language Models,https://arxiv.org/abs/2406.18219,2024-06-26,2024-06-27
MammothModa - Multi-Modal Large Language Model,https://arxiv.org/abs/2406.18193,2024-06-26,2024-06-27
Emergence of social hierarchies in a society with two competitive classes,https://arxiv.org/abs/2406.18168,2024-06-26,2024-06-27
ResumeAtlas - Revisiting Resume Classification with Large-Scale Datasets and Large Language Models,https://arxiv.org/abs/2406.18125,2024-06-26,2024-06-27
Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints,https://arxiv.org/abs/2406.18085,2024-06-26,2024-06-27
Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective,https://arxiv.org/abs/2406.17969,2024-06-26,2024-06-27
Why Line Search when you can Plane Search? SO-Friendly Neural Networks allow Per-Iteration Optimization of Learning and Momentum Rates for Every Layer,https://arxiv.org/abs/2406.17954,2024-06-26,2024-06-27
Emergence of Hidden Capabilities - Exploring Learning Dynamics in Concept Space,https://arxiv.org/abs/2406.19370,2024-06-27,2024-06-28
Efficient World Models with Context-Aware Tokenization,https://arxiv.org/abs/2406.19320,2024-06-27,2024-06-28
Commodification of Compute,https://arxiv.org/abs/2406.19261,2024-06-27,2024-06-28
Revealing Fine-Grained Values and Opinions in Large Language Models,https://arxiv.org/abs/2406.19238,2024-06-27,2024-06-28
T-FREE - Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings,https://arxiv.org/abs/2406.19223,2024-06-27,2024-06-28
Resolving Discrepancies in Compute-Optimal Scaling of Language Models,https://arxiv.org/abs/2406.19146,2024-06-27,2024-06-28
Adaptive Stochastic Weight Averaging,https://arxiv.org/abs/2406.19092,2024-06-27,2024-06-28
Dimensions underlying the representational alignment of deep neural networks with humans,https://arxiv.org/abs/2406.19087,2024-06-27,2024-06-28
The Rise of Artificial Intelligence in Educational Measurement - Opportunities and Ethical Challenges,https://arxiv.org/abs/2406.18900,2024-06-27,2024-06-28
All Random Features Representations are Equivalent,https://arxiv.org/abs/2406.18802,2024-06-27,2024-06-28
Infinite Width Models That Work - Why Feature Learning Doesn't Matter as Much as You Think,https://arxiv.org/abs/2406.18800,2024-06-27,2024-06-28
"Scaling Synthetic Data Creation with 1,000,000,000 Personas",https://arxiv.org/abs/2406.20094,2024-06-28,2024-07-02
Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs,https://arxiv.org/abs/2406.20086,2024-06-28,2024-07-02
LEMoE - Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models,https://arxiv.org/abs/2406.20030,2024-06-28,2024-07-02
Single Parent Family - A Spectrum of Family Members from a Single Pre-Trained Foundation Model,https://arxiv.org/abs/2406.19995,2024-06-28,2024-07-02
Unlocking Varied Perspectives - A Persona-Based Multi-Agent Framework with Debate-Driven Text Planning for Argument Generation,https://arxiv.org/abs/2406.19643,2024-06-28,2024-07-02
Mixture of In-Context Experts Enhance LLMs' Long Context Awareness,https://arxiv.org/abs/2406.19598,2024-06-28,2024-07-02
MInference 1.0 - Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention,https://arxiv.org/abs/2407.02490,2024-07-02,2024-07-03
Neurocache - Efficient Vector Retrieval for Long-range Language Modeling,https://arxiv.org/abs/2407.02486,2024-07-02,2024-07-03
Decentralized Intelligence Network (DIN),https://arxiv.org/abs/2407.02461,2024-07-02,2024-07-03
Predicting vs. Acting - A Trade-off Between World Modeling & Agent Modeling,https://arxiv.org/abs/2407.02446,2024-07-02,2024-07-03
On the Anatomy of Attention,https://arxiv.org/abs/2407.02423,2024-07-02,2024-07-03
Two-Step Q-Learning,https://arxiv.org/abs/2407.02369,2024-07-02,2024-07-03
"Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA",https://arxiv.org/abs/2407.02362,2024-07-02,2024-07-03
MORPHEUS - Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space,https://arxiv.org/abs/2407.02345,2024-07-02,2024-07-03
Efficient Sparse Attention needs Adaptive Token Release,https://arxiv.org/abs/2407.02328,2024-07-02,2024-07-03
Generative Monoculture in Large Language Models,https://arxiv.org/abs/2407.02209,2024-07-02,2024-07-03
Black Big Boxes - Do Language Models Hide a Theory of Adjective Order?,https://arxiv.org/abs/2407.02136,2024-07-02,2024-07-03
DiscoveryBench - Towards Data-Driven Discovery with Large Language Models,https://arxiv.org/abs/2407.01725,2024-07-02,2024-07-03
On Implications of Scaling Laws on Feature Superposition,https://arxiv.org/abs/2407.01459,2024-07-02,2024-07-03
Badllama 3 - removing safety finetuning from Llama 3 in minutes,https://arxiv.org/abs/2407.01376,2024-07-02,2024-07-03
Hypformer - Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space,https://arxiv.org/abs/2407.01290,2024-07-02,2024-07-03
Energy-Aware Decentralized Learning with Intermittent Model Training,https://arxiv.org/abs/2407.01283,2024-07-02,2024-07-03
Eliminating Position Bias of Language Models - A Mechanistic Approach,https://arxiv.org/abs/2407.01100,2024-07-02,2024-07-03
Min P Sampling - Balancing Creativity and Coherence at High Temperature,https://arxiv.org/abs/2407.01082,2024-07-02,2024-07-03
Swish-T  - Enhancing Swish Activation with Tanh Bias for Improved Neural Network Performance,https://arxiv.org/abs/2407.01012,2024-07-02,2024-07-03
DynaThink - Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models,https://arxiv.org/abs/2407.01009,2024-07-02,2024-07-03
"Engineering Conversational Search Systems - A Review of Applications, Architectures, and Functional Components",https://arxiv.org/abs/2407.00997,2024-07-02,2024-07-03
LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation,https://arxiv.org/abs/2407.00994,2024-07-02,2024-07-03
How Does Overparameterization Affect Features?,https://arxiv.org/abs/2407.00968,2024-07-02,2024-07-03
Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension,https://arxiv.org/abs/2407.00966,2024-07-02,2024-07-03
Universal Approximation Theory - The basic theory for large language models,https://arxiv.org/abs/2407.00958,2024-07-02,2024-07-03
Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models - Enhancing Performance and Reducing Inference Costs,https://arxiv.org/abs/2407.00945,2024-07-02,2024-07-03
Large Language Model Enhanced Knowledge Representation Learning - A Survey,https://arxiv.org/abs/2407.00936,2024-07-02,2024-07-03
Macroeconomic Forecasting with Large Language Models,https://arxiv.org/abs/2407.00890,2024-07-02,2024-07-03
Diffusion Models and Representation Learning - A Survey,https://arxiv.org/abs/2407.00783,2024-07-02,2024-07-03
It's Morphing Time - Unleashing the Potential of Multiple LLMs via Multi-objective Optimization,https://arxiv.org/abs/2407.00487,2024-07-02,2024-07-03
LLM-Generated Natural Language Meets Scaling Laws - New Explorations and Data Augmentation Methods,https://arxiv.org/abs/2407.00322,2024-07-02,2024-07-03
A Review of the Applications of Deep Learning-Based Emergent Communication,https://arxiv.org/abs/2407.03302,2024-07-03,2024-07-04
LLM Internal States Reveal Hallucination Risk Faced With a Query,https://arxiv.org/abs/2407.03282,2024-07-03,2024-07-04
Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing,https://arxiv.org/abs/2407.03185,2024-07-03,2024-07-04
Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models,https://arxiv.org/abs/2407.03181,2024-07-03,2024-07-04
A multi-objective combinatorial optimisation framework for large scale hierarchical population synthesis,https://arxiv.org/abs/2407.03180,2024-07-03,2024-07-04
Raw Text is All you Need - Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model,https://arxiv.org/abs/2407.03040,2024-07-03,2024-07-04
Large Language Models as Evaluators for Scientific Synthesis,https://arxiv.org/abs/2407.02977,2024-07-03,2024-07-04
Knowledge Composition using Task Vectors with Learned Anisotropic Scaling,https://arxiv.org/abs/2407.02880,2024-07-03,2024-07-04
Efficient Training of Language Models with Compact and Consistent Next Token Distributions,https://arxiv.org/abs/2407.02819,2024-07-03,2024-07-04
Learning to Reduce - Towards Improving Performance of Large Language Models on Structured Data,https://arxiv.org/abs/2407.02750,2024-07-03,2024-07-04
Universal Length Generalization with Turing Programs,https://arxiv.org/abs/2407.03310v1,2024-07-03,2024-07-04
Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension,https://arxiv.org/abs/2407.00966,2024-07-01,2024-07-04
Do language models plan ahead for future tokens -,https://arxiv.org/abs/2404.00859,2024-04-01,2024-07-04
Do language models plan ahead for future tokens -,https://arxiv.org/abs/2404.00859,2024-04-01,2024-07-04
Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging - A Comprehensive Evaluation,https://arxiv.org/abs/2406.14971,2024-06-21,2024-07-04
SAIL - Self-Improving Efficient Online Alignment of Large Language Models,https://arxiv.org/abs/2406.15567,2024-06-21,2024-07-04
Large Vocabulary Size Improves Large Language Models,https://arxiv.org/abs/2406.16508,2024-06-24,2024-07-04
Adam-mini - Use Fewer Learning Rates To Gain More,https://arxiv.org/abs/2406.16793,2024-06-24,2024-07-04
Free Energy in a Circumplex Model of Emotion,https://arxiv.org/abs/2407.02474v1,2024-07-02,2024-07-04
Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models -,https://arxiv.org/abs/2406.16316v1,2024-06-24,2024-07-04
Generalizability of experimental studies,https://arxiv.org/abs/2406.17374v1,2024-06-25,2024-07-04
Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions,https://arxiv.org/abs/2406.14701v1,2024-06-20,2024-07-04
On the Anatomy of Attention,https://arxiv.org/abs/2407.02423,2024-07-02,2024-07-04
Predicting vs. Acting - A Trade-off Between World Modeling & Agent Modeling,https://arxiv.org/abs/2407.02446,2024-07-02,2024-07-04
Large Language Models Assume People are More Rational than We Really are,https://arxiv.org/abs/2406.17055v2,2024-06-24,2024-07-04
CompactifAI - Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks,https://arxiv.org/abs/2401.14109,2024-01-25,2024-07-09
Data curation via joint example selection further accelerates multimodal learning,https://arxiv.org/abs/2406.17711v1,2024-06-25,2024-07-09
The Mathematics of Text Structure,https://arxiv.org/abs/1904.03478,2019-04-06,2024-07-10
Internet of Agents - Weaving a Web of Heterogeneous Agents for Collaborative Intelligence,https://arxiv.org/abs/2407.07061,2024-07-09,2024-07-10
Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning,https://arxiv.org/abs/2407.07011,2024-07-09,2024-07-10
Self-Recognition in Language Models,https://arxiv.org/abs/2407.06946,2024-07-09,2024-07-10
Aligning Cyber Space with Physical World - A Comprehensive Survey on Embodied AI,https://arxiv.org/abs/2407.06886,2024-07-09,2024-07-10
ChatGPT Doesn't Trust Chargers Fans - Guardrail Sensitivity in Context,https://arxiv.org/abs/2407.06866,2024-07-09,2024-07-10
SoftDedup - an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training,https://arxiv.org/abs/2407.06654,2024-07-09,2024-07-10
Virtual Personas for Language Models via an Anthology of Backstories,https://arxiv.org/abs/2407.06576,2024-07-09,2024-07-10
Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons,https://arxiv.org/abs/2407.06488,2024-07-09,2024-07-10
Optimal Decision Making Through Scenario Simulations Using Large Language Models,https://arxiv.org/abs/2407.06486,2024-07-09,2024-07-10
A Single Transformer for Scalable Vision-Language Modeling,https://arxiv.org/abs/2407.06438,2024-07-09,2024-07-10
How DNNs break the Curse of Dimensionality - Compositionality and Symmetry Learning,https://arxiv.org/abs/2407.05664,2024-07-09,2024-07-10
Multi-label Learning with Random Circular Vectors,https://arxiv.org/abs/2407.05656,2024-07-09,2024-07-10
LLMBox - A Comprehensive Library for Large Language Models,https://arxiv.org/abs/2407.05563,2024-07-09,2024-07-10
Can Machines Learn the True Probabilities?,https://arxiv.org/abs/2407.05526,2024-07-09,2024-07-10
A Theory of Machine Learning,https://arxiv.org/abs/2407.05520,2024-07-09,2024-07-10
Collective Innovation in Groups of Large Language Models,https://arxiv.org/abs/2407.05377,2024-07-09,2024-07-10
The US Algorithmic Accountability Act of 2022 vs. The EU Artificial Intelligence Act - What can they learn from each other?,https://arxiv.org/abs/2407.06234,2024-07-09,2024-07-10
AI and Social Theory,https://arxiv.org/abs/2407.06233,2024-07-09,2024-07-10
"Artificial intelligence, rationalization, and the limits of control in the public sector - the case of tax policy optimization",https://arxiv.org/abs/2407.05336,2024-07-09,2024-07-10
Associative Recurrent Memory Transformer,https://arxiv.org/abs/2407.04841,2024-07-09,2024-07-10
Lazarus - Resilient and Elastic Training of Mixture-of-Experts Models with Adaptive Expert Placement,https://arxiv.org/abs/2407.04656,2024-07-09,2024-07-10
Introducing 'Inside' Out of Distribution,https://arxiv.org/abs/2407.04534,2024-07-09,2024-07-10
Enhancing learning in artificial neural networks through cellular heterogeneity and neuromodulatory signaling,https://arxiv.org/abs/2407.04525,2024-07-09,2024-07-10
When LLMs Play the Telephone Game - Cumulative Changes and Attractors in Iterated Cultural Transmissions,https://arxiv.org/abs/2407.04503,2024-07-09,2024-07-10
Mixture of A Million Experts,https://arxiv.org/abs/2407.04153,2024-07-09,2024-07-10
Predictive Coding Networks and Inference Learning - Tutorial and Survey,https://arxiv.org/abs/2407.04117,2024-07-09,2024-07-10
Stephanie - Step-by-Step Dialogues for Mimicking Human Interactions in Social Conversations,https://arxiv.org/abs/2407.04093,2024-07-09,2024-07-10
Anthropocentric bias and the possibility of artificial cognition,https://arxiv.org/abs/2407.03859,2024-07-09,2024-07-10
A Survey of Controllable Learning - Methods and Applications in Information Retrieval,https://arxiv.org/abs/2407.06083,2024-07-09,2024-07-10
Improving Self Consistency in LLMs through Probabilistic Tokenization,https://arxiv.org/abs/2407.03678,2024-07-09,2024-07-10
Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial General Intelligence,https://arxiv.org/abs/2407.03652,2024-07-09,2024-07-10
The Mysterious Case of Neuron 1512 - Injectable Realignment Architectures Reveal Internal Characteristics of Meta's Llama 2 Model,https://arxiv.org/abs/2407.03621,2024-07-09,2024-07-10
MobileLLM - Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,https://arxiv.org/pdf/2402.14905,2024-02-22,2024-07-11
A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training,https://arxiv.org/abs/2407.07630,2024-07-10,2024-07-11
Probabilistic learning rate scheduler with provable convergence,https://arxiv.org/abs/2407.07613,2024-07-10,2024-07-11
Beyond Benchmarking - A New Paradigm for Evaluation and Assessment of Large Language Models,https://arxiv.org/abs/2407.07531,2024-07-10,2024-07-11
Bucket Pre-training is All You Need,https://arxiv.org/abs/2407.07495,2024-07-10,2024-07-11
LokiLM - Technical Report,https://arxiv.org/abs/2407.07370,2024-07-10,2024-07-11
Towards a theory of learning dynamics in deep state space models,https://arxiv.org/abs/2407.07279,2024-07-10,2024-07-11
How to Boost Any Loss Function,https://arxiv.org/abs/2407.02279,2024-07-02,2024-07-11
Surpassing Cosine Similarity for Multidimensional Comparisons - Dimension Insensitive Euclidean Metric (DIEM),https://arxiv.org/abs/2407.08623,2024-07-11,2024-07-12
FlashAttention-3 - Fast and Accurate Attention with Asynchrony and Low-precision,https://arxiv.org/abs/2407.08608,2024-07-11,2024-07-12
Parallelizing Autoregressive Generation with Variational State Space Models,https://arxiv.org/abs/2407.08415,2024-07-11,2024-07-12
United We Stand - Decentralized Multi-Agent Planning With Attrition,https://arxiv.org/abs/2407.08254,2024-07-11,2024-07-12
SwishReLU - A Unified Approach to Activation Functions for Enhanced Deep Neural Networks Performance,https://arxiv.org/abs/2407.08232,2024-07-11,2024-07-12
"Position - Measure Dataset Diversity, Don't Just Claim It",https://arxiv.org/abs/2407.08188,2024-07-11,2024-07-12
Learning to (Learn at Test Time) - RNNs with Expressive Hidden States,https://arxiv.org/abs/2407.04620,2024-07-05,2024-07-12
On the Universal Truthfulness Hyperplane Inside LLMs,https://arxiv.org/abs/2407.08582v1,2024-07-11,2024-07-12
FlashAttention-3 - Fast and Accurate Attention with Asynchrony and Low-precision,https://arxiv.org/abs/2407.08608v1,2024-07-11,2024-07-12
Real-Time Anomaly Detection and Reactive Planning with Large Language Models,https://arxiv.org/abs/2407.08735v1,2024-07-11,2024-07-12
Large language models can accurately predict searcher preferences,https://arxiv.org/abs/2309.10621,2023-09-19,2024-07-12
InternLM-XComposer-2.5 - A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output,https://arxiv.org/abs/2407.03320,2024-07-03,2024-07-12
Learning to (Learn at Test Time) - RNNs with Expressive Hidden States,https://arxiv.org/abs/2407.04620v1,2024-07-05,2024-07-12
The Synergy between Data and Multi-Modal Large Language Models - A Survey from Co-Development Perspective,https://arxiv.org/abs/2407.08583,2024-07-11,2024-07-12
SOWA - Adapting Hierarchical Frozen Window Self-Attention to Visual-Language Models for Better Anomaly Detection,https://arxiv.org/abs/2407.03634v1,2024-07-04,2024-07-12
Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks,https://arxiv.org/abs/2407.06146v2,2024-07-08,2024-07-12
Evaluating LLMs' Inherent Multi-hop Reasoning Ability,https://arxiv.org/abs/2402.11924v4,2024-02-19,2024-07-12
Human-like Episodic Memory for Infinite Context LLMs,https://arxiv.org/abs/2407.09450,2024-07-12,2024-07-15
Future Lens - Anticipating Subsequent Tokens from a Single Hidden State,https://arxiv.org/abs/2311.04897,2023-11-08,2024-07-16
Topology of deep neural networks,https://arxiv.org/abs/2004.06093,2020-04-13,2024-07-16
DataComp-LM - In search of the next generation of training sets for language models,https://arxiv.org/abs/2406.11794,2024-06-17,2024-07-17
ReFT - Representation Finetuning for Language Models,https://arxiv.org/pdf/2404.03592,2024-04-04,2024-07-17
Latent Causal Probing - A Formal Perspective on Probing with Causal Models of Data,https://arxiv.org/abs/2407.13765,2024-07-18,2024-07-19
Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models - A Tutorial and Review,https://arxiv.org/abs/2407.13734,2024-07-18,2024-07-19
Baba Is AI - Break the Rules to Beat the Benchmark,https://arxiv.org/abs/2407.13729,2024-07-18,2024-07-19
Compressing Structured Tensor Algebra,https://arxiv.org/abs/2407.13726,2024-07-18,2024-07-19
A Comprehensive Review of Recommender Systems - Transitioning from Theory to Practice,https://arxiv.org/abs/2407.13699,2024-07-18,2024-07-19
Attention Overflow - Language Model Input Blur during Long-Context Missing Items Recommendation,https://arxiv.org/abs/2407.13481,2024-07-18,2024-07-19
Combining Constraint Programming Reasoning with Large Language Model Predictions,https://arxiv.org/abs/2407.13490,2024-07-18,2024-07-19
The Art of Imitation - Learning Long-Horizon Manipulation Tasks from Few Demonstrations,https://arxiv.org/abs/2407.13432,2024-07-18,2024-07-19
Deep Time Series Models - A Comprehensive Survey and Benchmark,https://arxiv.org/abs/2407.13278,2024-07-18,2024-07-19
Transformer-based Single-Cell Language Model - A Survey,https://arxiv.org/abs/2407.13205,2024-07-18,2024-07-19
Compressed models are NOT miniature versions of large models,https://arxiv.org/abs/2407.13174,2024-07-18,2024-07-19
Establishing Knowledge Preference in Language Models,https://arxiv.org/abs/2407.13048,2024-07-18,2024-07-19
Proof-of-Collaborative-Learning - A Multi-winner Federated Learning Consensus Algorithm,https://arxiv.org/abs/2407.13018,2024-07-18,2024-07-19
DreamStory - Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion,https://arxiv.org/abs/2407.12899,2024-07-18,2024-07-19
E5-V - Universal Embeddings with Multimodal Large Language Models,https://arxiv.org/abs/2407.12580,2024-07-18,2024-07-19
A Survey on Universal Approximation Theorems,https://arxiv.org/abs/2407.12895,2024-07-18,2024-07-19
Struct-X - Enhancing Large Language Models Reasoning with Structured Data,https://arxiv.org/abs/2407.12522,2024-07-18,2024-07-19
The Better Angels of Machine Personality - How Personality Relates to LLM Safety,https://arxiv.org/abs/2407.12344,2024-07-18,2024-07-19
Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition,https://arxiv.org/abs/2407.12332,2024-07-18,2024-07-19
Information-Theoretic Foundations for Machine Learning,https://arxiv.org/abs/2407.12288,2024-07-18,2024-07-19
"Interpretability in Action - Exploratory Analysis of VPT, a Minecraft Agent",https://arxiv.org/abs/2407.12161,2024-07-18,2024-07-19
Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs,https://arxiv.org/abs/2407.12117,2024-07-18,2024-07-19
Tiled Bit Networks - Sub-Bit Neural Network Compression Through Reuse of Learnable Binary Vectors,https://arxiv.org/abs/2407.12075,2024-07-18,2024-07-19
Vectoring Languages,https://arxiv.org/abs/2407.11766,2024-07-18,2024-07-20
Bringing AI Participation Down to Scale - A Comment on Open AIs Democratic Inputs to AI Project,https://arxiv.org/abs/2407.11613,2024-07-18,2024-07-20
The Foundations of Tokenization - Statistical and Computational Concerns,https://arxiv.org/abs/2407.11606,2024-07-18,2024-07-20
Do LLMs have Consistent Values?,https://arxiv.org/abs/2407.12878,2024-07-18,2024-07-20
Navigating the swarm - Deep neural networks command emergent behaviours,https://arxiv.org/abs/2407.11330,2024-07-18,2024-07-20
From GaLore to WeLore - How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients,https://arxiv.org/abs/2407.11239,2024-07-18,2024-07-20
"Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique",https://arxiv.org/abs/2407.10887,2024-07-18,2024-07-20
Weighted Grouped Query Attention in Transformers,https://arxiv.org/abs/2407.10855,2024-07-18,2024-07-20
MetaLLM - A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs,https://arxiv.org/abs/2407.10834,2024-07-18,2024-07-20
BiasScanner - Automatic Detection and Classification of News Bias to Strengthen Democracy,https://arxiv.org/abs/2407.10829,2024-07-18,2024-07-20
Correlations Are Ruining Your Gradient Descent,https://arxiv.org/abs/2407.10780,2024-07-18,2024-07-20
What distinguishes conspiracy from critical narratives? A computational analysis of oppositional discourse,https://arxiv.org/abs/2407.10745,2024-07-18,2024-07-21
Practical Unlearning for Large Language Models,https://arxiv.org/abs/2407.10223,2024-07-18,2024-07-21
Curriculum Learning for Small Code Language Models,https://arxiv.org/abs/2407.10194,2024-07-18,2024-07-21
MaskMoE - Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts,https://arxiv.org/abs/2407.09816,2024-07-18,2024-07-21
Beyond KV Caching - Shared Attention for Efficient LLMs,https://arxiv.org/abs/2407.12866,2024-07-18,2024-07-21
Graph Transformers - A Survey,https://arxiv.org/abs/2407.09777,2024-07-18,2024-07-21
A Comprehensive Survey on Kolmogorov Arnold Networks (KAN),https://arxiv.org/abs/2407.11075,2024-07-18,2024-07-21
Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference,https://arxiv.org/abs/2407.09722,2024-07-18,2024-07-21
GRAD-SUM - Leveraging Gradient Summarization for Optimal Prompt Engineering,https://arxiv.org/abs/2407.12865,2024-07-18,2024-07-21
MUSCLE - A Model Update Strategy for Compatible LLM Evolution,https://arxiv.org/abs/2407.09435,2024-07-18,2024-07-21
Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text,https://arxiv.org/abs/2407.09364,2024-07-18,2024-07-21
A Chatbot for Asylum-Seeking Migrants in Europe,https://arxiv.org/abs/2407.09197,2024-07-18,2024-07-21
Decentralized multi-agent reinforcement learning algorithm using a cluster-synchronized laser network,https://arxiv.org/abs/2407.09124,2024-07-18,2024-07-21
New Desiderata for Direct Preference Optimization,https://arxiv.org/abs/2407.09072,2024-07-18,2024-07-21
SpreadsheetLLM - Encoding Spreadsheets for Large Language Models,https://arxiv.org/abs/2407.09025,2024-07-18,2024-07-21
Benchmarking Language Model Creativity - A Case Study on Code Generation,https://arxiv.org/abs/2407.09007,2024-07-18,2024-07-21
Self-Evolving GPT - A Lifelong Autonomous Experiential Learner,https://arxiv.org/abs/2407.08937,2024-07-18,2024-07-21
Flash normalization - fast RMSNorm for LLMs,https://arxiv.org/abs/2407.09577,2024-07-18,2024-07-21
KAN or MLP - A Fairer Comparison,https://arxiv.org/abs/2407.16674,2024-07-23,2024-07-25
Data Mixture Inference - What do BPE Tokenizers Reveal about their Training Data?,https://arxiv.org/abs/2407.16607,2024-07-23,2024-07-25
Shared Imagination - LLMs Hallucinate Alike,https://arxiv.org/abs/2407.16604,2024-07-23,2024-07-25
On The Expressive Power of Knowledge Graph Embedding Methods,https://arxiv.org/abs/2407.16326,2024-07-23,2024-07-25
"A Comprehensive Survey of LLM Alignment Techniques - RLHF, RLAIF, PPO, DPO and More",https://arxiv.org/abs/2407.16216,2024-07-23,2024-07-25
Graph-Structured Speculative Decoding,https://arxiv.org/abs/2407.16207,2024-07-23,2024-07-25
On the Benefits of Rank in Attention Layers,https://arxiv.org/abs/2407.16153,2024-07-23,2024-07-25
Explaining Decisions in ML Models - a Parameterized Complexity Analysis,https://arxiv.org/abs/2407.15780,2024-07-23,2024-07-26
Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability,https://arxiv.org/abs/2407.15720,2024-07-23,2024-07-26
Supporting the Digital Autonomy of Elders Through LLM Assistance,https://arxiv.org/abs/2407.15695,2024-07-23,2024-07-26
Psychometric Alignment - Capturing Human Knowledge Distributions via Language Models,https://arxiv.org/abs/2407.15645,2024-07-23,2024-07-26
Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models,https://arxiv.org/abs/2407.15516,2024-07-23,2024-07-26
Knowledge Mechanisms in Large Language Models - A Survey and Perspective,https://arxiv.org/abs/2407.15017,2024-07-23,2024-07-26
Dissecting Multiplication in Transformers - Insights into LLMs,https://arxiv.org/abs/2407.15360,2024-07-23,2024-07-26
Deep Learning for Economists,https://arxiv.org/abs/2407.15339,2024-07-23,2024-07-26
RazorAttention - Efficient KV Cache Compression Through Retrieval Heads,https://arxiv.org/abs/2407.15891,2024-07-23,2024-07-26
The Hitchhiker's Guide to Human Alignment with *PO,https://arxiv.org/abs/2407.15229,2024-07-23,2024-07-26
Efficient Visual Transformer by Learnable Token Merging,https://arxiv.org/abs/2407.15219,2024-07-23,2024-07-26
"Farewell to Length Extrapolation, a Training-Free Infinite Context with Finite Attention Scope",https://arxiv.org/abs/2407.15176,2024-07-23,2024-07-26
Generalization v.s. Memorization - Tracing Language Models' Capabilities Back to Pretraining Data,https://arxiv.org/abs/2407.14985,2024-07-23,2024-07-26
Open Artificial Knowledge,https://arxiv.org/abs/2407.14371,2024-07-23,2024-07-26
LazyLLM - Dynamic Token Pruning for Efficient Long Context LLM Inference,https://arxiv.org/abs/2407.14057,2024-07-23,2024-07-26
Operating System And Artificial Intelligence - A Systematic Review,https://arxiv.org/abs/2407.14567,2024-07-23,2024-07-26
Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models,https://arxiv.org/abs/2407.18158,2024-07-25,2024-07-26
Learning mental states estimation through self-observation - a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind,https://arxiv.org/abs/2407.18022,2024-07-25,2024-07-26
Keep the Cost Down - A Review on Methods to Optimize LLM' s KV-Cache Consumption,https://arxiv.org/abs/2407.18003,2024-07-25,2024-07-26
The Curious Case of Representational Alignment - Unravelling Visio-Linguistic Tasks in Emergent Communication,https://arxiv.org/abs/2407.17960,2024-07-25,2024-07-26
Relating the Seemingly Unrelated - Principled Understanding of Generalization for Generative Models in Arithmetic Reasoning Tasks,https://arxiv.org/abs/2407.17963,2024-07-25,2024-07-26
DAM - Towards A Foundation Model for Time Series Forecasting,https://arxiv.org/abs/2407.17880,2024-07-25,2024-07-26
Financial Statement Analysis with Large Language Models,https://arxiv.org/abs/2407.17866,2024-07-25,2024-07-26
Demystifying Verbatim Memorization in Large Language Models,https://arxiv.org/abs/2407.17817,2024-07-25,2024-07-26
Investigating learning-independent abstract reasoning in artificial neural networks,https://arxiv.org/abs/2407.17791,2024-07-25,2024-07-26
Optimal Trade and Industrial Policies in the Global Economy - A Deep Learning Framework,https://arxiv.org/abs/2407.17731,2024-07-25,2024-07-26
Nerva - a Truly Sparse Implementation of Neural Networks,https://arxiv.org/abs/2407.17437,2024-07-25,2024-07-26
Dynamic Graph Transformer with Correlated Spatial-Temporal Positional Encoding,https://arxiv.org/abs/2407.16959,2024-07-25,2024-07-26
An Adaptive Gradient Regularization Method,https://arxiv.org/abs/2407.16944,2024-07-25,2024-07-26
Early screening of potential breakthrough technologies with enhanced interpretability - A patent-specific hierarchical attention network model,https://arxiv.org/abs/2407.16939,2024-07-25,2024-07-26
Matryoshka Diffusion Models,https://arxiv.org/abs/2310.15111,2023-10-23,2024-07-26
Can machine learning solve the challenge of adaptive learning and the individualization of learning paths - A field experiment in an online learning platform,https://arxiv.org/abs/2407.03118v3,2024-07-03,2024-07-26
Large Language Models as Misleading Assistants in Conversation,https://arxiv.org/abs/2407.11789v1,2024-07-16,2024-07-26
Vectoring Languages,https://arxiv.org/abs/2407.11766v1,2024-07-16,2024-07-26
Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning,https://arxiv.org/abs/2407.18248v1,2024-07-25,2024-07-26
Recursive Introspection - Teaching Language Model Agents How to Self-Improve,https://arxiv.org/abs/2407.18219v1,2024-07-25,2024-07-26
Castling-ViT - Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference,https://arxiv.org/abs/2211.10526v5,2022-11-18,2024-07-26
Exploring Scaling Trends in LLM Robustness,https://arxiv.org/abs/2407.18213v1,2024-07-25,2024-07-26
Supertrust - Evolution-based superalignment strategy for safe coexistence,https://arxiv.org/abs/2407.20208,2024-07-29,2024-07-30
Eliminating Majority Illusion is Easy,https://arxiv.org/abs/2407.20187,2024-07-29,2024-07-30
MindSearch - Mimicking Human Minds Elicits Deep AI Searcher,https://arxiv.org/abs/2407.20183,2024-07-29,2024-07-30
Machine Learning for predicting chaotic systems,https://arxiv.org/abs/2407.20158,2024-07-29,2024-07-30
Mixture of Nested Experts - Adaptive Processing of Visual Tokens,https://arxiv.org/abs/2407.19985,2024-07-29,2024-07-30
ML-Mamba - Efficient Multi-Modal Large Language Model Utilizing Mamba-2,https://arxiv.org/abs/2407.19832,2024-07-29,2024-07-30
Do Language Models Have a Critical Period for Language Acquisition?,https://arxiv.org/abs/2407.19325,2024-07-29,2024-07-30
"Understanding Memorisation in LLMs - Dynamics, Influencing Factors, and Implications",https://arxiv.org/abs/2407.19262,2024-07-29,2024-07-30
Comprehensive Survey of Complex-Valued Neural Networks - Insights into Backpropagation and Activation Functions,https://arxiv.org/abs/2407.19258,2024-07-29,2024-07-30
Deep Companion Learning - Enhancing Generalization Through Historical Consistency,https://arxiv.org/abs/2407.18821,2024-07-29,2024-07-30
Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs,https://arxiv.org/abs/2407.18808,2024-07-29,2024-07-30
Towards Effective and Efficient Continual Pre-training of Large Language Models,https://arxiv.org/abs/2407.18743,2024-07-29,2024-07-30
"Towards More Accurate Prediction of Human Empathy and Emotion in Text and Multi-turn Conversations by Combining Advanced NLP, Transformers-based Networks, and Linguistic Methodologies",https://arxiv.org/abs/2407.18496,2024-07-29,2024-07-30
The Llama 3 Herd of Models,https://arxiv.org/abs/2407.21783,2024-07-31,2024-08-01
MoMa - Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts,https://arxiv.org/abs/2407.21770,2024-07-31,2024-08-01
Social Learning through Interactions with Other Agents - A Survey,https://arxiv.org/abs/2407.21713,2024-07-31,2024-08-01
Universal Approximation Theory - Foundations for Parallelism in Neural Networks,https://arxiv.org/abs/2407.21670,2024-07-31,2024-08-01
PMoE - Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning,https://arxiv.org/abs/2407.21571,2024-07-31,2024-08-01
Big Cooperative Learning,https://arxiv.org/abs/2407.21319,2024-07-31,2024-08-01
Lifelong Person Search,https://arxiv.org/abs/2407.21252,2024-07-31,2024-08-01
Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens,https://arxiv.org/abs/2407.21248,2024-07-31,2024-08-01
"Entropy, Thermodynamics and the Geometrization of the Language Model",https://arxiv.org/abs/2407.21092,2024-07-31,2024-08-01
Be aware of overfitting by hyperparameter optimization!,https://arxiv.org/abs/2407.20786,2024-07-31,2024-08-01
Exploring Loss Landscapes through the Lens of Spin Glass Theory,https://arxiv.org/abs/2407.20724,2024-07-31,2024-08-01
CultureVo - The Serious Game of Utilizing Gen AI for Enhancing Cultural Intelligence,https://arxiv.org/abs/2407.20685,2024-07-31,2024-08-01
The Entrapment Problem in Random Walk Decentralized Learning,https://arxiv.org/abs/2407.20611,2024-07-31,2024-08-01
Machine Unlearning in Generative AI - A Survey,https://arxiv.org/abs/2407.20516,2024-07-31,2024-08-01
Internal Consistency and Self-Feedback in Large Language Models - A Survey,https://arxiv.org/abs/2407.14507v1,2024-07-19,2024-08-02
I Could've Asked That - Reformulating Unanswerable Questions,https://arxiv.org/abs/2407.17469v1,2024-07-24,2024-08-02
A Unified Framework for Model Editing,https://arxiv.org/abs/2403.14236v4,2024-03-21,2024-08-02
Trajectory-aligned Space-time Tokens for Few-shot Action Recognition,https://arxiv.org/abs/2407.18249v1,2024-07-25,2024-08-02
Meta-Task Prompting Elicits Embeddings from Large Language Models,https://arxiv.org/abs/2402.18458v2,2024-02-28,2024-08-02
The Art of Refusal - A Survey of Abstention in Large Language Models,https://arxiv.org/abs/2407.18418v1,2024-07-25,2024-08-02
Enhancing Training Efficiency Using Packing with Flash Attention,https://arxiv.org/abs/2407.09105v3,2024-07-12,2024-08-02
An introduction to reinforcement learning for neuroscience,https://arxiv.org/abs/2311.07315v2,2023-11-13,2024-08-02
A Notion of Complexity for Theory of Mind via Discrete World Models,https://arxiv.org/abs/2406.11911v2,2024-06-16,2024-08-02
"Entropy, Thermodynamics and the Geometrization of the Language Model",https://arxiv.org/abs/2407.21092v1,2024-07-30,2024-08-02
Do Language Models Have a Critical Period for Language Acquisition -,https://arxiv.org/abs/2407.19325v1,2024-07-27,2024-08-02
World Model on Million-Length Video And Language With Blockwise RingAttention,https://arxiv.org/abs/2402.08268v3,2024-02-13,2024-08-02
LLMs' Understanding of Natural Language Revealed,https://arxiv.org/abs/2407.19630v1,2024-07-29,2024-08-02
Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages - Technologies and Experiences,https://arxiv.org/abs/2407.12620v2,2024-07-17,2024-08-02
Modular Sentence Encoders - Separating Language Specialization from Cross-Lingual Alignment,https://arxiv.org/abs/2407.14878v1,2024-07-20,2024-08-02
Meta-Rewarding Language Models - Self-Improving Alignment with LLM-as-a-Meta-Judge,https://arxiv.org/abs/2407.19594,2024-07-28,2024-08-02
Mixture of Nested Experts - Adaptive Processing of Visual Tokens,https://arxiv.org/abs/2407.19985,2024-07-29,2024-08-02
Large Language Monkeys - Scaling Inference Compute with Repeated Sampling,https://arxiv.org/abs/2407.21787,2024-07-31,2024-08-02
MindSearch - Mimicking Human Minds Elicits Deep AI Searcher,https://arxiv.org/abs/2407.20183,2024-07-29,2024-08-02
On the Design and Analysis of LLM-Based Algorithms,https://arxiv.org/abs/2407.14788v1,2024-07-20,2024-08-02
Unexpected Benefits of Self-Modeling in Neural Systems,https://arxiv.org/abs/2407.10188,2024-07-14,2024-08-05
Gemma 2 - Improving Open Language Models at a Practical Size,https://arxiv.org/abs/2408.00118,2024-07-31,2024-08-05
Next Generation Reservoir Computing,https://arxiv.org/abs/2106.07688,2021-06-14,2024-08-05
Derivation of Back-propagation for Graph Convolutional Networks using Matrix Calculus and its Application to Explainable Artificial Intelligence,https://arxiv.org/abs/2408.01408,2024-08-02,2024-08-05
Transformers are Universal In-context Learners,https://arxiv.org/abs/2408.01367,2024-08-02,2024-08-05
Autoencoders in Function Space,https://arxiv.org/abs/2408.01362,2024-08-02,2024-08-05
Data Debugging is NP-hard for Classifiers Trained with SGD,https://arxiv.org/abs/2408.01365,2024-08-02,2024-08-05
Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models,https://arxiv.org/abs/2408.01308,2024-08-02,2024-08-05
Detection and Characterization of Coordinated Online Behavior - A Survey,https://arxiv.org/abs/2408.01257,2024-08-02,2024-08-05
A Survey on Self-play Methods in Reinforcement Learning,https://arxiv.org/abs/2408.01072,2024-08-02,2024-08-05
On the Resilience of Multi-Agent Systems with Malicious Agents,https://arxiv.org/abs/2408.00989,2024-08-02,2024-08-05
Equivariant neural networks and piecewise linear representation theory,https://arxiv.org/abs/2408.00949,2024-08-02,2024-08-05
Generalisation of Total Uncertainty in AI - A Theoretical Study,https://arxiv.org/abs/2408.00946,2024-08-02,2024-08-05
Disentangling Dense Embeddings with Sparse Autoencoders,https://arxiv.org/abs/2408.00657,2024-08-02,2024-08-05
"SentenceVAE - Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models",https://arxiv.org/abs/2408.00655,2024-08-02,2024-08-05
A Systematic Review on Long-Tailed Learning,https://arxiv.org/abs/2408.00483,2024-08-02,2024-08-05
What comes after transformers? -- A selective survey connecting ideas in deep learning,https://arxiv.org/abs/2408.00386,2024-08-02,2024-08-05
Stretching Each Dollar - Diffusion Training from Scratch on a Micro-Budget,https://arxiv.org/abs/2407.15811,2024-07-22,2024-08-06
Apple Intelligence Foundation Language Models,https://arxiv.org/abs/2407.21075,2024-07-29,2024-08-06
Adversarial Examples that Fool both Computer Vision and Time-Limited Humans,https://arxiv.org/abs/1802.08195,2018-02-22,2024-08-06
Language Model Can Listen While Speaking,https://arxiv.org/abs/2408.02622,2024-08-05,2024-08-06
SEAS - Self-Evolving Adversarial Safety Optimization for Large Language Models,https://arxiv.org/abs/2408.02632,2024-08-05,2024-08-06
Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages,https://arxiv.org/abs/2408.02290,2024-08-05,2024-08-06
Spin glass model of in-context learning,https://arxiv.org/abs/2408.02288,2024-08-05,2024-08-06
DeMansia - Mamba Never Forgets Any Tokens,https://arxiv.org/abs/2408.01986,2024-08-05,2024-08-06
Cross-layer Attention Sharing for Large Language Models,https://arxiv.org/abs/2408.01890,2024-08-05,2024-08-06
STBLLM - Breaking the 1-Bit Barrier with Structured Binary LLMs,https://arxiv.org/abs/2408.01803,2024-08-05,2024-08-06
Classical Machine Learning - Seventy Years of Algorithmic Learning Evolution,https://arxiv.org/abs/2408.01747,2024-08-05,2024-08-06
Neural Machine Translation without Embeddings,https://arxiv.org/abs/2008.09396,2020-08-21,2024-08-06
From pixels to planning - scale-free active inference,https://arxiv.org/abs/2407.20292,2024-07-27,2024-08-07
The Need for a Big World Simulator - A Scientific Challenge for Continual Learning,https://arxiv.org/abs/2408.02930,2024-08-06,2024-08-07
Self-Compressing Neural Networks,https://arxiv.org/abs/2301.13142,2023-01-30,2024-08-07
From Words to Worth - Newborn Article Impact Prediction with LLM,https://arxiv.org/abs/2408.03934,2024-08-07,2024-08-08
Why transformers are obviously good models of language,https://arxiv.org/abs/2408.03855,2024-08-07,2024-08-08
Generative Design of Periodic Orbits in the Restricted Three-Body Problem,https://arxiv.org/abs/2408.03691,2024-08-07,2024-08-08
Is Child-Directed Speech Effective Training Data for Language Models?,https://arxiv.org/abs/2408.03617,2024-08-07,2024-08-08
"1.5-Pints Technical Report - Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data",https://arxiv.org/abs/2408.03506,2024-08-07,2024-08-08
Automated Theorem Provers Help Improve Large Language Model Reasoning,https://arxiv.org/abs/2408.03492,2024-08-07,2024-08-08
Automated mapping of virtual environments with visual predictive coding,https://arxiv.org/abs/2308.10913,2023-08-20,2024-08-09
Learn To Learn More Precisely,https://arxiv.org/abs/2408.04590,2024-08-08,2024-08-09
Conversational Prompt Engineering,https://arxiv.org/abs/2408.04560,2024-08-08,2024-08-09
How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression,https://arxiv.org/abs/2408.04532,2024-08-08,2024-08-09
Partial Experts Checkpoint - Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training,https://arxiv.org/abs/2408.04307,2024-08-08,2024-08-09
The Ungrounded Alignment Problem,https://arxiv.org/abs/2408.04242,2024-08-08,2024-08-09
Diffusion Guided Language Modeling,https://arxiv.org/abs/2408.04220,2024-08-08,2024-08-09
A tutorial on the dynamic Laplacian,https://arxiv.org/abs/2408.04149,2024-08-08,2024-08-09
Step Saver - Predicting Minimum Denoising Steps for Diffusion Model Image Generation,https://arxiv.org/abs/2408.02054v1,2024-08-04,2024-08-09
Climbing the Complexity Ladder with Expressive Attention,https://arxiv.org/abs/2407.18601v1,2024-07-26,2024-08-09
Longhorn - State Space Models are Amortized Online Learners,https://arxiv.org/abs/2407.14207v3,2024-07-19,2024-08-09
Vision language models are blind,https://arxiv.org/abs/2407.06581v5,2024-07-09,2024-08-09
Developing Safe and Responsible Large Language Model  - Can We Balance Bias Reduction and Language Understanding in Large Language Models -,https://arxiv.org/abs/2404.01399v4,2024-04-01,2024-08-09
LLMs' Understanding of Natural Language Revealed,https://arxiv.org/abs/2407.19630v1,2024-07-29,2024-08-09
Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages,https://arxiv.org/abs/2408.02290v1,2024-08-05,2024-08-09
Mixture of Modular Experts - Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models,https://arxiv.org/abs/2407.19610v1,2024-07-28,2024-08-09
Bigger is not Always Better - Scaling Properties of Latent Diffusion Models,https://arxiv.org/abs/2404.01367,2024-04-01,2024-08-09
Mixture-of-Depths - Dynamically allocating compute in transformer-based language models,https://arxiv.org/abs/2404.02258,2024-04-02,2024-08-09
Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,https://arxiv.org/abs/2408.03314,2024-08-06,2024-08-09
Self-Taught Evaluators,https://arxiv.org/abs/2408.02666,2024-08-05,2024-08-09
POA - Pre-training Once for Models of All Sizes,https://arxiv.org/abs/2408.01031,2024-08-02,2024-08-09
Language Model Can Listen While Speaking,https://arxiv.org/abs/2408.02622,2024-08-05,2024-08-09
Autoencoders in Function Space,https://arxiv.org/abs/2408.01362,2024-08-02,2024-08-09
Transformers are Universal In-context Learners,https://arxiv.org/abs/2408.01367,2024-08-02,2024-08-09
EXAONE 3.0 7.8B Instruction Tuned Language Model,https://arxiv.org/abs/2408.03541,2024-08-07,2024-08-09
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-11
Language Models Don't Always Say What They Think - Unfaithful Explanations in Chain-of-Thought Prompting,https://arxiv.org/abs/2305.04388,2023-05-07,2024-08-11
OTCE - Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser,https://arxiv.org/abs/2406.16495,2024-06-24,2024-08-11
LoRA-Pro - Are Low-Rank Adapters Properly Optimized -,https://arxiv.org/abs/2407.18242,2024-07-25,2024-08-11
Rewrite the Stars,https://arxiv.org/abs/2403.19967,2024-03-29,2024-08-11
Tree Cross Attention,https://arxiv.org/abs/2309.17388,2023-09-29,2024-08-11
ReST-MCTS - - LLM Self-Training via Process Reward Guided Tree Search,https://arxiv.org/abs/2406.03816,2024-06-06,2024-08-12
Can Turing machine be curious about its Turing test results - Three informal lectures on physics of intelligence,https://arxiv.org/abs/1606.08109,2016-06-27,2024-08-12
The Physics of Learning - From Autoencoders to Truly Autonomous Learning Machines,https://arxiv.org/abs/2407.04700,2024-02-12,2024-08-12
Understanding Learning through the Lens of Dynamical Invariants,https://arxiv.org/abs/2401.10428,2024-01-19,2024-08-12
Gemma Scope - Open Sparse Autoencoders Everywhere All At Once on Gemma 2,https://arxiv.org/abs/2408.05147,2024-08-10,2024-08-12
"Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks",https://arxiv.org/abs/2408.04965,2024-08-10,2024-08-12
On the Geometry of Deep Learning,https://arxiv.org/abs/2408.04809,2024-08-10,2024-08-12
Liquid Time-constant Networks,https://arxiv.org/abs/2006.04439,2020-06-08,2024-08-13
"Animate, or Inanimate, That is the Question for Large Language Models",https://arxiv.org/abs/2408.06332,2024-08-12,2024-08-13
Reciprocal Learning,https://arxiv.org/abs/2408.06257,2024-08-12,2024-08-13
"A Single Goal is All You Need - Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals",https://arxiv.org/abs/2408.05804,2024-08-12,2024-08-13
Predicting Chaotic System Behavior using Machine Learning Techniques,https://arxiv.org/abs/2408.05702,2024-08-12,2024-08-13
Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms,https://arxiv.org/abs/2406.02900,2024-06-05,2024-08-14
Liquid Structural State-Space Models,https://arxiv.org/abs/2209.12951,2022-09-26,2024-08-14
LLMs can Schedule,https://arxiv.org/abs/2408.06993,2024-08-13,2024-08-14
Layerwise Recurrent Router for Mixture-of-Experts,https://arxiv.org/abs/2408.06793,2024-08-13,2024-08-14
OpenEP - Open-Ended Future Event Prediction,https://arxiv.org/abs/2408.06578,2024-08-13,2024-08-14
AquilaMoE - Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies,https://arxiv.org/abs/2408.06567,2024-08-13,2024-08-14
Introducing the NewsPaLM MBR and QE Dataset - LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data,https://arxiv.org/abs/2408.06537,2024-08-13,2024-08-14
Longhorn - State Space Models are Amortized Online Learners,https://arxiv.org/abs/2407.14207,2024-07-19,2024-08-14
Hierarchical Working Memory and a New Magic Number,https://arxiv.org/abs/2408.07637,2024-08-14,2024-08-15
VideoLLaMA 2 - Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs,https://arxiv.org/abs/2406.07476,2024-06-11,2024-08-16
Diversity Empowers Intelligence - Integrating Expertise of Software Engineering Agents,https://www.arxiv.org/abs/2408.07060,2024-08-13,2024-08-16
Step Saver - Predicting Minimum Denoising Steps for Diffusion Model Image Generation,https://arxiv.org/abs/2408.02054v1,2024-08-04,2024-08-16
Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer,https://arxiv.org/abs/2408.01402v1,2024-08-02,2024-08-16
Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning,https://arxiv.org/abs/2408.00690v2,2024-08-01,2024-08-16
One-Shot Collaborative Data Distillation,https://arxiv.org/abs/2408.02266v1,2024-08-05,2024-08-16
Convergence Conditions for Stochastic Line Search Based Optimization of Over-parametrized Models,https://www.arxiv.org/abs/2408.03199,2024-08-06,2024-08-16
An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models,https://arxiv.org/abs/2408.00724,2024-08-01,2024-08-16
Patch-Level Training for Large Language Models,https://arxiv.org/abs/2407.12665,2024-07-17,2024-08-16
The ShareLM Collection and Plugin - Contributing Human-Model Chats for the Benefit of the Community,https://arxiv.org/abs/2408.08291,2024-08-15,2024-08-16
BAM! Just Like That - Simple and Efficient Parameter Upcycling for Mixture of Experts,https://arxiv.org/abs/2408.08274,2024-08-15,2024-08-16
Not Every Image is Worth a Thousand Words - Quantifying Originality in Stable Diffusion,https://arxiv.org/abs/2408.08184,2024-08-15,2024-08-16
Extracting Sentence Embeddings from Pretrained Transformer Models,https://arxiv.org/abs/2408.08073,2024-08-15,2024-08-16
The Clever Hans Effect in Unsupervised Learning,https://arxiv.org/abs/2408.08041,2024-08-15,2024-08-16
Instruct Large Language Models to Generate Scientific Literature Survey Step by Step,https://arxiv.org/abs/2408.07884,2024-08-15,2024-08-16
Capturing the Complexity of Human Strategic Decision-Making with Machine Learning,https://arxiv.org/abs/2408.07865,2024-08-15,2024-08-16
MoFO - Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning,https://arxiv.org/abs/2407.20999,2024-07-30,2024-08-18
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-18
MoFO - Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning,https://arxiv.org/abs/2407.20999,2024-07-30,2024-08-18
Multi-Meta-RAG - Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata,https://arxiv.org/abs/2406.13213,2024-06-19,2024-08-19
xGen-MM (BLIP-3) - A Family of Open Large Multimodal Models,https://arxiv.org/abs/2408.08872,2024-08-16,2024-08-19
Automated Design of Agentic Systems,https://arxiv.org/abs/2408.08435,2024-08-15,2024-08-19
KAN 2.0 - Kolmogorov-Arnold Networks Meet Science,https://arxiv.org/abs/2408.10205,2024-08-19,2024-08-20
Solving a Rubik's Cube Using its Local Graph Structure,https://arxiv.org/abs/2408.07945,2024-08-15,2024-08-20
Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model,https://arxiv.org/abs/2408.11039,2024-08-20,2024-08-21
Scaling Law with Learning Rate Annealing,https://arxiv.org/abs/2408.11029,2024-08-20,2024-08-21
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control,https://arxiv.org/abs/2408.10970,2024-08-20,2024-08-21
Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations,https://arxiv.org/abs/2408.10920,2024-08-20,2024-08-22
Learning Randomized Algorithms with Transformers,https://arxiv.org/abs/2408.10818,2024-08-20,2024-08-22
Beyond English-Centric LLMs - What Language Do Multilingual Language Models Think in?,https://arxiv.org/abs/2408.10811,2024-08-20,2024-08-22
HMoE - Heterogeneous Mixture of Experts for Language Modeling,https://arxiv.org/abs/2408.10681,2024-08-20,2024-08-22
Strategist - Learning Strategic Skills by LLMs via Bi-Level Tree Search,https://arxiv.org/abs/2408.10635,2024-08-20,2024-08-22
Demystifying the Communication Characteristics for Distributed Transformer Models,https://arxiv.org/abs/2408.10197,2024-08-20,2024-08-22
SMILE - Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models,https://arxiv.org/abs/2408.10174,2024-08-20,2024-08-22
The Exploration-Exploitation Dilemma Revisited - An Entropy Perspective,https://arxiv.org/abs/2408.09974,2024-08-20,2024-08-22
Performance Law of Large Language Models,https://arxiv.org/abs/2408.09895,2024-08-20,2024-08-22
Importance Weighting Can Help Large Language Models Self-Improve,https://arxiv.org/abs/2408.09849,2024-08-20,2024-08-22
Machine Learning with Physics Knowledge for Prediction - A Survey,https://arxiv.org/abs/2408.09840,2024-08-20,2024-08-22
Faster Adaptive Decentralized Learning Algorithms,https://arxiv.org/abs/2408.09775,2024-08-20,2024-08-22
AdapMoE - Adaptive Sensitivity-based Expert Gating and Management for Efficient MoE Inference,https://arxiv.org/abs/2408.10284,2024-08-20,2024-08-22
Acquiring Bidirectionality via Large and Small Language Models,https://arxiv.org/abs/2408.09640,2024-08-20,2024-08-22
Attention is a smoothed cubic spline,https://arxiv.org/abs/2408.09624,2024-08-20,2024-08-22
Latent Causal Probing - A Formal Perspective on Probing with Causal Models of Data,https://arxiv.org/abs/2407.13765,2024-07-18,2024-08-22
Emergent Representations of Program Semantics in Language Models Trained on Programs,https://arxiv.org/abs/2305.11169v3,2023-05-18,2024-08-22
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-22
From pixels to planning - scale-free active inference,https://arxiv.org/abs/2407.20292,2024-07-27,2024-08-22
Critique-out-Loud Reward Models,https://arxiv.org/abs/2408.11791,2024-08-21,2024-08-23
Personality Alignment of Large Language Models,https://arxiv.org/abs/2408.11779,2024-08-21,2024-08-23
FocusLLM - Scaling LLM's Context by Parallel Decoding,https://arxiv.org/abs/2408.11745,2024-08-21,2024-08-23
Memorization In In-Context Learning,https://arxiv.org/abs/2408.11546,2024-08-21,2024-08-23
First Activations Matter - Training-Free Methods for Dynamic Activation in Large Language Models,https://arxiv.org/abs/2408.11393,2024-08-21,2024-08-23
Empirical Equilibria in Agent-based Economic systems with Learning agents,https://arxiv.org/abs/2408.12038,2024-08-21,2024-08-23
Matmul or No Matmal in the Era of 1-bit LLMs,https://arxiv.org/abs/2408.11939,2024-08-21,2024-08-23
Scaling Laws with Vocabulary - Larger Models Deserve Larger Vocabularies,https://arxiv.org/abs/2407.13623,2024-07-18,2024-08-23
LLM Pruning and Distillation in Practice - The Minitron Approach,https://arxiv.org/abs/2408.11796,2024-08-21,2024-08-23
Mission - Impossible Language Models,https://arxiv.org/abs/2401.06416,2024-01-12,2024-08-23
Let Me Speak Freely - A Study on the Impact of Format Restrictions on Performance of Large Language Models,https://arxiv.org/abs/2408.02442,2024-08-05,2024-08-23
Controllable Text Generation for Large Language Models - A Survey,https://arxiv.org/abs/2408.12599,2024-08-22,2024-08-23
MuMA-ToM - Multi-modal Multi-Agent Theory of Mind,https://arxiv.org/abs/2408.12574,2024-08-22,2024-08-23
Jamba-1.5 - Hybrid Transformer-Mamba Models at Scale,https://arxiv.org/abs/2408.12570,2024-08-22,2024-08-23
Not All Samples Should Be Utilized Equally - Towards Understanding and Improving Dataset Distillation,https://arxiv.org/abs/2408.12483,2024-08-22,2024-08-23
Search-Based LLMs for Code Optimization,https://arxiv.org/abs/2408.12159,2024-08-22,2024-08-23
Advancing Prompt Learning through an External Layer,https://arxiv.org/abs/2407.19674v5,2024-07-29,2024-08-23
Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model,https://arxiv.org/abs/2408.11039,2024-08-20,2024-08-23
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-08-23
Generative Verifiers - Reward Modeling as Next-Token Prediction,https://arxiv.org/abs/2408.15240,2024-08-27,2024-08-29
The Mamba in the Llama - Distilling and Accelerating Hybrid Models,https://arxiv.org/abs/2408.15237,2024-08-27,2024-08-29
How transformers learn structured data - insights from hierarchical filtering,https://arxiv.org/abs/2408.15138,2024-08-27,2024-08-29
Evaluating the Energy Consumption of Machine Learning - Systematic Literature Review and Experiments,https://arxiv.org/abs/2408.15128,2024-08-27,2024-08-29
SpikingSSMs - Learning Long Sequences with Sparse and Parallel Spiking State Space Models,https://arxiv.org/abs/2408.14909,2024-08-27,2024-08-29
Inverse-Q* - Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data,https://arxiv.org/abs/2408.14874,2024-08-27,2024-08-29
Diffusion Models Are Real-Time Game Engines,https://arxiv.org/abs/2408.14837,2024-08-27,2024-08-29
Brain-inspired Artificial Intelligence - A Comprehensive Review,https://arxiv.org/abs/2408.14811,2024-08-27,2024-08-29
Emergent Language in Open-Ended Environments,https://arxiv.org/abs/2408.14649,2024-08-27,2024-08-29
1-Bit FQT - Pushing the Limit of Fully Quantized Training to 1-bit,https://arxiv.org/abs/2408.14267,2024-08-27,2024-08-29
Exploring GPU-to-GPU Communication - Insights into Supercomputer Interconnects,https://arxiv.org/abs/2408.14090,2024-08-27,2024-08-30
Category-Theoretical and Topos-Theoretical Frameworks in Machine Learning - A Survey,https://arxiv.org/abs/2408.14014,2024-08-27,2024-08-30
Artificial intelligence for science - The easy and hard problems,https://arxiv.org/abs/2408.14508,2024-08-27,2024-08-30
Selective Preference Optimization via Token-Level Reward Function Estimation,https://arxiv.org/abs/2408.13518,2024-08-27,2024-08-30
A Law of Next-Token Prediction in Large Language Models,https://arxiv.org/abs/2408.13442,2024-08-27,2024-08-30
How Diffusion Models Learn to Factorize and Compose,https://arxiv.org/abs/2408.13256,2024-08-27,2024-08-30
Double Descent - Understanding Linear Model Estimation of Nonidentifiable Parameters and a Model for Overfitting,https://arxiv.org/abs/2408.13235,2024-08-27,2024-08-30
Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,https://arxiv.org/abs/2408.13233,2024-08-27,2024-08-30
Evolvable Psychology Informed Neural Network for Memory Behavior Modeling,https://arxiv.org/abs/2408.14492,2024-08-27,2024-08-30
Automating Thought of Search - A Journey Towards Soundness and Completeness,https://arxiv.org/abs/2408.11326,2024-08-21,2024-08-30
Generative Verifiers - Reward Modeling as Next-Token Prediction,https://arxiv.org/abs/2408.15240,2024-08-27,2024-08-30
Sapiens - Foundation for Human Vision Models,https://arxiv.org/abs/2408.12569,2024-08-22,2024-08-30
Diffusion Models Are Real-Time Game Engines,https://arxiv.org/abs/2408.14837,2024-08-27,2024-08-30
"Mini-Omni - Language Models Can Hear, Talk While Thinking in Streaming",https://arxiv.org/abs/2408.16725,2024-08-29,2024-08-30
LLMs generate structurally realistic social networks but overestimate political homophily,https://arxiv.org/abs/2408.16629,2024-08-29,2024-08-30
Self-Improving Diffusion Models with Synthetic Data,https://arxiv.org/abs/2408.16333,2024-08-29,2024-08-30
Guided Reasoning - A Non-Technical Introduction,https://arxiv.org/abs/2408.16331,2024-08-29,2024-08-30
EPO - Hierarchical LLM Agents with Environment Preference Optimization,https://arxiv.org/abs/2408.16090,2024-08-29,2024-08-31
Is Personality Prediction Possible Based on Reddit Comments?,https://arxiv.org/abs/2408.16089,2024-08-29,2024-08-31
Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects - An Empirical Test of 133 Published Experimental Research Findings,https://arxiv.org/abs/2408.16073,2024-08-29,2024-08-31
Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need,https://arxiv.org/abs/2408.15997,2024-08-29,2024-08-31
In-Context Imitation Learning via Next-Token Prediction,https://arxiv.org/abs/2408.15980,2024-08-29,2024-08-31
Atari-GPT - Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games,https://arxiv.org/abs/2408.15950,2024-08-29,2024-08-31
Nexus - Specialization meets Adaptability for Efficiently Training Mixture of Experts,https://arxiv.org/abs/2408.15901,2024-08-29,2024-08-31
Efficient LLM Scheduling by Learning to Rank,https://arxiv.org/abs/2408.15792,2024-08-29,2024-08-31
Implicit Regularization Paths of Weighted Neural Representations,https://arxiv.org/abs/2408.15784,2024-08-29,2024-08-31
Harmonized Speculative Sampling,https://arxiv.org/abs/2408.15766,2024-08-29,2024-08-31
Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts,https://arxiv.org/abs/2408.15664,2024-08-29,2024-08-31
SIaM - Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models,https://arxiv.org/abs/2408.15565,2024-08-29,2024-08-31
Dolphin - Long Context as a New Modality for Energy-Efficient On-Device Language Models,https://arxiv.org/abs/2408.15518,2024-08-29,2024-08-31
Remove Symmetries to Control Model Expressivity,https://arxiv.org/abs/2408.15495,2024-08-29,2024-08-31
Avoiding Generative Model Writer's Block With Embedding Nudging,https://arxiv.org/abs/2408.15450,2024-08-29,2024-08-31
Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts,https://arxiv.org/abs/2408.15664,2024-08-28,2024-08-31
"SLM Meets LLM - Balancing Latency, Interpretability and Consistency in Hallucination Detection",https://arxiv.org/abs/2408.12748,2024-08-22,2024-08-31
In-Context Learning with Representations - Contextual Generalization of Trained Transformers,https://arxiv.org/abs/2408.10147v1,2024-08-19,2024-08-31
DimeRec - A Unified Framework for Enhanced Sequential Recommendation via Generative Diffusion Models,https://arxiv.org/abs/2408.12153v1,2024-08-22,2024-08-31
An Overview on Machine Learning Methods for Partial Differential Equations - from Physics Informed Neural Networks to Deep Operator Learning,https://arxiv.org/abs/2408.13222,2024-08-23,2024-08-31
Diffusion Models Are Real-Time Game Engines,https://arxiv.org/abs/2408.14837,2024-08-27,2024-08-31
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-09-02
Bidirectional Decoding - Improving Action Chunking via Closed-Loop Resampling,https://arxiv.org/abs/2408.17355,2024-08-30,2024-09-02
Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts,https://arxiv.org/abs/2408.17280,2024-08-30,2024-09-02
Geometry of Lightning Self-Attention - Identifiability and Dimension,https://arxiv.org/abs/2408.17221,2024-08-30,2024-09-02
Towards Hyper-parameter-free Federated Learning,https://arxiv.org/abs/2408.17145,2024-08-30,2024-09-02
Beyond Preferences in AI Alignment,https://arxiv.org/abs/2408.16984,2024-08-30,2024-09-02
Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer,https://arxiv.org/abs/2408.16978,2024-08-30,2024-09-02
Distributed Inference and Fine-tuning of Large Language Models Over The Internet,https://arxiv.org/abs/2312.08361,2023-12-13,2024-09-02
Learning (With) Distributed Optimization,https://arxiv.org/abs/2308.05548,2023-08-10,2024-09-02
Federated Learning - A Cutting-Edge Survey of the Latest Advancements and Applications,https://arxiv.org/abs/2310.05269,2023-10-08,2024-09-02
Adversarial Training Using Feedback Loops,https://arxiv.org/abs/2308.11881,2023-08-23,2024-09-02
OLMoE - Open Mixture-of-Experts Language Models,https://arxiv.org/abs/2409.02060,2024-09-03,2024-09-04
Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling,https://arxiv.org/abs/2409.02908,2024-09-04,2024-09-05
LongCite - Enabling LLMs to Generate Fine-grained Citations in Long-context QA,https://arxiv.org/abs/2409.02897,2024-09-04,2024-09-05
Configurable Foundation Models - Building LLMs from a Modular Perspective,https://arxiv.org/abs/2409.02877,2024-09-04,2024-09-05
A Comparative Study of Pre-training and Self-training,https://arxiv.org/abs/2409.02751,2024-09-04,2024-09-05
Pooling And Attention - What Are Effective Designs For LLm-Based Embedding Models?,https://arxiv.org/abs/2409.02727,2024-09-04,2024-09-05
Neural timescales from a computational perspective,https://arxiv.org/abs/2409.02684,2024-09-04,2024-09-05
Introduction to Machine Learning,https://arxiv.org/abs/2409.02668,2024-09-04,2024-09-05
A Survey on Emergent Language,https://arxiv.org/abs/2409.02645,2024-09-04,2024-09-05
Accelerating Large Language Model Training with Hybrid GPU-based Compression,https://arxiv.org/abs/2409.02423,2024-09-04,2024-09-05
Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering,https://arxiv.org/abs/2409.02426,2024-09-04,2024-09-05
Scaling Laws for Economic Productivity - Experimental Evidence in LLM-Assisted Translation,https://arxiv.org/abs/2409.02391,2024-09-04,2024-09-05
TimeDiT - General-purpose Diffusion Transformers for Time Series Foundation Model,https://arxiv.org/abs/2409.02322,2024-09-04,2024-09-05
Unforgettable Generalization in Language Models,https://arxiv.org/abs/2409.02228,2024-09-04,2024-09-05
Collaboratively Learning Federated Models from Noisy Decentralized Data,https://arxiv.org/abs/2409.02189,2024-09-04,2024-09-05
On a heuristic approach to the description of consciousness as a hypercomplex system state and the possibility of machine consciousness (German edition),https://arxiv.org/abs/2409.02100,2024-09-04,2024-09-05
Learning Machines - In Search of a Concept Oriented Language,https://arxiv.org/abs/2409.01968,2024-09-04,2024-09-05
Training on the Benchmark Is Not All You Need,https://arxiv.org/abs/2409.01790,2024-09-04,2024-09-05
Federated Prediction-Powered Inference from Decentralized Data,https://arxiv.org/abs/2409.01730,2024-09-04,2024-09-05
From Yes-Men to Truth-Tellers - Addressing Sycophancy in Large Language Models with Pinpoint Tuning,https://arxiv.org/abs/2409.01658,2024-09-04,2024-09-05
Dreaming is All You Need,https://arxiv.org/abs/2409.01633,2024-09-04,2024-09-05
On-chain Validation of Tracking Data Messages (TDM) Using Distributed Deep Learning on a Proof of Stake (PoS) Blockchain,https://arxiv.org/abs/2409.01614,2024-09-04,2024-09-05
An Implementation of Werewolf Agent That does not Truly Trust LLMs,https://arxiv.org/abs/2409.01575,2024-09-04,2024-09-05
Quantifying Emergence in Neural Networks - Insights from Pruning and Training Dynamics,https://arxiv.org/abs/2409.01568,2024-09-04,2024-09-05
The Compressor-Retriever Architecture for Language Model OS,https://arxiv.org/abs/2409.01495,2024-09-04,2024-09-05
The Role of Transformer Models in Advancing Blockchain Technology - A Systematic Review,https://arxiv.org/abs/2409.02139,2024-09-04,2024-09-05
Evolution of Social Norms in LLM Agents using Natural Language,https://arxiv.org/abs/2409.00993,2024-09-04,2024-09-05
Beyond Parameter Count - Implicit Bias in Soft Mixture of Experts,https://arxiv.org/abs/2409.00879,2024-09-04,2024-09-05
Self-evolving Agents with reflective and memory-augmented abilities,https://arxiv.org/abs/2409.00872,2024-09-04,2024-09-05
How does the brain compute with probabilities?,https://arxiv.org/abs/2409.02709,2024-09-04,2024-09-05
LanguaShrink - Reducing Token Overhead with Psycholinguistics,https://arxiv.org/abs/2409.00855,2024-09-04,2024-09-05
Interpretable Clustering - A Survey,https://arxiv.org/abs/2409.00743,2024-09-04,2024-09-05
Hyper-Compression - Model Compression via Hyperfunction,https://arxiv.org/abs/2409.00592,2024-09-04,2024-09-05
Diffusion Policy Policy Optimization,https://arxiv.org/abs/2409.00588,2024-09-04,2024-09-05
The Unbearable Slowness of Being,https://www.arxiv.org/abs/2408.10234v1,2024-08-03,2024-09-05
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-09-05
Attention Heads of Large Language Models - A Survey,https://arxiv.org/abs/2409.03752,2024-09-05,2024-09-06
LAST - Language Model Aware Speech Tokenization,https://arxiv.org/abs/2409.03701,2024-09-05,2024-09-06
A Fused Large Language Model for Predicting Startup Success,https://arxiv.org/abs/2409.03668,2024-09-05,2024-09-06
On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization,https://arxiv.org/abs/2409.03650,2024-09-05,2024-09-06
"Attend First, Consolidate Later - On the Importance of Attention in Different LLM Layers",https://arxiv.org/abs/2409.03621,2024-09-05,2024-09-06
"Disclosure of AI-Generated News Increases Engagement but Does Not Reduce Aversion, Despite Positive Quality Ratings",https://arxiv.org/abs/2409.03500,2024-09-05,2024-09-06
KAN See In the Dark,https://arxiv.org/abs/2409.03404,2024-09-05,2024-09-06
Game On - Towards Language Models as RL Experimenters,https://arxiv.org/abs/2409.03402,2024-09-05,2024-09-06
Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization,https://arxiv.org/abs/2409.03303,2024-09-05,2024-09-06
Memory Augmented Language Models through Mixture of Word Experts,https://arxiv.org/abs/2311.10768,2023-11-15,2024-09-09
Banishing LLM Hallucinations Requires Rethinking Generalization,https://arxiv.org/abs/2406.17642,2024-06-25,2024-09-09
From MOOC to MAIC - Reshaping Online Teaching and Learning through LLM-driven Agents,https://arxiv.org/abs/2409.03512,2024-09-05,2024-09-09
"The AdEMAMix Optimizer - Better, Faster, Older",https://arxiv.org/abs/2409.03137,2024-09-05,2024-09-09
Implicit Chain of Thought Reasoning via Knowledge Distillation,https://arxiv.org/abs/2311.01460,2023-11-02,2024-09-09
From Explicit CoT to Implicit CoT - Learning to Internalize CoT Step by Step,https://arxiv.org/abs/2405.14838,2024-05-23,2024-09-09
Improving Transformer Models by Reordering their Sublayers,https://arxiv.org/abs/1911.03864,2019-11-10,2024-09-09
Brainformers - Trading Simplicity for Efficiency,https://arxiv.org/abs/2306.00008,2023-05-29,2024-09-09
"The AdEMAMix Optimizer - Better, Faster, Older",https://arxiv.org/abs/2409.03137,2024-09-05,2024-09-09
"Theory, Analysis, and Best Practices for Sigmoid Self-Attention",https://arxiv.org/abs/2409.04431,2024-09-06,2024-09-10
Residual Stream Analysis with Multi-Layer SAEs,https://arxiv.org/abs/2409.04185,2024-09-06,2024-09-10
Half-VAE - An Encoder-Free VAE to Bypass Explicit Inverse Mapping,https://arxiv.org/abs/2409.04140,2024-09-06,2024-09-10
Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,https://arxiv.org/abs/2409.04109,2024-09-06,2024-09-10
STLM Engineering Report - Dropout,https://arxiv.org/abs/2409.05423,2024-09-06,2024-09-10
Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic Gradients,https://arxiv.org/abs/2409.05305,2024-09-06,2024-09-10
"Some Results on Neural Network Stability, Consistency, and Convergence - Insights into Non-IID Data, High-Dimensional Settings, and Physics-Informed Neural Networks",https://arxiv.org/abs/2409.05030,2024-09-06,2024-09-10
Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings,https://arxiv.org/abs/2409.04949,2024-09-06,2024-09-10
Learning Joint Models of Prediction and Optimization,https://arxiv.org/abs/2409.04898,2024-09-06,2024-09-10
FedModule - A Modular Federated Learning Framework,https://arxiv.org/abs/2409.04849,2024-09-06,2024-09-10
Achieving Peak Performance for Large Language Models - A Systematic Review,https://arxiv.org/abs/2409.04833,2024-09-06,2024-09-10
Optimization Hyper-parameter Laws for Large Language Models,https://arxiv.org/abs/2409.04777,2024-09-06,2024-09-10
BPE Gets Picky - Efficient Vocabulary Refinement During Tokenizer Training,https://arxiv.org/abs/2409.04599,2024-09-06,2024-09-10
E2LLM - Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning,https://arxiv.org/abs/2409.06679,2024-09-10,2024-09-11
DA-MoE - Towards Dynamic Expert Allocation for Mixture-of-Experts Models,https://arxiv.org/abs/2409.06669,2024-09-10,2024-09-11
LLaMA-Omni - Seamless Speech Interaction with Large Language Models,https://arxiv.org/abs/2409.06666,2024-09-10,2024-09-11
Alleviating Hallucinations in Large Language Models with Scepticism Modeling,https://arxiv.org/abs/2409.06601,2024-09-10,2024-09-11
Extracting Paragraphs from LLM Token Activations,https://arxiv.org/abs/2409.06328,2024-09-10,2024-09-11
Unified Neural Network Scaling Laws and Scale-time Equivalence,https://arxiv.org/abs/2409.05782,2024-09-10,2024-09-11
Advanced LSTM Neural Networks for Predicting Directional Changes in Sector-Specific ETFs Using Machine Learning Techniques,https://arxiv.org/abs/2409.05778,2024-09-10,2024-09-11
Breaking Neural Network Scaling Laws with Modularity,https://arxiv.org/abs/2409.05780,2024-09-10,2024-09-11
"LLMs Will Always Hallucinate, and We Need to Live With This",https://arxiv.org/abs/2409.05746,2024-09-10,2024-09-11
Harmonic Reasoning in Large Language Models,https://arxiv.org/abs/2409.05521,2024-09-10,2024-09-11
"Interpolation, Extrapolation, Hyperpolation - Generalising into new dimensions",https://arxiv.org/abs/2409.05513,2024-09-10,2024-09-11
Towards Automated Machine Learning Research,https://arxiv.org/abs/2409.05258,2024-09-10,2024-09-11
"Theory, Analysis, and Best Practices for Sigmoid Self-Attention",https://arxiv.org/abs/2409.04431,2024-09-06,2024-09-13
Sentence Bottleneck Autoencoders from Transformer Language Models,https://arxiv.org/abs/2109.00055,2021-08-31,2024-09-13
"Theory, Analysis, and Best Practices for Sigmoid Self-Attention",https://arxiv.org/abs/2409.04431,2024-09-06,2024-09-13
A mathematical framework of intelligence and consciousness based on Riemannian Geometry,https://arxiv.org/abs/2407.11024v3,2024-07-02,2024-09-13
Improving Pretraining Data Using Perplexity Correlations,https://arxiv.org/abs/2409.05816,2024-09-09,2024-09-13
Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption,https://arxiv.org/abs/2409.07751,2024-09-12,2024-09-13
Large Language Models are Pattern Matchers - Editing Semi-Structured and Structured Documents with ChatGPT,https://arxiv.org/abs/2409.07732,2024-09-12,2024-09-13
Super Monotonic Alignment Search,https://arxiv.org/abs/2409.07704,2024-09-12,2024-09-13
Synthetic continued pretraining,https://arxiv.org/abs/2409.07431,2024-09-12,2024-09-13
Representation Tuning,https://arxiv.org/abs/2409.06927,2024-09-12,2024-09-13
A mathematical framework of intelligence and consciousness based on Riemannian Geometry,https://arxiv.org/pdf/2407.11024,2024-07-02,2024-09-16
A mathematical framework of intelligence and consciousness based on Riemannian Geometry,https://arxiv.org/abs/2407.11024,2024-07-02,2024-09-16
RetrievalAttention - Accelerating Long-Context LLM Inference via Vector Retrieval,https://arxiv.org/abs/2409.10516,2024-09-16,2024-09-17
Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles,https://arxiv.org/abs/2409.10502,2024-09-16,2024-09-17
Schrodinger's Memory - Large Language Models,https://arxiv.org/abs/2409.10482,2024-09-16,2024-09-17
Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors,https://arxiv.org/abs/2409.09905,2024-09-16,2024-09-17
GFlowNet Pretraining with Inexpensive Rewards,https://arxiv.org/abs/2409.09702,2024-09-16,2024-09-17
MindScape Study - Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences,https://arxiv.org/abs/2409.09570,2024-09-16,2024-09-17
"Language Models ""Grok"" to Copy",https://arxiv.org/abs/2409.09281,2024-09-16,2024-09-17
Autoregressive + Chain of Thought (CoT) $\simeq$ Recurrent - Recurrence's Role in Language Models and a Revist of Recurrent Transformer,https://arxiv.org/abs/2409.09239,2024-09-16,2024-09-17
Multi-modal Speech Transformer Decoders - When Do Multiple Modalities Improve Accuracy?,https://arxiv.org/abs/2409.09221,2024-09-16,2024-09-17
Synthetic Human Memories - AI-Edited Images and Videos Can Implant False Memories and Distort Recollection,https://arxiv.org/abs/2409.08895,2024-09-16,2024-09-17
Your Weak LLM is Secretly a Strong Teacher for Alignment,https://arxiv.org/abs/2409.08813,2024-09-16,2024-09-17
What You Say = What You Want? Teaching Humans to Articulate Requirements for LLMs,https://arxiv.org/abs/2409.08775,2024-09-16,2024-09-17
Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding,https://arxiv.org/abs/2409.08561,2024-09-16,2024-09-17
When Context Leads but Parametric Memory Follows in Large Language Models,https://arxiv.org/abs/2409.08435,2024-09-16,2024-09-17
RetrievalAttention - Accelerating Long-Context LLM Inference via Vector Retrieval,https://arxiv.org/abs/2409.10516,2024-09-16,2024-09-17
What is the Role of Small Models in the LLM Era - A Survey,https://arxiv.org/abs/2409.06857,2024-09-10,2024-09-17
Towards Time Series Reasoning with LLMs,https://arxiv.org/abs/2409.11376,2024-09-17,2024-09-18
AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances,https://arxiv.org/abs/2409.11360,2024-09-17,2024-09-18
LPT++ - Efficient Training on Mixture of Long-tailed Experts,https://arxiv.org/abs/2409.11323,2024-09-17,2024-09-18
Linear Recency Bias During Training Improves Transformers' Fit to Reading Times,https://arxiv.org/abs/2409.11250,2024-09-17,2024-09-18
Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization,https://arxiv.org/abs/2409.11212,2024-09-17,2024-09-18
Adaptive Large Language Models By Layerwise Attention Shortcuts,https://arxiv.org/abs/2409.10870,2024-09-17,2024-09-18
Qwen2.5-Coder Technical Report,https://arxiv.org/abs/2409.12186,2024-09-18,2024-09-19
Qwen2.5-Math Technical Report - Toward Mathematical Expert Model via Self-Improvement,https://arxiv.org/abs/2409.12122,2024-09-18,2024-09-19
Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking,https://arxiv.org/abs/2409.12059,2024-09-18,2024-09-19
LLMs + Persona-Plug = Personalized LLMs,https://arxiv.org/abs/2409.11901,2024-09-18,2024-09-19
Human-like Affective Cognition in Foundation Models,https://arxiv.org/abs/2409.11733,2024-09-18,2024-09-19
Revealing the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing,https://arxiv.org/abs/2409.11726,2024-09-18,2024-09-19
Beyond Closure Models - Learning Chaotic-Systems via Physics-Informed Neural Operators,https://arxiv.org/abs/2408.05177,2024-08-09,2024-09-20
When Can LLMs Actually Correct Their Own Mistakes - A Critical Survey of Self-Correction of LLMs,https://arxiv.org/abs/2406.01297,2024-06-03,2024-09-20
TabKANet - Tabular Data Modelling with Kolmogorov-Arnold Network and Transformer,https://arxiv.org/abs/2409.08806,2024-09-13,2024-09-20
Large Language Monkeys - Scaling Inference Compute with Repeated Sampling,https://arxiv.org/abs/2407.21787,2024-07-31,2024-09-20
Kolmogorov-Arnold Transformer,https://arxiv.org/abs/2409.10594,2024-09-16,2024-09-20
Rolling Diffusion Models,https://arxiv.org/abs/2402.09470v3,2024-02-12,2024-09-20
SELF-[IN]CORRECT - LLMs Struggle with Discriminating Self-Generated Responses,https://arxiv.org/abs/2404.04298v3,2024-04-04,2024-09-20
Convergence of the denoising diffusion probabilistic models,https://arxiv.org/abs/2406.01320v2,2024-06-03,2024-09-20
Training Language Models to Self-Correct via Reinforcement Learning,https://arxiv.org/abs/2409.12917,2024-09-19,2024-09-23
MoEUT - Mixture-of-Experts Universal Transformers,https://arxiv.org/abs/2405.16039,2024-05-25,2024-09-24
Training Language Models to Self-Correct via Reinforcement Learning,https://arxiv.org/abs/2409.12917,2024-09-19,2024-09-24
LLMs Still Can't Plan; Can LRMs - A Preliminary Evaluation of OpenAI's o1 on PlanBench,https://arxiv.org/abs/2409.13373,2024-09-20,2024-09-24
Molmo and PixMo - Open Weights and Open Data for State-of-the-Art Multimodal Models,https://arxiv.org/abs/2409.17146,2024-09-25,2024-09-26
Characterizing stable regions in the residual stream of LLMs,https://arxiv.org/abs/2409.17113,2024-09-25,2024-09-26
Counterfactual Token Generation in Large Language Models,https://arxiv.org/abs/2409.17027,2024-09-25,2024-09-26
PeerArg - Argumentative Peer Review with LLMs,https://arxiv.org/abs/2409.16813,2024-09-25,2024-09-26
Large Language Model Predicts Above Normal All India Summer Monsoon Rainfall in 2024,https://arxiv.org/abs/2409.16799,2024-09-25,2024-09-26
Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts,https://arxiv.org/abs/2409.16658,2024-09-25,2024-09-26
A Character-Centric Creative Story Generation via Imagination,https://arxiv.org/abs/2409.16667,2024-09-25,2024-09-26
Is All Learning (Natural) Gradient Descent?,https://arxiv.org/abs/2409.16422,2024-09-25,2024-09-26
LLM Echo Chamber - personalized and automated disinformation,https://arxiv.org/abs/2409.16241,2024-09-25,2024-09-26
Merging LoRAs like Playing LEGO - Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering,https://arxiv.org/abs/2409.16167,2024-09-25,2024-09-26
Self-attention as an attractor network - transient memories without backpropagation,https://arxiv.org/abs/2409.16112,2024-09-25,2024-09-26
Assessing Simplification Levels in Neural Networks - The Impact of Hyperparameter Configurations on Complexity and Sensitivity,https://arxiv.org/abs/2409.16086,2024-09-25,2024-09-26
Time-MoE - Billion-Scale Time Series Foundation Models with Mixture of Experts,https://arxiv.org/abs/2409.16040,2024-09-25,2024-09-26
Grounded Computation & Consciousness - A Framework for Exploring Consciousness in Machines & Other Organisms,https://arxiv.org/abs/2409.16036,2024-09-25,2024-09-26
AI Can Be Cognitively Biased - An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment,https://arxiv.org/abs/2409.16022,2024-09-25,2024-09-26
Artificial Human Intelligence - The role of Humans in the Development of Next Generation AI,https://arxiv.org/abs/2409.16001,2024-09-25,2024-09-26
Planning in the Dark - LLM-Symbolic Planning Pipeline without Experts,https://arxiv.org/abs/2409.15915,2024-09-25,2024-09-26
HLB - Benchmarking LLMs' Humanlikeness in Language Use,https://arxiv.org/abs/2409.15890,2024-09-25,2024-09-26
Supervised Fine-Tuning - An Activation Pattern Optimization Process for Attention Heads,https://arxiv.org/abs/2409.15820,2024-09-25,2024-09-26
Federated Large Language Models - Current Progress and Future Directions,https://arxiv.org/abs/2409.15723,2024-09-25,2024-09-26
Looped Transformers for Length Generalization,https://arxiv.org/abs/2409.15647,2024-09-25,2024-09-26
Personalized Federated Learning via Backbone Self-Distillation,https://arxiv.org/abs/2409.15636,2024-09-25,2024-09-26
On The Specialization of Neural Modules,https://arxiv.org/abs/2409.14981,2024-09-25,2024-09-26
Why Is Anything Conscious?,https://arxiv.org/abs/2409.14545,2024-09-25,2024-09-26
On a measure of intelligence,https://arxiv.org/abs/2409.14496,2024-09-25,2024-09-26
A Large Language Model and Denoising Diffusion Framework for Targeted Design of Microstructures with Commands in Natural Language,https://arxiv.org/abs/2409.14473,2024-09-25,2024-09-26
Flat-LoRA - Low-Rank Adaption over a Flat Loss Landscape,https://arxiv.org/abs/2409.14396,2024-09-25,2024-09-26
Investigating Layer Importance in Large Language Models,https://arxiv.org/abs/2409.14381,2024-09-25,2024-09-26
Uncovering Latent Chain of Thought Vectors in Language Models,https://arxiv.org/abs/2409.14026,2024-09-25,2024-09-26
Guided Profile Generation Improves Personalization with LLMs,https://arxiv.org/abs/2409.13093,2024-09-25,2024-09-26
"Re-Introducing LayerNorm - Geometric Meaning, Irreversibility and a Comparative Study with RMSNorm",https://arxiv.org/abs/2409.12951,2024-09-25,2024-09-26
Scaling Smart - Accelerating Large Language Model Pre-training with Small Model Initialization,https://arxiv.org/abs/2409.12903,2024-09-25,2024-09-26
The Central Role of the Loss Function in Reinforcement Learning,https://arxiv.org/abs/2409.12799,2024-09-25,2024-09-26
Scaling FP8 training to trillion-token LLMs,https://arxiv.org/abs/2409.12517,2024-09-25,2024-09-26
Backtracking Improves Generation Safety,https://arxiv.org/abs/2409.14586,2024-09-22,2024-09-26
SequenceMatch - Imitation Learning for Autoregressive Sequence Modelling with Backtracking,https://arxiv.org/abs/2306.05426,2023-06-08,2024-09-26
From Explicit CoT to Implicit CoT - Learning to Internalize CoT Step by Step,https://arxiv.org/abs/2405.14838,2024-05-23,2024-09-27
Consistency for Large Neural Networks,https://arxiv.org/abs/2409.14123,2024-09-21,2024-09-27
Molmo and PixMo - Open Weights and Open Data for State-of-the-Art Multimodal Models,https://arxiv.org/abs/2409.17146v1,2024-09-25,2024-09-27
Cyclic image generation using chaotic dynamics,https://arxiv.org/abs/2405.20717v2,2024-05-31,2024-09-27
Infer Human's Intentions Before Following Natural Language Instructions,https://arxiv.org/abs/2409.18073,2024-09-26,2024-09-27
"EMOVA - Empowering Language Models to See, Hear and Speak with Vivid Emotions",https://arxiv.org/abs/2409.18042,2024-09-26,2024-09-27
"Why Companies ""Democratise"" Artificial Intelligence - The Case of Open Source Software Donations",https://arxiv.org/abs/2409.17876,2024-09-26,2024-09-27
MIO - A Foundation Model on Multimodal Tokens,https://arxiv.org/abs/2409.17692,2024-09-26,2024-09-27
Optimal Memorization Capacity of Transformers,https://arxiv.org/abs/2409.17677,2024-09-26,2024-09-27
Logic-of-Thought - Injecting Logic into Contexts for Full Reasoning in Large Language Models,https://arxiv.org/abs/2409.17539,2024-09-26,2024-09-27
NeuroPath - A Neural Pathway Transformer for Joining the Dots of Human Connectomes,https://arxiv.org/abs/2409.17510,2024-09-26,2024-09-27
Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers,https://arxiv.org/abs/2409.20537,2024-09-30,2024-10-01
COLLAGE - Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models,https://arxiv.org/abs/2409.20502,2024-09-30,2024-10-01
The Perfect Blend - Redefining RLHF with Mixture of Judges,https://arxiv.org/abs/2409.20370,2024-09-30,2024-10-01
A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions,https://arxiv.org/abs/2409.20303,2024-09-30,2024-10-01
PersonalLLM - Tailoring LLMs to Individual Preferences,https://arxiv.org/abs/2409.20296,2024-09-30,2024-10-01
1 Trillion Token (1TT) Platform - A Novel Framework for Efficient Data Sharing and Compensation in Large Language Models,https://arxiv.org/abs/2409.20149,2024-09-30,2024-10-01
Counter-Current Learning - A Biologically Plausible Dual Network Approach for Deep Learning,https://arxiv.org/abs/2409.19841,2024-09-30,2024-10-01
Revealing Personality Traits - A New Benchmark Dataset for Explainable Personality Recognition on Dialogues,https://arxiv.org/abs/2409.19723,2024-09-30,2024-10-01
Unifying back-propagation and forward-forward algorithms through model predictive control,https://arxiv.org/abs/2409.19561,2024-09-30,2024-10-01
'Simulacrum of Stories' - Examining Large Language Models as Qualitative Research Participants,https://arxiv.org/abs/2409.19430,2024-09-30,2024-10-01
Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?,https://arxiv.org/abs/2409.19151,2024-09-30,2024-10-01
HM3 - Hierarchical Multi-Objective Model Merging for Pretrained Models,https://arxiv.org/abs/2409.18893,2024-09-30,2024-10-01
Hierarchical Federated ADMM,https://arxiv.org/abs/2409.18796,2024-09-30,2024-10-01
Cottention - Linear Transformers With Cosine Attention,https://arxiv.org/abs/2409.18747,2024-09-30,2024-10-01
"Effects of AI Feedback on Learning, the Skill Gap, and Intellectual Diversity",https://arxiv.org/abs/2409.18660,2024-09-30,2024-10-01
Kolmogorov-Arnold Network Autoencoders,https://arxiv.org/abs/2410.02077,2024-10-02,2024-10-05
PHNNs - Lightweight Neural Networks via Parameterized Hypercomplex Convolutions,https://arxiv.org/abs/2110.04176,2021-10-08,2024-10-05
Compacter - Efficient Low-Rank Hypercomplex Adapter Layers,https://arxiv.org/abs/2106.04647,2021-06-08,2024-10-05
PeerArg - Argumentative Peer Review with LLMs,https://arxiv.org/abs/2409.16813,2024-09-25,2024-10-05
"When a language model is optimized for reasoning, does it still show embers of autoregression - An analysis of OpenAI o1",https://arxiv.org/abs/2410.01792,2024-10-02,2024-10-05
Erasing Conceptual Knowledge from Language Models,https://arxiv.org/abs/2410.02760,2024-10-03,2024-10-05
"Adaptive Inference-Time Compute - LLMs Can Predict if They Can Do Better, Even Mid-Generation",https://arxiv.org/abs/2410.02725,2024-10-03,2024-10-05
LLMs Know More Than They Show - On the Intrinsic Representation of LLM Hallucinations,https://arxiv.org/abs/2410.02707,2024-10-03,2024-10-05
Selective Attention Improves Transformer,https://arxiv.org/abs/2410.02703,2024-10-03,2024-10-05
On the Proper Treatment of Tokenization in Psycholinguistics,https://arxiv.org/abs/2410.02691,2024-10-03,2024-10-05
FAN - Fourier Analysis Networks,https://arxiv.org/abs/2410.02675,2024-10-03,2024-10-05
How to Train Long-Context Language Models (Effectively),https://arxiv.org/abs/2410.02660,2024-10-03,2024-10-05
Generalization emerges from local optimization in a self-organized learning network,https://arxiv.org/abs/2410.02590,2024-10-03,2024-10-05
Diffusion Models are Evolutionary Algorithms,https://arxiv.org/abs/2410.02543,2024-10-03,2024-10-05
Fair Decentralized Learning,https://arxiv.org/abs/2410.02541,2024-10-03,2024-10-05
Intelligence at the Edge of Chaos,https://arxiv.org/abs/2410.02536,2024-10-03,2024-10-05
Defining Knowledge - Bridging Epistemology and Large Language Models,https://arxiv.org/abs/2410.02499,2024-10-03,2024-10-05
Meta-Models - An Architecture for Decoding LLM Behaviors Through Interpreted Embeddings and Natural Language,https://arxiv.org/abs/2410.02472,2024-10-03,2024-10-05
Towards a Theoretical Understanding of Memorization in Diffusion Models,https://arxiv.org/abs/2410.02467,2024-10-03,2024-10-05
Llama SLayer 8B - Shallow Layers Hold the Key to Knowledge Injection,https://arxiv.org/abs/2410.02330,2024-10-03,2024-10-05
Post-edits Are Preferences Too,https://arxiv.org/abs/2410.02320,2024-10-03,2024-10-05
Theoretical Insights into Fine-Tuning Attention Mechanism - Generalization and Optimization,https://arxiv.org/abs/2410.02247,2024-10-03,2024-10-05
EmbedLLM - Learning Compact Representations of Large Language Models,https://arxiv.org/abs/2410.02223,2024-10-03,2024-10-05
Towards Better Generalization - Weight Decay Induces Low-rank Bias for Neural Networks,https://arxiv.org/abs/2410.02176,2024-10-03,2024-10-05
Planning in Strawberry Fields - Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1,https://arxiv.org/abs/2410.02162,2024-10-03,2024-10-05
Mitigating Memorization In Language Models,https://arxiv.org/abs/2410.02159,2024-10-03,2024-10-05
"Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL",https://arxiv.org/abs/2410.01930,2024-10-03,2024-10-05
"When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1",https://arxiv.org/abs/2410.01792,2024-10-03,2024-10-05
Quantifying Generalization Complexity for Large Language Models,https://arxiv.org/abs/2410.01769,2024-10-03,2024-10-05
U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,https://arxiv.org/abs/2410.01692,2024-10-03,2024-10-05
On The Adaptation of Unlimiformer for Decoder-Only Transformers,https://arxiv.org/abs/2410.01637,2024-10-03,2024-10-05
ENTP - Encoder-only Next Token Prediction,https://arxiv.org/abs/2410.01600,2024-10-03,2024-10-05
Lines of Thought in Large Language Models,https://arxiv.org/abs/2410.01545,2024-10-03,2024-10-05
House of Cards - Massive Weights in LLMs,https://arxiv.org/abs/2410.01866,2024-10-03,2024-10-05
Geometric Signatures of Compositionality Across a Language Model's Lifetime,https://arxiv.org/abs/2410.01444,2024-10-03,2024-10-05
FlashMask - Efficient and Rich Mask Extension of FlashAttention,https://arxiv.org/abs/2410.01359,2024-10-03,2024-10-05
Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models,https://arxiv.org/abs/2410.01280,2024-10-03,2024-10-05
nGPT - Normalized Transformer with Representation Learning on the Hypersphere,https://arxiv.org/abs/2410.01131,2024-10-03,2024-10-05
Draft on the Fly - Adaptive Self-Speculative Decoding using Cosine Similarity,https://arxiv.org/abs/2410.01028,2024-10-03,2024-10-05
Investigating the Synergistic Effects of Dropout and Residual Connections on Language Model Training,https://arxiv.org/abs/2410.01019,2024-10-03,2024-10-05
Do Music Generation Models Encode Music Theory?,https://arxiv.org/abs/2410.00872,2024-10-03,2024-10-05
"RisingBALLER - A player is a token, a match is a sentence, A path towards a foundational model for football players data analytics",https://arxiv.org/abs/2410.00943,2024-10-03,2024-10-05
Self-Updatable Large Language Models with Parameter Integration,https://arxiv.org/abs/2410.00487,2024-10-03,2024-10-05
Stability analysis of chaotic systems in latent spaces,https://arxiv.org/abs/2410.00480,2024-10-03,2024-10-05
MoS - Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards,https://arxiv.org/abs/2410.00938,2024-10-03,2024-10-05
EKAN - Equivariant Kolmogorov-Arnold Networks,https://arxiv.org/abs/2410.00435,2024-10-03,2024-10-05
LayerKV - Optimizing Large Language Model Serving with Layer-wise KV Cache Management,https://arxiv.org/abs/2410.00428,2024-10-03,2024-10-05
Are LLMs Aware that Some Questions are not Open-ended?,https://arxiv.org/abs/2410.00423,2024-10-03,2024-10-05
TikGuard - A Deep Learning Transformer-Based Solution for Detecting Unsuitable TikTok Content for Kids,https://arxiv.org/abs/2410.00403,2024-10-03,2024-10-05
Vision Language Models See What You Want but not What You See,https://arxiv.org/abs/2410.00324,2024-10-03,2024-10-05
MM-Ego - Towards Building Egocentric Multimodal LLMs,https://arxiv.org/abs/2410.07177,2024-10-09,2024-10-10
Glider - Global and Local Instruction-Driven Expert Router,https://arxiv.org/abs/2410.07172,2024-10-09,2024-10-10
"Continual Learning - Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay",https://arxiv.org/abs/2410.07110,2024-10-09,2024-10-10
"PositionID - LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",https://arxiv.org/abs/2410.07035,2024-10-09,2024-10-10
CursorCore - Assist Programming through Aligning Anything,https://arxiv.org/abs/2410.07002,2024-10-09,2024-10-10
MatMamba - A Matryoshka State Space Model,https://arxiv.org/abs/2410.06718,2024-10-09,2024-10-10
Decouple-Then-Merge - Towards Better Training for Diffusion Models,https://arxiv.org/abs/2410.06664,2024-10-09,2024-10-10
The Accuracy Paradox in RLHF - When Better Reward Models Don't Yield Better Language Models,https://arxiv.org/abs/2410.06554,2024-10-09,2024-10-10
Does Spatial Cognition Emerge in Frontier Models?,https://arxiv.org/abs/2410.06468,2024-10-09,2024-10-10
NLP Case Study on Predicting the Before and After of the Ukraine-Russia and Hamas-Israel Conflicts,https://arxiv.org/abs/2410.06427,2024-10-09,2024-10-10
MC-MoE - Mixture Compressor for Mixture-of-Experts LLMs Gains More,https://arxiv.org/abs/2410.06270,2024-10-09,2024-10-10
"RL, but don't do anything I wouldn't do",https://arxiv.org/abs/2410.06213,2024-10-09,2024-10-10
Round and Round We Go! What makes Rotary Positional Encodings useful?,https://arxiv.org/abs/2410.06205,2024-10-09,2024-10-10
From Tokens to Words - on the inner lexicon of LLMs,https://arxiv.org/abs/2410.05864,2024-10-09,2024-10-10
Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting,https://arxiv.org/abs/2410.05711,2024-10-09,2024-10-10
Everything Everywhere All at Once - LLMs can In-Context Learn Multiple Tasks in Superposition,https://arxiv.org/abs/2410.05603,2024-10-09,2024-10-10
Differential Transformer,https://arxiv.org/abs/2410.05258,2024-10-09,2024-10-10
LLMs Are In-Context Reinforcement Learners,https://arxiv.org/abs/2410.05362,2024-10-09,2024-10-10
Falcon Mamba - The First Competitive Attention-free 7B Language Model,https://arxiv.org/abs/2410.05355,2024-10-09,2024-10-10
Towards a Categorical Foundation of Deep Learning - A Survey,https://arxiv.org/abs/2410.05353,2024-10-09,2024-10-10
Grokking at the Edge of Linear Separability,https://arxiv.org/abs/2410.04489,2024-10-09,2024-10-10
"Blocks Architecture (BloArk) - Efficient, Cost-Effective, and Incremental Dataset Architecture for Wikipedia Revision History",https://arxiv.org/abs/2410.04410,2024-10-09,2024-10-10
ReTok - Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model,https://arxiv.org/abs/2410.04335,2024-10-09,2024-10-10
Gradient Routing - Masking Gradients to Localize Computation in Neural Networks,https://arxiv.org/abs/2410.04332,2024-10-09,2024-10-10
Evaluating Language Model Character Traits,https://arxiv.org/abs/2410.04272,2024-10-09,2024-10-10
LoRTA - Low Rank Tensor Adaptation of Large Language Models,https://arxiv.org/abs/2410.04060,2024-10-09,2024-10-10
pFedGame -- Decentralized Federated Learning using Game Theory in Dynamic Topology,https://arxiv.org/abs/2410.04058,2024-10-09,2024-10-10
Neuron-Level Sequential Editing for Large Language Models,https://arxiv.org/abs/2410.04045,2024-10-09,2024-10-10
From Pixels to Personas - Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues,https://arxiv.org/abs/2410.03870,2024-10-09,2024-10-10
Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style,https://arxiv.org/abs/2410.03848,2024-10-09,2024-10-10
Teaching Transformers Modular Arithmetic at Scale,https://arxiv.org/abs/2410.03569,2024-10-09,2024-10-10
Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs),https://arxiv.org/abs/2410.03568,2024-10-09,2024-10-10
Mixture of Attentions For Speculative Decoding,https://arxiv.org/abs/2410.03804,2024-10-09,2024-10-10
Autoregressive Large Language Models are Computationally Universal,https://arxiv.org/abs/2410.03170,2024-10-09,2024-10-10
In-context Learning in Presence of Spurious Correlations,https://arxiv.org/abs/2410.03140,2024-10-09,2024-10-10
Integrating Natural Language Prompting Tasks in Introductory Programming Courses,https://arxiv.org/abs/2410.03063,2024-10-09,2024-10-10
Were RNNs All We Needed -,https://arxiv.org/abs/2410.01201,2024-10-02,2024-10-12
VPTQ - Extreme Low-bit Vector Post-Training Quantization for Large Language Models,https://arxiv.org/abs/2409.17066,2024-09-25,2024-10-12
Intelligence at the Edge of Chaos,https://www.arxiv.org/abs/2410.02536,2024-10-03,2024-10-12
One Initialization to Rule them All - Fine-tuning via Explained Variance Adaptation,https://arxiv.org/abs/2410.07170,2024-10-09,2024-10-12
MLE-bench - Evaluating Machine Learning Agents on Machine Learning Engineering,https://arxiv.org/abs/2410.07095,2024-10-09,2024-10-12
Think Twice - A Human-like Two-stage Conversational Agent for Emotional Response Generation,https://arxiv.org/abs/2301.04907v3,2023-01-12,2024-10-12
"Kiss up, Kick down - Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas",https://arxiv.org/abs/2410.03181v1,2024-10-04,2024-10-12
A Stability Principle for Learning under Non-Stationarity,https://arxiv.org/abs/2310.18304v3,2023-10-27,2024-10-12
Unification of popular artificial neural network activation functions,https://arxiv.org/abs/2302.11007v3,2023-02-21,2024-10-12
Hyper-Connections,https://arxiv.org/abs/2409.19606v1,2024-09-29,2024-10-12
Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism,https://arxiv.org/abs/2409.17625,2024-09-26,2024-10-12
Causal Inference with Large Language Model - A Survey,https://arxiv.org/abs/2409.09822,2024-09-15,2024-10-12
Why Do We Need Weight Decay in Modern Deep Learning -,https://arxiv.org/abs/2310.04415,2023-10-06,2024-10-12
OpenDiLoCo - An Open-Source Framework for Globally Distributed Low-Communication Training,https://arxiv.org/abs/2407.07852,2024-07-10,2024-10-12
Navigating the Digital World as Humans Do - Universal Visual Grounding for GUI Agents,https://arxiv.org/abs/2410.05243,2024-10-07,2024-10-12
Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision,https://arxiv.org/abs/2410.08209,2024-10-10,2024-10-12
Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity,https://arxiv.org/abs/2410.08198,2024-10-10,2024-10-12
Benign Overfitting in Single-Head Attention,https://arxiv.org/abs/2410.07746,2024-10-10,2024-10-12
GSM-Symbolic - Understanding the Limitations of Mathematical Reasoning in Large Language Models,https://arxiv.org/abs/2410.05229,2024-10-07,2024-10-13
Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free,https://arxiv.org/abs/2410.10814,2024-10-14,2024-10-15
HART - Efficient Visual Generation with Hybrid Autoregressive Transformer,https://arxiv.org/abs/2410.10812,2024-10-14,2024-10-15
Local and Global Decoding in Text Generation,https://arxiv.org/abs/2410.10810,2024-10-14,2024-10-15
When Attention Sink Emerges in Language Models - An Empirical View,https://arxiv.org/abs/2410.10781,2024-10-14,2024-10-15
SeedLM - Compressing LLM Weights into Seeds of Pseudo-Random Generators,https://arxiv.org/abs/2410.10714,2024-10-14,2024-10-15
Thinking LLMs - General Instruction Following with Thought Generation,https://arxiv.org/abs/2410.10630,2024-10-14,2024-10-15
MoH - Multi-Head Attention as Mixture-of-Head Attention,https://arxiv.org/abs/2410.11842,2024-10-15,2024-10-17
A Hitchhiker's Guide to Scaling Law Estimation,https://arxiv.org/abs/2410.11840,2024-10-15,2024-10-17
Language Models Encode Numbers Using Digit Representations in Base 10,https://arxiv.org/abs/2410.11781,2024-10-15,2024-10-17
Encoding architecture algebra,https://arxiv.org/abs/2410.11776,2024-10-15,2024-10-17
Personas with Attitudes - Controlling LLMs for Diverse Data Annotation,https://arxiv.org/abs/2410.11745,2024-10-15,2024-10-17
Light-Weight Fault Tolerant Attention for Large Language Model Training,https://arxiv.org/abs/2410.11720,2024-10-15,2024-10-17
Zero-shot Model-based Reinforcement Learning using Large Language Models,https://arxiv.org/abs/2410.11711,2024-10-15,2024-10-17
Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing,https://arxiv.org/abs/2410.11462,2024-10-15,2024-10-17
A Case for AI Consciousness - Language Agents and Global Workspace Theory,https://arxiv.org/abs/2410.11407,2024-10-15,2024-10-17
Convergence to the Truth,https://arxiv.org/abs/2410.11399,2024-10-15,2024-10-17
Role of Delay in Brain Dynamics,https://arxiv.org/abs/2410.11384,2024-10-15,2024-10-17
Subspace Optimization for Large Language Models with Convergence Guarantees,https://arxiv.org/abs/2410.11289,2024-10-15,2024-10-17
Cross-Dataset Generalization in Deep Learning,https://arxiv.org/abs/2410.11207,2024-10-15,2024-10-17
Interpretability as Compression - Reconsidering SAE Explanations of Neural Activations with MDL-SAEs,https://arxiv.org/abs/2410.11179,2024-10-15,2024-10-17
Model Swarms - Collaborative Search to Adapt LLM Experts via Swarm Intelligence,https://arxiv.org/abs/2410.11163,2024-10-15,2024-10-17
Differentiable Weightless Neural Networks,https://arxiv.org/abs/2410.11112,2024-10-15,2024-10-17
Varying Shades of Wrong - Aligning LLMs with Wrong Answers Only,https://arxiv.org/abs/2410.11055,2024-10-15,2024-10-17
Personality Differences Drive Conversational Dynamics - A High-Dimensional NLP Approach,https://arxiv.org/abs/2410.11043,2024-10-15,2024-10-17
Liger Kernel - Efficient Triton Kernels for LLM Training,https://arxiv.org/abs/2410.10989,2024-10-15,2024-10-17
A Unified Approach to Routing and Cascading for LLMs,https://arxiv.org/abs/2410.10347,2024-10-15,2024-10-17
Is Parameter Collision Hindering Continual Learning in LLMs?,https://arxiv.org/abs/2410.10179,2024-10-15,2024-10-17
Reverse Modeling in Large Language Models,https://arxiv.org/abs/2410.09817,2024-10-15,2024-10-17
Janus - Decoupling Visual Encoding for Unified Multimodal Understanding and Generation,https://arxiv.org/abs/2410.13848,2024-10-17,2024-10-19
Thinking LLMs - General Instruction Following with Thought Generation,https://arxiv.org/abs/2410.10630,2024-10-14,2024-10-19
Scaling Laws For Diffusion Transformers,https://arxiv.org/abs/2410.08184,2024-10-10,2024-10-19
Agent-as-a-Judge - Evaluate Agents with Agents,https://arxiv.org/abs/2410.10934,2024-10-14,2024-10-19
Representation Alignment for Generation - Training Diffusion Transformers Is Easier Than You Think,https://arxiv.org/abs/2410.06940,2024-10-09,2024-10-19
GSM-Symbolic - Understanding the Limitations of Mathematical Reasoning in Large Language Models,https://arxiv.org/abs/2410.05229,2024-10-07,2024-10-19
Fluid - Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens,https://arxiv.org/abs/2410.13863,2024-10-17,2024-10-19
How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs,https://arxiv.org/abs/2410.13857,2024-10-17,2024-10-19
Diffusing States and Matching Scores - A New Framework for Imitation Learning,https://arxiv.org/abs/2410.13855,2024-10-17,2024-10-19
Active-Dormant Attention Heads - Mechanistically Demystifying Extreme-Token Phenomena in LLMs,https://arxiv.org/abs/2410.13835,2024-10-17,2024-10-19
Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions,https://arxiv.org/abs/2410.13788,2024-10-17,2024-10-19
Looking Inward - Language Models Can Learn About Themselves by Introspection,https://arxiv.org/abs/2410.13787,2024-10-17,2024-10-19
Reducing the Transformer Architecture to a Minimum,https://arxiv.org/abs/2410.13732,2024-10-17,2024-10-19
A Comparative Study on Reasoning Patterns of OpenAI's o1 Model,https://arxiv.org/abs/2410.13639,2024-10-17,2024-10-19
Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation,https://arxiv.org/abs/2410.13640,2024-10-17,2024-10-19
MoR - Mixture of Ranks for Low-Rank Adaptation Tuning,https://arxiv.org/abs/2410.13408,2024-10-17,2024-10-19
Metacognitive Monitoring - A Human Ability Beyond Generative Artificial Intelligence,https://arxiv.org/abs/2410.13392,2024-10-17,2024-10-19
SeerAttention - Learning Intrinsic Sparse Attention in Your LLMs,https://arxiv.org/abs/2410.13276,2024-10-17,2024-10-19
Hypothesis Testing the Circuit Hypothesis in LLMs,https://arxiv.org/abs/2410.13032,2024-10-17,2024-10-19
Merge to Learn - Efficiently Adding Skills to Language Models with Model Merging,https://arxiv.org/abs/2410.12937,2024-10-17,2024-10-19
FusionLLM - A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression,https://arxiv.org/abs/2410.12707,2024-10-17,2024-10-19
One Step Diffusion via Shortcut Models,https://arxiv.org/abs/2410.12557,2024-10-17,2024-10-19
Investigating Sensitive Directions in GPT-2 - An Improved Baseline and Comparative Analysis of SAEs,https://arxiv.org/abs/2410.12555,2024-10-17,2024-10-19
Unifying Economic and Language Models for Enhanced Sentiment Analysis of the Oil Market,https://arxiv.org/abs/2410.12473,2024-10-17,2024-10-19
Conformity in Large Language Models,https://arxiv.org/abs/2410.12428,2024-10-17,2024-10-19
Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding,https://arxiv.org/abs/2410.12413,2024-10-17,2024-10-19
Tracking Universal Features Through Fine-Tuning and Model Merging,https://arxiv.org/abs/2410.12391,2024-10-17,2024-10-19
SimLayerKV - A Simple Framework for Layer-Level KV Cache Reduction,https://arxiv.org/abs/2410.13846,2024-10-17,2024-10-20
Looking Inward - Language Models Can Learn About Themselves by Introspection,https://arxiv.org/abs/2410.13787,2024-10-17,2024-10-20
Understanding Likelihood Over-optimisation in Direct Alignment Algorithms,https://arxiv.org/abs/2410.11677,2024-10-15,2024-10-20
Decentralized Federated Learning with Gradient Tracking over Time-Varying Directed Networks,https://arxiv.org/abs/2409.17189,2024-09-25,2024-10-20
How Feature Learning Can Improve Neural Scaling Laws,https://arxiv.org/abs/2409.17858,2024-09-26,2024-10-20
Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism,https://arxiv.org/abs/2409.17625,2024-09-26,2024-10-20
MiniPLM - Knowledge Distillation for Pre-Training Language Models,https://arxiv.org/abs/2410.17215,2024-10-22,2024-10-24
Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence,https://arxiv.org/abs/2410.17161,2024-10-22,2024-10-24
Methods of improving LLM training stability,https://arxiv.org/abs/2410.16682,2024-10-22,2024-10-24
BIG5-CHAT - Shaping LLM Personalities Through Training on Human-Grounded Data,https://arxiv.org/abs/2410.16491,2024-10-22,2024-10-24
PODTILE - Facilitating Podcast Episode Browsing with Auto-generated Chapters,https://arxiv.org/abs/2410.16148,2024-10-22,2024-10-24
"1-bit AI Infra - Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs",https://arxiv.org/abs/2410.16144,2024-10-22,2024-10-24
CartesianMoE - Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts,https://arxiv.org/abs/2410.16077,2024-10-22,2024-10-24
Generalized Probabilistic Attention Mechanism in Transformers,https://arxiv.org/abs/2410.15578,2024-10-22,2024-10-24
Economic Anthropology in the Era of Generative Artificial Intelligence,https://arxiv.org/abs/2410.15238,2024-10-22,2024-10-24
Adversarial Training - A Survey,https://arxiv.org/abs/2410.15042,2024-10-22,2024-10-24
Decomposing The Dark Matter of Sparse Autoencoders,https://arxiv.org/abs/2410.14670,2024-10-22,2024-10-24
How Does Data Diversity Shape the Weight Landscape of Neural Networks?,https://arxiv.org/abs/2410.14602,2024-10-22,2024-10-24
MomentumSMoE - Integrating Momentum into Sparse Mixture of Experts,https://arxiv.org/abs/2410.14574,2024-10-22,2024-10-24
A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference,https://arxiv.org/abs/2410.14442,2024-10-22,2024-10-24
MoDification - Mixture of Depths Made Easy,https://arxiv.org/abs/2410.14268,2024-10-22,2024-10-24
On the Sparsity of the Strong Lottery Ticket Hypothesis,https://arxiv.org/abs/2410.14754,2024-10-22,2024-10-24
Montessori-Instruct - Generate Influential Training Data Tailored for Student Learning,https://arxiv.org/abs/2410.14208,2024-10-22,2024-10-24
Speciesism in Natural Language Processing Research,https://arxiv.org/abs/2410.14194,2024-10-22,2024-10-24
Beyond Autoregression - Discrete Diffusion for Complex Reasoning and Planning,https://arxiv.org/abs/2410.14157,2024-10-22,2024-10-24
Addition is All You Need for Energy-efficient Language Models,https://arxiv.org/abs/2410.00907,2024-10-01,2024-10-27
Looking Inward - Language Models Can Learn About Themselves by Introspection,https://arxiv.org/abs/2410.13787,2024-10-17,2024-10-27
Differential Informed Auto-Encoder,https://arxiv.org/abs/2410.18593,2024-10-24,2024-10-29
LEGO - Language Model Building Blocks,https://arxiv.org/abs/2410.18287,2024-10-24,2024-10-29
Beyond position - how rotary embeddings shape representations and memory in autoregressive transfomers,https://arxiv.org/abs/2410.18067,2024-10-24,2024-10-29
Inferring stability properties of chaotic systems on autoencoders' latent spaces,https://arxiv.org/abs/2410.18003,2024-10-24,2024-10-29
Stick-breaking Attention,https://arxiv.org/abs/2410.17980,2024-10-24,2024-10-29
ExpertFlow - Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference,https://arxiv.org/abs/2410.17954,2024-10-24,2024-10-29
Future Token Prediction -- Causal Language Modelling with Per-Token Semantic State Vector for Multi-Token Prediction,https://arxiv.org/abs/2410.18160,2024-10-24,2024-10-29
Value Residual Learning For Alleviating Attention Concentration In Transformers,https://arxiv.org/abs/2410.17897,2024-10-24,2024-10-29
Scaling Diffusion Language Models via Adaptation from Autoregressive Models,https://arxiv.org/abs/2410.17891,2024-10-24,2024-10-29
Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition,https://arxiv.org/abs/2410.17765,2024-10-24,2024-10-29
Beyond Backpropagation - Optimization with Multi-Tangent Forward Gradients,https://arxiv.org/abs/2410.17764,2024-10-24,2024-10-29
Dreaming Learning,https://arxiv.org/abs/2410.18156,2024-10-24,2024-10-29
Towards a Similarity-adjusted Surprisal Theory,https://arxiv.org/abs/2410.17676,2024-10-24,2024-10-29
LMLPA - Language Model Linguistic Personality Assessment,https://arxiv.org/abs/2410.17632,2024-10-24,2024-10-29
LayerSkip - Enabling Early Exit Inference and Self-Speculative Decoding,https://arxiv.org/pdf/2404.16710,2024-04-25,2024-10-30
O1 Replication Journey - A Strategic Progress Report -- Part 1,https://arxiv.org/abs/2410.18982,2024-10-08,2024-10-30
Sparse Crosscoders for Cross-Layer Features and Model Diffing,https://transformer-circuits.pub/2024/crosscoders/index.html,2024-10-30,2024-10-30
Using Dictionary Learning Features as Classifiers,https://transformer-circuits.pub/2024/features-as-classifiers/index.html,2024-10-30,2024-10-30
Circuits Updates - September 2024,https://transformer-circuits.pub/2024/september-update/index.html,2024-10-30,2024-10-30
Circuits Updates - August 2024,https://transformer-circuits.pub/2024/august-update/index.html,2024-10-30,2024-10-30
Large Language Models Reflect the Ideology of their Creators,https://arxiv.org/abs/2410.18417,2024-10-24,2024-10-30
The Geometry of Concepts - Sparse Autoencoder Feature Structure,https://arxiv.org/abs/2410.19750,2024-10-10,2024-11-01
Length-Induced Embedding Collapse in Transformer-based Models,https://arxiv.org/abs/2410.24200,2024-10-31,2024-11-01
Thought Space Explorer - Navigating and Expanding Thought Space for Large Language Model Reasoning,https://arxiv.org/abs/2410.24155,2024-10-31,2024-11-01
A Visual Case Study of the Training Dynamics in Neural Networks,https://arxiv.org/abs/2410.24050,2024-10-31,2024-11-01
What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking - A Gradient Perspective,https://arxiv.org/abs/2410.23743,2024-10-31,2024-11-01
Towards Reliable Alignment - Uncertainty-aware RLHF,https://arxiv.org/abs/2410.23726,2024-10-31,2024-11-01
Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers,https://arxiv.org/abs/2410.23684,2024-10-31,2024-11-01
Global Convergence in Training Large-Scale Transformers,https://arxiv.org/abs/2410.23610,2024-10-31,2024-11-01
Tiny Transformers Excel at Sentence Compression,https://arxiv.org/abs/2410.23510,2024-10-31,2024-11-01
All or None - Identifiable Linear Properties of Next-token Predictors in Language Modeling,https://arxiv.org/abs/2410.23501,2024-10-31,2024-11-01
Social Science Meets LLMs - How Reliable Are Large Language Models in Social Simulations?,https://arxiv.org/abs/2410.23426,2024-10-31,2024-11-01
TokenFormer - Rethinking Transformer Scaling with Tokenized Model Parameters,https://arxiv.org/abs/2410.23168,2024-10-31,2024-11-01
Toward Understanding In-context vs. In-weight Learning,https://arxiv.org/abs/2410.23042,2024-10-31,2024-11-01
Abrupt Learning in Transformers - A Case Study on Matrix Completion,https://arxiv.org/abs/2410.22244,2024-10-31,2024-11-01
Diffusion as Reasoning - Enhancing Object Goal Navigation with LLM-Biased Diffusion Model,https://arxiv.org/abs/2410.21842,2024-10-31,2024-11-01
How Does Critical Batch Size Scale in Pre-training?,https://arxiv.org/abs/2410.21676,2024-10-31,2024-11-02
MultiTok - Variable-Length Tokenization for Efficient LLMs Adapted from LZW Compression,https://arxiv.org/abs/2410.21548,2024-10-31,2024-11-02
L3Ms -- Lagrange Large Language Models,https://arxiv.org/abs/2410.21533,2024-10-31,2024-11-02
Moral Agency in Silico - Exploring Free Will in Large Language Models,https://arxiv.org/abs/2410.23310,2024-10-31,2024-11-02
Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups,https://arxiv.org/abs/2410.21508,2024-10-31,2024-11-02
Modular Duality in Deep Learning,https://arxiv.org/abs/2410.21265,2024-10-31,2024-11-02
LoRA vs Full Fine-tuning - An Illusion of Equivalence,https://arxiv.org/abs/2410.21228,2024-10-31,2024-11-02
MrT5 - Dynamic Token Merging for Efficient Byte-level Language Models,https://arxiv.org/abs/2410.20771,2024-10-31,2024-11-02
Matryoshka - Learning to Drive Black-Box LLMs with LLMs,https://arxiv.org/abs/2410.20749,2024-10-31,2024-11-02
Relaxed Recursive Transformers - Effective Parameter Sharing with Layer-wise LoRA,https://arxiv.org/abs/2410.20672,2024-10-31,2024-11-02
Diffusion Forcing - Next-token Prediction Meets Full-Sequence Diffusion,https://arxiv.org/abs/2407.01392,2024-07-01,2024-11-02
Decentralized Federated Learning with Gradient Tracking over Time-Varying Directed Networks,https://arxiv.org/abs/2409.17189,2024-09-25,2024-11-02
Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism,https://arxiv.org/abs/2409.17625,2024-09-26,2024-11-02
AutoTrain - No-code training for state-of-the-art models,https://arxiv.org/abs/2410.15735v1,2024-10-21,2024-11-02
EconoJax - A Fast & Scalable Economic Simulation in Jax,https://arxiv.org/abs/2410.22165v1,2024-10-29,2024-11-02
Analyzing Multi-Stage Loss Curve - Plateau and Descent Mechanisms in Neural Networks,https://arxiv.org/abs/2410.20119v1,2024-10-26,2024-11-02
KVSharer - Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing,https://arxiv.org/abs/2410.18517,2024-10-24,2024-11-02
softmax is not enough (for sharp out-of-distribution),https://arxiv.org/abs/2410.01104,2024-10-01,2024-11-02
Physics in Next-token Prediction,https://arxiv.org/abs/2411.00660,2024-11-01,2024-11-05
How Far is Video Generation from World Model - A Physical Law Perspective,https://arxiv.org/abs/2411.02385,2024-11-04,2024-11-05
Collective Model Intelligence Requires Compatible Specialization,https://arxiv.org/abs/2411.02207,2024-11-04,2024-11-05
"Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models",https://arxiv.org/abs/2411.02083,2024-11-04,2024-11-05
"Ask, and it shall be given - Turing completeness of prompting",https://arxiv.org/abs/2411.01992,2024-11-04,2024-11-05
Understanding Variational Autoencoders with Intrinsic Dimension and Information Imbalance,https://arxiv.org/abs/2411.01978,2024-11-04,2024-11-05
Can Language Models Learn to Skip Steps?,https://arxiv.org/abs/2411.01855,2024-11-04,2024-11-05
Context Parallelism for Scalable Million-Token Inference,https://arxiv.org/abs/2411.01783,2024-11-04,2024-11-05
Unlocking the Theory Behind Scaling 1-Bit Neural Networks,https://arxiv.org/abs/2411.01663,2024-11-04,2024-11-05
Data movement limits to frontier model training,https://arxiv.org/abs/2411.01137,2024-11-04,2024-11-05
Adapting Language Models via Token Translation,https://arxiv.org/abs/2411.00593,2024-11-04,2024-11-05
Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset,https://arxiv.org/abs/2411.04034,2024-11-06,2024-11-06
Do Mice Grok? Glimpses of Hidden Progress During Overtraining in Sensory Cortex,https://arxiv.org/abs/2411.03541,2024-11-06,2024-11-06
Enhancing Transformer Training Efficiency with Dynamic Dropout,https://arxiv.org/abs/2411.03236,2024-11-06,2024-11-06
Photon - Federated LLM Pre-Training,https://arxiv.org/abs/2411.02908,2024-11-06,2024-11-06
DroidSpeak - Enhancing Cross-LLM Communication,https://arxiv.org/abs/2411.02820,2024-11-06,2024-11-06
The Evolution of RWKV - Advancements in Efficient Language Modeling,https://arxiv.org/abs/2411.02795,2024-11-06,2024-11-06
A Survey of Small Language Models,https://arxiv.org/abs/2410.20011,2024-10-25,2024-11-07
MobileLLM - Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,https://arxiv.org/pdf/2402.14905,2024-02-22,2024-11-07
Mixture-of-Transformers - A Sparse and Scalable Architecture for Multi-Modal Foundation Models,https://arxiv.org/abs/2411.04996,2024-11-07,2024-11-08
Which bits went where? Past and future transfer entropy decomposition with the information bottleneck,https://arxiv.org/abs/2411.04992,2024-11-07,2024-11-08
The Semantic Hub Hypothesis - Language Models Share Semantic Representations Across Languages and Modalities,https://arxiv.org/abs/2411.04986,2024-11-07,2024-11-08
BitNet a4.8 - 4-bit Activations for 1-bit LLMs,https://arxiv.org/abs/2411.04965,2024-11-07,2024-11-08
OpenCoder - The Open Cookbook for Top-Tier Code Large Language Models,https://arxiv.org/abs/2411.04905,2024-11-07,2024-11-08
GUI Agents with Foundation Models - A Comprehensive Survey,https://arxiv.org/abs/2411.04890,2024-11-07,2024-11-08
Peri-midFormer - Periodic Pyramid Transformer for Time Series Analysis,https://arxiv.org/abs/2411.04554,2024-11-07,2024-11-08
Gradient Localization Improves Lifelong Pretraining of Language Models,https://arxiv.org/abs/2411.04448,2024-11-07,2024-11-08
Scaling Laws for Pre-training Agents and World Models,https://arxiv.org/abs/2411.04434,2024-11-07,2024-11-08
LLMPhy - Complex Physical Reasoning Using Large Language Models and World Models,https://arxiv.org/abs/2411.08027,2024-11-12,2024-11-13
Language Models as Causal Effect Generators,https://arxiv.org/abs/2411.08019,2024-11-12,2024-11-13
Gini Coefficient as a Unified Metric for Evaluating Many-versus-Many Similarity in Vector Spaces,https://arxiv.org/abs/2411.07983,2024-11-12,2024-11-13
Trustful LLMs - Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders,https://arxiv.org/abs/2411.07870,2024-11-12,2024-11-13
Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices,https://arxiv.org/abs/2411.07826,2024-11-12,2024-11-13
Unraveling the Gradient Descent Dynamics of Transformers,https://arxiv.org/abs/2411.07538,2024-11-12,2024-11-13
LAUREL - Learned Augmented Residual Layer,https://arxiv.org/abs/2411.07501,2024-11-12,2024-11-13
Warmstarting for Scaling Language Models,https://arxiv.org/abs/2411.07340,2024-11-12,2024-11-13
The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,https://arxiv.org/abs/2411.07279,2024-11-12,2024-11-13
TreeCoders - Trees of Transformers,https://arxiv.org/abs/2411.07218,2024-11-12,2024-11-13
Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis,https://arxiv.org/abs/2411.07132,2024-11-12,2024-11-13
Token2Wave,https://arxiv.org/abs/2411.06989,2024-11-12,2024-11-13
On the Limits of Language Generation - Trade-Offs Between Hallucination and Mode Collapse,https://arxiv.org/abs/2411.09642,2024-11-14,2024-11-17
Local-Global Attention - An Adaptive Mechanism for Multi-Scale Feature Integration,https://arxiv.org/abs/2411.09604,2024-11-14,2024-11-17
ResidualDroppath - Enhancing Feature Reuse over Residual Connections,https://arxiv.org/abs/2411.09475,2024-11-14,2024-11-17
Multi-scale Generative Modeling for Fast Sampling,https://arxiv.org/abs/2411.09356,2024-11-14,2024-11-17
Artificial Theory of Mind and Self-Guided Social Organisation,https://arxiv.org/abs/2411.09169,2024-11-14,2024-11-17
Theory of Mind Enhances Collective Intelligence,https://arxiv.org/abs/2411.09168,2024-11-14,2024-11-17
Cut Your Losses in Large-Vocabulary Language Models,https://arxiv.org/abs/2411.09009,2024-11-14,2024-11-17
Theoretical Analysis of Byte-Pair Encoding,https://arxiv.org/abs/2411.08671,2024-11-14,2024-11-17
Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,https://arxiv.org/abs/2411.08324,2024-11-14,2024-11-17
Tackling Polysemanticity with Neuron Embeddings,https://arxiv.org/abs/2411.08166,2024-11-14,2024-11-17
Emergent field theories from neural networks,https://arxiv.org/abs/2411.08138,2024-11-14,2024-11-17
"Deep Learning 2.0 - Artificial Neurons That Matter -- Reject Correlation, Embrace Orthogonality",https://arxiv.org/abs/2411.08085,2024-11-14,2024-11-17
Large Language Models as Neurolinguistic Subjects - Identifying Internal Representations for Form and Meaning,https://arxiv.org/abs/2411.07533,2024-11-14,2024-11-17
The Super Weight in Large Language Models,https://arxiv.org/abs/2411.07191,2024-11-14,2024-11-17
Meta-Chunking - Learning Efficient Text Segmentation via Logical Perception,https://www.arxiv.org/abs/2410.12788v1,2024-10-16,2024-11-22
Mixture-of-Transformers - A Sparse and Scalable Architecture for Multi-Modal Foundation Models,https://arxiv.org/abs/2411.04996,2024-11-07,2024-11-23
LLM2CLIP - Powerful Language Model Unlocks Richer Visual Representation,https://arxiv.org/abs/2411.04997,2024-11-07,2024-11-23
LLaVA-o1 - Let Vision Language Models Reason Step-by-Step,https://arxiv.org/abs/2411.10440,2024-11-15,2024-11-23
Adding Error Bars to Evals - A Statistical Approach to Language Model Evaluations,https://arxiv.org/abs/2411.00640,2024-11-01,2024-11-23
Marco-o1 - Towards Open Reasoning Models for Open-Ended Solutions,https://arxiv.org/abs/2411.14405,2024-11-21,2024-11-23
Multimodal Autoregressive Pre-training of Large Vision Encoders,https://arxiv.org/abs/2411.14402,2024-11-21,2024-11-23
Meta-Chunking - Learning Efficient Text Segmentation via Logical Perception,https://www.arxiv.org/abs/2410.12788v1,2024-10-16,2024-11-23
Art-Free Generative Models - Art Creation Without Graphic Art Knowledge,https://arxiv.org/abs/2412.00176,2024-11-29,2024-12-05
STAR - Synthesis of Tailored Architectures,https://arxiv.org/abs/2411.17800,2024-11-26,2024-12-05
Hymba - A Hybrid-head Architecture for Small Language Models,https://arxiv.org/abs/2411.13676,2024-11-20,2024-12-05
LLaVA-CoT - Let Vision Language Models Reason Step-by-Step,https://www.arxiv.org/abs/2411.10440v1,2024-11-15,2024-12-05
Star Attention - Efficient LLM Inference over Long Sequences,https://www.arxiv.org/abs/2411.17116v1,2024-11-26,2024-12-05
"Generative Agent Simulations of 1,000 People",https://www.arxiv.org/abs/2411.10109v1,2024-11-15,2024-12-05
TÜLU 3 - Pushing Frontiers in Open Language Model Post-Training,https://arxiv.org/abs/2411.15124,2024-11-22,2024-12-05
DeMo - Decoupled Momentum Optimization,https://arxiv.org/abs/2411.19870,2024-11-29,2024-12-05
Analogical Reasoning Within a Conceptual Hyperspace,https://arxiv.org/abs/2411.08684,2024-11-13,2024-12-05
Large Language Models Can Self-Improve in Long-context Reasoning,https://arxiv.org/abs/2411.08147,2024-11-12,2024-12-05
What Representational Similarity Measures Imply about Decodable Information,https://arxiv.org/abs/2411.08197,2024-11-12,2024-12-05
Are LLMs Prescient - A Continuous Evaluation using Daily News as the Oracle,https://arxiv.org/abs/2411.08324,2024-11-13,2024-12-05
Searching Latent Program Spaces,https://arxiv.org/abs/2411.08706,2024-11-13,2024-12-05
"OML - Open, Monetizable, and Loyal AI",https://arxiv.org/abs/2411.03887,2024-11-01,2024-12-05
China and the U.S. produce more impactful AI research when collaborating together,https://arxiv.org/abs/2304.11123,2023-04-21,2024-12-05
LAuReL - Learned Augmented Residual Layer,https://arxiv.org/abs/2411.07501,2024-11-12,2024-12-05
Comment on Is Complexity an Illusion -,https://arxiv.org/abs/2411.08897,2024-10-29,2024-12-05
Multi-scale Generative Modeling for Fast Sampling,https://arxiv.org/abs/2411.09356,2024-11-14,2024-12-05
Theory of Mind Enhances Collective Intelligence,https://arxiv.org/abs/2411.09168,2024-11-14,2024-12-05
Artificial Theory of Mind and Self-Guided Social Organisation,https://arxiv.org/abs/2411.09169,2024-11-14,2024-12-05
ResidualDroppath - Enhancing Feature Reuse over Residual Connections,https://arxiv.org/abs/2411.09475,2024-11-14,2024-12-05
Beyond the Doors of Perception - Vision Transformers Represent Relations Between Objects,https://arxiv.org/abs/2406.15955,2024-06-22,2024-12-05
Semantics and Spatiality of Emergent Communication,https://arxiv.org/abs/2411.10173,2024-11-15,2024-12-05
Orca - Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits,https://arxiv.org/abs/2411.10006,2024-11-15,2024-12-05
Interpolating neural network - A novel unification of machine learning and interpolation theory,https://arxiv.org/abs/2404.10296,2024-04-16,2024-12-05
DiffLoRA - Generating Personalized Low-Rank Adaptation Weights with Diffusion,https://arxiv.org/abs/2408.06740,2024-08-13,2024-12-05
Pluralistic Alignment Over Time,https://arxiv.org/abs/2411.10654,2024-11-16,2024-12-05
Being Considerate as a Pathway Towards Pluralistic Alignment for Agentic AI,https://arxiv.org/abs/2411.10613,2024-11-15,2024-12-05
DEBUG-HD - Debugging TinyML models on-device using Hyper-Dimensional computing,https://arxiv.org/abs/2411.10692,2024-11-16,2024-12-05
SageAttention2 Technical Report - Accurate 4 Bit Attention for Plug-and-play Inference Acceleration,https://arxiv.org/abs/2411.10958,2024-11-17,2024-12-05
MoE-Lightning - High-Throughput MoE Inference on Memory-constrained GPUs,https://arxiv.org/abs/2411.11217,2024-11-18,2024-12-05
Conversational Medical AI - Ready for Practice,https://arxiv.org/abs/2411.12808,2024-11-19,2024-12-05
"A Survey of Financial AI - Architectures, Advances and Open Challenges",https://arxiv.org/abs/2411.12747,2024-11-01,2024-12-05
Logic Augmented Generation,https://arxiv.org/abs/2411.14012,2024-11-21,2024-12-05
RV4Chatbot - Are Chatbots Allowed to Dream of Electric Sheep -,https://arxiv.org/abs/2411.14368,2024-11-21,2024-12-05
Hymba - A Hybrid-head Architecture for Small Language Models,https://arxiv.org/abs/2411.13676,2024-11-20,2024-12-05
Natural Language Reinforcement Learning,https://arxiv.org/abs/2411.14251,2024-11-21,2024-12-05
Associative Knowledge Graphs for Efficient Sequence Storage and Retrieval,https://arxiv.org/abs/2411.14480,2024-11-19,2024-12-05
Improving training time and GPU utilization in geo-distributed language model training,https://arxiv.org/abs/2411.14458,2024-11-16,2024-12-05
Understanding World or Predicting Future - A Comprehensive Survey of World Models,https://arxiv.org/abs/2411.14499,2024-11-21,2024-12-05
Planning-Driven Programming - A Large Language Model Programming Workflow,https://arxiv.org/abs/2411.14503,2024-11-21,2024-12-05
FuseGPT - Learnable Layers Fusion of Generative Pre-trained Transformers,https://arxiv.org/abs/2411.14507,2024-11-21,2024-12-05
Comparative Analysis of Pooling Mechanisms in LLMs - A Sentiment Analysis Perspective,https://arxiv.org/abs/2411.14654,2024-11-22,2024-12-05
LIBER - Lifelong User Behavior Modeling Based on Large Language Models,https://arxiv.org/abs/2411.14713,2024-11-22,2024-12-05
The Zamba2 Suite - Technical Report,https://arxiv.org/abs/2411.15242,2024-11-22,2024-12-05
Bio-inspired AI - Integrating Biological Complexity into Artificial Intelligence,https://arxiv.org/abs/2411.15243,2024-11-22,2024-12-05
Soft-TransFormers for Continual Learning,https://arxiv.org/abs/2411.16073,2024-11-25,2024-12-05
The brain versus AI - World-model-based versatile circuit computation underlying diverse functions in the neocortex and cerebellum,https://arxiv.org/abs/2411.16075,2024-11-25,2024-12-05
Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability,https://arxiv.org/abs/2411.16105,2024-11-25,2024-12-05
One Diffusion to Generate Them All,https://arxiv.org/abs/2411.16318,2024-11-25,2024-12-05
"Brain-like emergent properties in deep networks - impact of network architecture, datasets and training",https://arxiv.org/abs/2411.16326,2024-11-25,2024-12-05
"The Two-Hop Curse - LLMs trained on A- -B, B- -C fail to learn A-- -C",https://arxiv.org/abs/2411.16353,2024-11-25,2024-12-05
"O1 Replication Journey -- Part 2 - Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson -",https://arxiv.org/abs/2411.16489,2024-11-25,2024-12-05
Meaningless is better - hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning,https://arxiv.org/abs/2411.17304,2024-11-26,2024-12-05
SoK - Decentralized AI (DeAI),https://arxiv.org/abs/2411.17461,2024-11-26,2024-12-05
Neural Networks Use Distance Metrics,https://arxiv.org/abs/2411.17932,2024-11-26,2024-12-05
Disentangling Memory and Reasoning Ability in Large Language Models,https://arxiv.org/abs/2411.13504v2,2024-11-20,2024-12-05
Geometry of fibers of the multiplication map of deep linear neural networks,https://arxiv.org/abs/2411.19920,2024-11-29,2024-12-05
Context-Aware Multimodal Pretraining,https://arxiv.org/abs/2411.15099,2024-11-22,2024-12-05
Reinforcement Learning - An Overview,https://arxiv.org/abs/2412.05265,2024-12-06,2024-12-09
100% Hallucination Elimination Using Acurai,https://arxiv.org/abs/2412.05223,2024-12-06,2024-12-09
Transformers Can Navigate Mazes With Multi-Step Prediction,https://arxiv.org/abs/2412.05117,2024-12-06,2024-12-09
Adaptive Dropout for Pruning Conformers,https://arxiv.org/abs/2412.04836,2024-12-06,2024-12-09
Latent Space Characterization of Autoencoder Variants,https://arxiv.org/abs/2412.04755,2024-12-06,2024-12-09
Transformers Struggle to Learn to Search,https://arxiv.org/abs/2412.04703,2024-12-06,2024-12-09
ARC Prize 2024 - Technical Report,https://arxiv.org/abs/2412.04604,2024-12-06,2024-12-09
Dissociating Artificial Intelligence from Artificial Consciousness,https://arxiv.org/abs/2412.04571,2024-12-06,2024-12-09
Artificial intelligence and the internal processes of creativity,https://arxiv.org/abs/2412.04366,2024-12-06,2024-12-09
Densing Law of LLMs,https://arxiv.org/abs/2412.04315,2024-12-06,2024-12-09
The Hyperfitting Phenomenon - Sharpening and Stabilizing LLMs for Open-Ended Text Generation,https://arxiv.org/abs/2412.04318,2024-12-06,2024-12-09
Augmenting Minds or Automating Skills - The Differential Role of Human Capital in Generative AI's Impact on Creative Tasks,https://arxiv.org/abs/2412.03963,2024-12-06,2024-12-09
A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios,https://arxiv.org/abs/2412.03920,2024-12-06,2024-12-09
"What Do Machine Learning Researchers Mean by ""Reproducible""?",https://arxiv.org/abs/2412.03854,2024-12-06,2024-12-09
From Language Models over Tokens to Language Models over Characters,https://arxiv.org/abs/2412.03719,2024-12-06,2024-12-09
ParetoFlow - Guided Flows in Multi-Objective Optimization,https://arxiv.org/abs/2412.03718,2024-12-06,2024-12-09
Good practices for evaluation of machine learning systems,https://arxiv.org/abs/2412.03700,2024-12-06,2024-12-09
Navigation World Models,https://arxiv.org/abs/2412.03572,2024-12-06,2024-12-09
Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning,https://arxiv.org/abs/2412.03343,2024-12-06,2024-12-09
FlashAttention on a Napkin - A Diagrammatic Approach to Deep Learning IO-Awareness,https://arxiv.org/abs/2412.03317,2024-12-06,2024-12-09
AIM - Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning,https://arxiv.org/abs/2412.03248,2024-12-06,2024-12-09
Controlling the Mutation in Large Language Models for the Efficient Evolution of Algorithms,https://arxiv.org/abs/2412.03250,2024-12-06,2024-12-09
"Survey of different Large Language Model Architectures - Trends, Benchmarks, and Challenges",https://arxiv.org/abs/2412.03220,2024-12-06,2024-12-09
Byte BPE Tokenization as an Inverse string Homomorphism,https://arxiv.org/abs/2412.03160,2024-12-06,2024-12-09
Large Language Models show both individual and collective creativity comparable to humans,https://arxiv.org/abs/2412.03151,2024-12-06,2024-12-09
A surprisal oracle for when every layer counts,https://arxiv.org/abs/2412.03098,2024-12-06,2024-12-09
Theoretical limitations of multi-layer Transformer,https://arxiv.org/abs/2412.02975,2024-12-06,2024-12-09
Inverse Delayed Reinforcement Learning,https://arxiv.org/abs/2412.02931,2024-12-06,2024-12-09
Is Large-Scale Pretraining the Secret to Good Domain Generalization?,https://arxiv.org/abs/2412.02856,2024-12-06,2024-12-09
Batch Normalization Decomposed,https://arxiv.org/abs/2412.02843,2024-12-06,2024-12-09
An Evolutionary Large Language Model for Hallucination Mitigation,https://arxiv.org/abs/2412.02790,2024-12-06,2024-12-09
The Asymptotic Behavior of Attention in Transformers,https://arxiv.org/abs/2412.02682,2024-12-06,2024-12-09
Scaling Image Tokenizers with Grouped Spherical Quantization,https://arxiv.org/abs/2412.02632,2024-12-06,2024-12-09
Time-Reversal Provides Unsupervised Feedback to LLMs,https://arxiv.org/abs/2412.02626,2024-12-06,2024-12-09
Nemotron-CC - Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset,https://arxiv.org/abs/2412.02595,2024-12-06,2024-12-09
Fractional Order Distributed Optimization,https://arxiv.org/abs/2412.02546,2024-12-06,2024-12-09
What should a neuron aim for? Designing local objective functions based on information theory,https://arxiv.org/abs/2412.02482,2024-12-06,2024-12-09
Sustainable Self-evolution Adversarial Training,https://arxiv.org/abs/2412.02270,2024-12-06,2024-12-09
Privacy-Preserving Federated Learning via Homomorphic Adversarial Networks,https://arxiv.org/abs/2412.01650,2024-12-06,2024-12-09
Review of Mathematical Optimization in Federated Learning,https://arxiv.org/abs/2412.01630,2024-12-06,2024-12-09
Multi-objective Deep Learning - Taxonomy and Survey of the State of the Art,https://arxiv.org/abs/2412.01566,2024-12-06,2024-12-09
Early Exit Is a Natural Capability in Transformer-based Models - An Empirical Study on Early Exit without Joint Optimization,https://arxiv.org/abs/2412.01455,2024-12-06,2024-12-09
Bio-Inspired Adaptive Neurons for Dynamic Weighting in Artificial Neural Networks,https://arxiv.org/abs/2412.01454,2024-12-06,2024-12-09
Learning Elementary Cellular Automata with Transformers,https://arxiv.org/abs/2412.01417,2024-12-06,2024-12-09
Hierarchical VAE with a Diffusion-based VampPrior,https://arxiv.org/abs/2412.01373,2024-12-06,2024-12-09
An overview of diffusion models for generative artificial intelligence,https://arxiv.org/abs/2412.01371,2024-12-06,2024-12-09
TinyFusion - Diffusion Transformers Learned Shallow,https://arxiv.org/abs/2412.01199,2024-12-06,2024-12-09
AI Benchmarks and Datasets for LLM Evaluation,https://arxiv.org/abs/2412.01020,2024-12-06,2024-12-09
Large Language Models as Mirrors of Societal Moral Standards,https://arxiv.org/abs/2412.00956,2024-12-06,2024-12-09
Playable Game Generation,https://arxiv.org/abs/2412.00887,2024-12-06,2024-12-09
The Advancement of Personalized Learning Potentially Accelerated by Generative AI,https://arxiv.org/abs/2412.00691,2024-12-06,2024-12-09
Homeostazis and Sparsity in Transformer,https://arxiv.org/abs/2412.00503,2024-12-06,2024-12-09
LMSeg - Unleashing the Power of Large-Scale Models for Open-Vocabulary Semantic Segmentation,https://arxiv.org/abs/2412.00364,2024-12-06,2024-12-09
Does Self-Attention Need Separate Weights in Transformers?,https://arxiv.org/abs/2412.00359,2024-12-06,2024-12-09
Not All Language Model Features Are Linear,https://arxiv.org/abs/2405.14860,2024-05-23,2024-12-11
Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations,https://arxiv.org/abs/2408.10920,2024-08-20,2024-12-11
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-12-11
Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://arxiv.org/abs/1503.00075,2015-02-28,2024-12-11
Visual Autoregressive Modeling - Scalable Image Generation via Next-Scale Prediction,https://arxiv.org/abs/2404.02905,2024-04-03,2024-12-11
Large Concept Models - Language Modeling in a Sentence Representation Space,https://arxiv.org/abs/2412.08821,2024-12-11,2024-12-18
Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture,https://arxiv.org/abs/2412.15113,2024-12-19,2024-12-20
Why language models collapse when trained on recursively generated text,https://arxiv.org/abs/2412.14872,2024-12-19,2024-12-20
A Survey of RWKV,https://arxiv.org/abs/2412.14847,2024-12-19,2024-12-20
Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning,https://arxiv.org/abs/2412.14780,2024-12-19,2024-12-20
ALKAFI-LLAMA3 - Fine-Tuning LLMs for Precise Legal Understanding in Palestine,https://arxiv.org/abs/2412.14771,2024-12-19,2024-12-20
AIArena - A Blockchain-Based Decentralized AI Training Platform,https://arxiv.org/abs/2412.14566,2024-12-19,2024-12-20
The Digital Ecosystem of Beliefs - does evolution favour AI over humans?,https://arxiv.org/abs/2412.14500,2024-12-19,2024-12-20
Experience of Training a 1.7B-Parameter LLaMa Model From Scratch,https://arxiv.org/abs/2412.13335,2024-12-19,2024-12-20
Agnosticism About Artificial Consciousness,https://arxiv.org/abs/2412.13145,2024-12-19,2024-12-20
AI PERSONA - Towards Life-long Personalization of LLMs,https://arxiv.org/abs/2412.13103,2024-12-19,2024-12-20
Falcon - Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree,https://arxiv.org/abs/2412.12639,2024-12-19,2024-12-20
Echo - Simulating Distributed Training At Scale,https://arxiv.org/abs/2412.12487,2024-12-19,2024-12-20
Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat,https://arxiv.org/abs/2412.14205,2024-12-19,2024-12-20
SepLLM - Accelerate Large Language Models by Compressing One Segment into One Separator,https://arxiv.org/abs/2412.12094,2024-12-19,2024-12-20
Inferring Functionality of Attention Heads from their Parameters,https://arxiv.org/abs/2412.11965,2024-12-19,2024-12-20
SoftVQ-VAE - Efficient 1-Dimensional Continuous Tokenizer,https://arxiv.org/abs/2412.10958,2024-12-19,2024-12-20
Exploring Grokking - Experimental and Mechanistic Investigations,https://arxiv.org/abs/2412.10898,2024-12-19,2024-12-20
Zigzag Diffusion Sampling - Diffusion Models Can Self-Improve via Self-Reflection,https://arxiv.org/abs/2412.10891,2024-12-19,2024-12-20
Diffusion Model from Scratch,https://arxiv.org/abs/2412.10824,2024-12-19,2024-12-20
OP-LoRA - The Blessing of Dimensionality,https://arxiv.org/abs/2412.10362,2024-12-19,2024-12-20
Small Language Model as Data Prospector for Large Language Model,https://arxiv.org/abs/2412.09990,2024-12-19,2024-12-20
Simulating Hard Attention Using Soft Attention,https://arxiv.org/abs/2412.09925,2024-12-19,2024-12-20
Byte Latent Transformer - Patches Scale Better Than Tokens,https://arxiv.org/abs/2412.09871,2024-12-19,2024-12-20
The Complexity Dynamics of Grokking,https://arxiv.org/abs/2412.09810,2024-12-19,2024-12-20
Model-diff - A Tool for Comparative Study of Language Models in the Input Space,https://arxiv.org/abs/2412.12177,2024-12-19,2024-12-20
Memory Layers at Scale,https://arxiv.org/abs/2412.09764,2024-12-19,2024-12-20
Does Representation Matter? Exploring Intermediate Layers in Large Language Models,https://arxiv.org/abs/2412.09563,2024-12-19,2024-12-20
Opinion de-polarization of social networks with GNNs,https://arxiv.org/abs/2412.09404,2024-12-19,2024-12-20
Densing Law of LLMs,https://arxiv.org/abs/2412.04315,2024-12-05,2024-12-20
From Language Models over Tokens to Language Models over Characters,https://arxiv.org/abs/2412.03719,2024-12-04,2024-12-20
What Do Machine Learning Researchers Mean by  -Reproducible - -,https://arxiv.org/abs/2412.03854,2024-12-05,2024-12-20
Dissociating Artificial Intelligence from Artificial Consciousness,https://arxiv.org/abs/2412.04571,2024-12-05,2024-12-20
ARC Prize 2024 - Technical Report,https://arxiv.org/abs/2412.04604,2024-12-05,2024-12-20
Black Swan - Abductive and Defeasible Video Reasoning in Unpredictable Events,https://arxiv.org/abs/2412.05725,2024-12-07,2024-12-20
BatchTopK Sparse Autoencoders,https://arxiv.org/abs/2412.06410,2024-12-09,2024-12-20
Artificial Intelligence without Restriction Surpassing Human Intelligence with Probability One - Theoretical Insight into Secrets of the Brain with AI Twins of the Brain,https://arxiv.org/abs/2412.06820,2024-12-04,2024-12-20
"Toward AI-Driven Digital Organism - Multiscale Foundation Models for Predicting, Simulating and Programming Biology at All Levels",https://arxiv.org/abs/2412.06993,2024-12-09,2024-12-20
Superficial Consciousness Hypothesis for Autoregressive Transformers,https://arxiv.org/abs/2412.07278,2024-12-10,2024-12-20
Causal World Representation in the GPT Model,https://arxiv.org/abs/2412.07446,2024-12-10,2024-12-20
Swarm Behavior Cloning,https://arxiv.org/abs/2412.07617,2024-12-10,2024-12-20
Incentivized Symbiosis - A Paradigm for Human-Agent Coevolution,https://arxiv.org/abs/2412.06855,2024-12-08,2024-12-20
Large Language Models - An Applied Econometric Framework,https://arxiv.org/abs/2412.07031,2024-12-09,2024-12-20
Non-Progressive Influence Maximization in Dynamic Social Networks,https://arxiv.org/abs/2412.07402,2024-12-10,2024-12-20
Machines of Meaning,https://arxiv.org/abs/2412.07975,2024-12-10,2024-12-20
Digital Democracy in the Age of Artificial Intelligence,https://arxiv.org/abs/2412.07791,2024-11-26,2024-12-20
LatentSpeech - Latent Diffusion for Text-To-Speech Generation,https://arxiv.org/abs/2412.08117,2024-12-11,2024-12-20
Autoformalizing and Simulating Game-Theoretic Scenarios using LLM-augmented Agents,https://arxiv.org/abs/2412.08805,2024-12-11,2024-12-20
LMAgent - A Large-scale Multimodal Agents Society for Multi-user Simulation,https://arxiv.org/abs/2412.09237,2024-12-12,2024-12-20
BABILong - Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack,https://arxiv.org/abs/2406.10149,2024-06-14,2024-12-20
"Imitate, Explore, and Self-Improve - A Reproduction Report on Slow-thinking Reasoning Systems",https://arxiv.org/abs/2412.09413v1,2024-12-12,2024-12-20
Easy-to-Hard Generalization - Scalable Alignment Beyond Human Supervision,https://arxiv.org/abs/2403.09472,2024-03-14,2024-12-20
Wonderful Matrices - Combining for a More Efficient and Effective Foundation Model Architecture,https://arxiv.org/abs/2412.11834v1,2024-12-16,2024-12-20
Artificial intelligence and the internal processes of creativity,https://arxiv.org/abs/2412.04366v2,2024-12-05,2024-12-20
Densing Law of LLMs,https://arxiv.org/abs/2412.04315v2,2024-12-05,2024-12-20
Mind the Gap - Examining the Self-Improvement Capabilities of Large Language Models,https://www.arxiv.org/abs/2412.02674,2024-12-03,2024-12-20
Training Large Language Models to Reason in a Continuous Latent Space,https://www.arxiv.org/abs/2412.06769,2024-12-09,2024-12-20
Phi-4 Technical Report,https://www.arxiv.org/abs/2412.08905,2024-12-12,2024-12-20
Star Attention - Efficient LLM Inference over Long Sequences,https://www.arxiv.org/abs/2411.17116,2024-11-26,2024-12-20
Byte Latent Transformer - Patches Scale Better Than Tokens,https://www.arxiv.org/abs/2412.09871,2024-12-13,2024-12-20
Flow Matching Guide and Code,https://www.arxiv.org/abs/2412.06264,2024-12-09,2024-12-20
Reverse Thinking Makes LLMs Stronger Reasoners,https://www.arxiv.org/abs/2411.19865,2024-11-29,2024-12-20
Cultural Evolution of Cooperation among LLM Agents,https://www.arxiv.org/abs/2412.10270,2024-12-13,2024-12-20
Large Concept Models - Language Modeling in a Sentence Representation Space,https://www.arxiv.org/abs/2412.08821,2024-12-11,2024-12-20
Reinforcement Learning - An Overview,https://www.arxiv.org/abs/2412.05265,2024-12-06,2024-12-20
Flex Attention - A Programming Model for Generating Optimized Attention Kernels,https://www.arxiv.org/abs/2412.05496,2024-12-07,2024-12-20
Multimodal Latent Language Modeling with Next-Token Diffusion,https://www.arxiv.org/abs/2412.08635,2024-12-11,2024-12-20
No More Adam - Learning Rate Scaling at Initialization is All You Need,https://www.arxiv.org/abs/2412.11768,2024-12-16,2024-12-20
GenEx - Generating an Explorable World,https://www.arxiv.org/abs/2412.09624,2024-12-12,2024-12-20
Are Your LLMs Capable of Stable Reasoning -,https://www.arxiv.org/abs/2412.13147,2024-12-17,2024-12-20
[MASK] is All You Need,https://www.arxiv.org/abs/2412.06787,2024-12-09,2024-12-20
Causal Diffusion Transformers for Generative Modeling,https://www.arxiv.org/abs/2412.12095,2024-12-16,2024-12-20
"Machine Unlearning Doesn't Do What You Think - Lessons for Generative AI Policy, Research, and Practice",https://arxiv.org/abs/2412.06966,2024-12-09,2024-12-20
Superhuman performance of a large language model on the reasoning tasks of a physician,https://arxiv.org/abs/2412.10849,2024-12-14,2024-12-20
Flow Matching Guide and Code,https://arxiv.org/abs/2412.06264,2024-12-09,2024-12-22
Reverse Thinking Makes LLMs Stronger Reasoners,https://www.arxiv.org/abs/2411.19865v1,2024-11-29,2024-12-22
Fine-Tuning Language Models with Just Forward Passes,https://www.arxiv.org/abs/2305.17333v3,2023-05-27,2024-12-22
Training Large Language Models to Reason in a Continuous Latent Space,https://www.arxiv.org/abs/2412.06769,2024-12-09,2024-12-22
Mind the Gap - Examining the Self-Improvement Capabilities of Large Language Models,https://www.arxiv.org/abs/2412.02674,2024-12-03,2024-12-22
"Blending Is All You Need - Cheaper, Better Alternative to Trillion-Parameters LLM",https://arxiv.org/abs/2401.02994,2024-01-04,2024-12-23
A phase transition between positional and semantic learning in a solvable model of dot-product attention,https://arxiv.org/abs/2402.03902,2024-02-06,2024-12-23
MOMENT - A Family of Open Time-series Foundation Models,https://arxiv.org/abs/2402.03885,2024-02-06,2024-12-23
Evolution Transformer - In-Context Evolutionary Optimization,https://arxiv.org/abs/2403.02985,2024-03-05,2024-12-23
ShortGPT - Layers in Large Language Models are More Redundant Than You Expect,https://arxiv.org/abs/2403.03853,2024-03-06,2024-12-23
Training LLMs over Neurally Compressed Text,https://arxiv.org/abs/2404.03626,2024-04-04,2024-12-23
Inheritune - Training Smaller Yet More Attentive Language Models,https://arxiv.org/abs/2404.08634,2024-04-12,2024-12-23
NExT - Teaching Large Language Models to Reason about Code Execution,https://arxiv.org/abs/2404.14662,2024-04-23,2024-12-23
You Only Cache Once - Decoder-Decoder Architectures for Language Models,https://arxiv.org/abs/2405.05254,2024-05-08,2024-12-23
Stacking Your Transformers - A Closer Look at Model Growth for Efficient LLM Pre-Training,https://arxiv.org/abs/2405.15319,2024-05-24,2024-12-23
gzip Predicts Data-dependent Scaling Laws,https://arxiv.org/abs/2405.16684,2024-05-26,2024-12-23
TextGrad - Automatic  -Differentiation - via Text,https://arxiv.org/abs/2406.07496,2024-06-11,2024-12-23
MLKV - Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding,https://arxiv.org/abs/2406.09297,2024-06-13,2024-12-23
Tokenization Falling Short - On Subword Robustness in Large Language Models,https://arxiv.org/abs/2406.11687,2024-06-17,2024-12-23
Diffusion Forcing - Next-token Prediction Meets Full-Sequence Diffusion,https://arxiv.org/abs/2407.01392,2024-07-01,2024-12-23
From GaLore to WeLore - How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients,https://arxiv.org/abs/2407.11239,2024-07-15,2024-12-23
POA - Pre-training Once for Models of All Sizes,https://arxiv.org/abs/2408.01031,2024-08-02,2024-12-23
Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,https://arxiv.org/abs/2408.13233,2024-08-23,2024-12-23
"A Comprehensive Survey of Small Language Models in the Era of Large Language Models - Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness",https://arxiv.org/abs/2411.03350,2024-11-04,2024-12-23
Natural Language Reinforcement Learning,https://arxiv.org/abs/2411.14251,2024-11-21,2024-12-23
Large Multi-modal Models Can Interpret Features in Large Multi-modal Models,https://arxiv.org/abs/2411.14982,2024-11-22,2024-12-23
Low-Bit Quantization Favors Undertrained LLMs - Scaling Laws for Quantized LLMs with 100T Training Tokens,https://arxiv.org/abs/2411.17691,2024-11-26,2024-12-23
Deliberation in Latent Space via Differentiable Cache Augmentation,https://arxiv.org/abs/2412.17747,2024-12-23,2024-12-25
RDPM - Solve Diffusion Probabilistic Models via Recurrent Token Prediction,https://arxiv.org/abs/2412.18390,2024-12-24,2024-12-26
In Case You Missed It - ARC 'Challenge' Is Not That Challenging,https://arxiv.org/abs/2412.17758,2024-12-24,2024-12-26
Tracking the Feature Dynamics in LLM Training - A Mechanistic Study,https://arxiv.org/abs/2412.17626,2024-12-24,2024-12-26
A Survey of Query Optimization in Large Language Models,https://arxiv.org/abs/2412.17558,2024-12-24,2024-12-26
ANID - How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance,https://arxiv.org/abs/2412.17632,2024-12-24,2024-12-26
A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression,https://arxiv.org/abs/2412.17483,2024-12-24,2024-12-26
ORIGAMI - A generative transformer architecture for predictions from semi-structured data,https://arxiv.org/abs/2412.17348,2024-12-24,2024-12-26
Generative Diffusion Modeling - A Practical Handbook,https://arxiv.org/abs/2412.17162,2024-12-24,2024-12-26
Reversed Attention - On The Gradient Descent Of Attention Layers In GPT,https://arxiv.org/abs/2412.17019,2024-12-24,2024-12-26
Assessing Social Alignment - Do Personality-Prompted Large Language Models Behave Like Humans?,https://arxiv.org/abs/2412.16772,2024-12-24,2024-12-26
OpenAI o1 System Card,https://arxiv.org/abs/2412.16720,2024-12-24,2024-12-26
Attention Entropy is a Key Factor - An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models,https://arxiv.org/abs/2412.16545,2024-12-24,2024-12-26
Personalized Representation from Personalized Generation,https://arxiv.org/abs/2412.16156,2024-12-24,2024-12-26
AutoLife - Automatic Life Journaling with Smartphones and LLMs,https://arxiv.org/abs/2412.15714,2024-12-24,2024-12-26
Concept Boundary Vectors,https://arxiv.org/abs/2412.15698,2024-12-24,2024-12-26
Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models,https://arxiv.org/abs/2412.15501,2024-12-24,2024-12-26
Deliberation in Latent Space via Differentiable Cache Augmentation,https://arxiv.org/abs/2412.17747,2024-12-23,2024-12-29
Scaling Laws for Precision,https://arxiv.org/abs/2411.04330,2024-11-07,2024-12-29
Beyond Scaling Laws - Understanding Transformer Performance with Associative Memory,https://arxiv.org/abs/2405.08707,2024-05-14,2024-12-30
Hopfield Networks is All You Need,https://arxiv.org/abs/2008.02217,2020-07-16,2024-12-30
Simple and Scalable Strategies to Continually Pre-train Large Language Models,https://arxiv.org/abs/2403.08763,2024-03-13,2024-12-31
Sparse chaos in cortical circuits,https://arxiv.org/abs/2412.21188,2024-12-30,2025-01-01
Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs,https://arxiv.org/abs/2412.21187,2024-12-30,2025-01-01
Natural Language Fine-Tuning,https://arxiv.org/abs/2412.20382,2024-12-30,2025-01-01
An analytic theory of creativity in convolutional diffusion models,https://arxiv.org/abs/2412.20292,2024-12-30,2025-01-01
"""My life is miserable, have to sign 500 autographs everyday"" - Exposing Humblebragging, the Brags in Disguise",https://arxiv.org/abs/2412.20057,2024-12-30,2025-01-01
Toward Adaptive Reasoning in Large Language Models with Thought Rollback,https://arxiv.org/abs/2412.19707,2024-12-30,2025-01-01
DeepSeek-V3 Technical Report,https://arxiv.org/abs/2412.19437,2024-12-30,2025-01-01
Introduction to Graph Neural Networks - A Starting Point for Machine Learning Engineers,https://arxiv.org/abs/2412.19419,2024-12-30,2025-01-01
Multi-matrix Factorization Attention,https://arxiv.org/abs/2412.19255,2024-12-30,2025-01-01
Optimizing Fantasy Sports Team Selection with Deep Reinforcement Learning,https://arxiv.org/abs/2412.19215,2024-12-30,2025-01-01
Repository Structure-Aware Training Makes SLMs Better Issue Resolver,https://arxiv.org/abs/2412.19031,2024-12-30,2025-01-01
2 OLMo 2 Furious,https://arxiv.org/abs/2501.00656,2024-12-31,2025-01-07
An analytic theory of creativity in convolutional diffusion models,https://arxiv.org/abs/2412.20292,2024-12-28,2025-01-07
Transformer with Fourier Integral Attentions,https://arxiv.org/abs/2206.00206,2022-06-01,2025-01-09
Planarian Neural Networks - Evolutionary Patterns from Basic Bilateria Shaping Modern Artificial Neural Network Architectures,https://arxiv.org/abs/2501.04700,2025-01-08,2025-01-09
Grokking at the Edge of Numerical Stability,https://arxiv.org/abs/2501.04697,2025-01-08,2025-01-09
A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI,https://arxiv.org/abs/2501.04641,2025-01-08,2025-01-09
Federated Fine-Tuning of LLMs - Framework Comparison and Research Directions,https://arxiv.org/abs/2501.04436,2025-01-08,2025-01-09
Lossless Privacy-Preserving Aggregation for Decentralized Federated Learning,https://arxiv.org/abs/2501.04409,2025-01-08,2025-01-09
SEO - Stochastic Experience Optimization for Large Language Models,https://arxiv.org/abs/2501.04393,2025-01-08,2025-01-09
AutoDFL - A Scalable and Automated Reputation-Aware Decentralized Federated Learning,https://arxiv.org/abs/2501.04331,2025-01-08,2025-01-09
VerifBFL - Leveraging zk-SNARKs for A Verifiable Blockchained Federated Learning,https://arxiv.org/abs/2501.04319,2025-01-08,2025-01-09
Mapping the Edge of Chaos - Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models,https://arxiv.org/abs/2501.04286,2025-01-08,2025-01-09
"Fixed Points of Deep Neural Networks - Emergence, Stability, and Applications",https://arxiv.org/abs/2501.04182,2025-01-08,2025-01-09
Symmetry and Generalisation in Machine Learning,https://arxiv.org/abs/2501.03858,2025-01-08,2025-01-09
Cosmos World Foundation Model Platform for Physical AI,https://arxiv.org/abs/2501.03575,2025-01-08,2025-01-09
Neural Cellular Automata and Deep Equilibrium Models,https://arxiv.org/abs/2501.03573,2025-01-08,2025-01-09
Entropy-Guided Attention for Private LLMs,https://arxiv.org/abs/2501.03489,2025-01-08,2025-01-09
Optimization Learning,https://arxiv.org/abs/2501.03443,2025-01-08,2025-01-09
Scalable Forward-Forward Algorithm,https://arxiv.org/abs/2501.03176,2025-01-08,2025-01-09
Deep-Relative-Trust-Based Diffusion for Decentralized Deep Learning,https://arxiv.org/abs/2501.03162,2025-01-08,2025-01-09
Proof-of-Data - A Consensus Protocol for Collaborative Intelligence,https://arxiv.org/abs/2501.02971,2025-01-08,2025-01-09
Key-value memory in the brain,https://arxiv.org/abs/2501.02950,2025-01-08,2025-01-09
Test-time Computing - from System-1 Thinking to System-2 Thinking,https://arxiv.org/abs/2501.02497,2025-01-08,2025-01-09
The Meta-Representation Hypothesis,https://arxiv.org/abs/2501.02481,2025-01-08,2025-01-09
LLMPC - Large Language Model Predictive Control,https://arxiv.org/abs/2501.02486,2025-01-08,2025-01-09
Scaling Laws for Floating Point Quantization Training,https://arxiv.org/abs/2501.02423,2025-01-08,2025-01-09
Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers,https://arxiv.org/abs/2501.02393,2025-01-08,2025-01-09
Syntactic Evolution in Language Usage,https://arxiv.org/abs/2501.02392,2025-01-08,2025-01-09
Tensor-GaLore - Memory-Efficient Training via Gradient Tensor Decomposition,https://arxiv.org/abs/2501.02379,2025-01-08,2025-01-09
Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison,https://arxiv.org/abs/2501.02370,2025-01-08,2025-01-09
Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional Data,https://arxiv.org/abs/2501.02364,2025-01-08,2025-01-09
Learning Evolution via Optimization Knowledge Adaptation,https://arxiv.org/abs/2501.02200,2025-01-08,2025-01-09
Fresh-CL - Feature Realignment through Experts on Hypersphere in Continual Learning,https://arxiv.org/abs/2501.02198,2025-01-08,2025-01-09
REINFORCE++ - A Simple and Efficient Approach for Aligning Large Language Models,https://arxiv.org/abs/2501.03262,2025-01-08,2025-01-09
The Race to Efficiency - A New Perspective on AI Scaling Laws,https://arxiv.org/abs/2501.02156,2025-01-08,2025-01-09
Resolving the Exploration-Exploitation Dilemma in Evolutionary Algorithms - A Novel Human-Centered Framework,https://arxiv.org/abs/2501.02153,2025-01-08,2025-01-09
Practical machine learning is learning on small samples,https://arxiv.org/abs/2501.01836,2025-01-08,2025-01-09
A non-ergodic framework for understanding emergent capabilities in Large Language Models,https://arxiv.org/abs/2501.01638,2025-01-08,2025-01-09
Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts,https://arxiv.org/abs/2501.02009,2025-01-08,2025-01-09
TabTreeFormer - Tabular Data Generation Using Hybrid Tree-Transformer,https://arxiv.org/abs/2501.01216,2025-01-08,2025-01-09
TART - Token-based Architecture Transformer for Neural Network Performance Prediction,https://arxiv.org/abs/2501.02007,2025-01-08,2025-01-09
The Silent Majority - Demystifying Memorization Effect in the Presence of Spurious Correlations,https://arxiv.org/abs/2501.00961,2025-01-08,2025-01-09
Representation in large language models,https://arxiv.org/abs/2501.00885,2025-01-08,2025-01-09
Interactionalism - Re-Designing Higher Learning for the Large Language Agent Era,https://arxiv.org/abs/2501.00867,2025-01-08,2025-01-09
Decoupling Knowledge and Reasoning in Transformers - A Modular Architecture with Generalized Cross-Attention,https://arxiv.org/abs/2501.00823,2025-01-08,2025-01-09
Grade Inflation in Generative Models,https://arxiv.org/abs/2501.00664,2025-01-08,2025-01-09
Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph,https://arxiv.org/abs/2501.00659,2025-01-08,2025-01-09
Superposition in Transformers - A Novel Way of Building Mixture of Experts,https://arxiv.org/abs/2501.00530,2025-01-08,2025-01-09
Proactive Conversational Agents with Inner Thoughts,https://arxiv.org/abs/2501.00383,2025-01-08,2025-01-09
Chunk-Distilled Language Modeling,https://arxiv.org/abs/2501.00343,2025-01-08,2025-01-09
Dual Diffusion for Unified Image Generation and Understanding,https://arxiv.org/abs/2501.00289,2025-01-08,2025-01-09
Generative Emergent Communication - Large Language Model is a Collective World Model,https://arxiv.org/abs/2501.00226,2025-01-08,2025-01-09
Make Domain Shift a Catastrophic Forgetting Alleviator in Class-Incremental Learning,https://arxiv.org/abs/2501.00237,2025-01-08,2025-01-09
Debunking the CUDA Myth Towards GPU-based AI Systems,https://arxiv.org/abs/2501.00210,2025-01-08,2025-01-09
Ring Attention with Blockwise Transformers for Near-Infinite Context,https://arxiv.org/abs/2310.01889,2023-10-03,2025-01-10
Unexpected Benefits of Self-Modeling in Neural Systems,https://arxiv.org/abs/2407.10188,2024-07-14,2025-01-10
Language Models Represent Beliefs of Self and Others,https://arxiv.org/abs/2402.18496,2024-02-28,2025-01-10
Pointer Networks,https://arxiv.org/abs/1506.03134,2015-06-09,2025-01-10
Order Matters - Sequence to sequence for sets,https://arxiv.org/abs/1511.06391,2015-11-19,2025-01-10
GPipe - Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/pdf/1811.06965,2018-11-16,2025-01-10
Multi-Scale Context Aggregation by Dilated Convolutions,https://arxiv.org/pdf/1511.07122,2015-11-23,2025-01-10
Neural Message Passing for Quantum Chemistry,https://arxiv.org/pdf/1704.01212,2017-04-04,2025-01-10
Identity Mappings in Deep Residual Networks,https://arxiv.org/pdf/1603.05027,2016-03-16,2025-01-10
A simple neural network module for relational reasoning,https://arxiv.org/pdf/1706.01427,2017-06-05,2025-01-10
Variational Lossy Autoencoder,https://arxiv.org/pdf/1611.02731,2016-11-08,2025-01-10
Relational recurrent neural networks,https://arxiv.org/pdf/1806.01822,2018-06-05,2025-01-10
Quantifying the Rise and Fall of Complexity in Closed Systems - The Coffee Automaton,https://arxiv.org/pdf/1405.6903,2014-05-27,2025-01-10
Neural Turing Machines,https://arxiv.org/pdf/1410.5401,2014-10-20,2025-01-10
rStar-Math - Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,https://arxiv.org/abs/2501.04519,2025-01-08,2025-01-14
Towards Large Language Models as Copilots for Theorem Proving in Lean,https://arxiv.org/abs/2404.12534,2024-04-18,2025-01-14
GamePad - A Learning Environment for Theorem Proving,https://arxiv.org/abs/1806.00608,2018-06-02,2025-01-14
A Survey on Deep Learning for Theorem Proving,https://arxiv.org/abs/2404.09939,2024-04-15,2025-01-14
TabTreeFormer - Tabular Data Generation Using Hybrid Tree-Transformer,https://www.arxiv.org/abs/2501.01216v2,2025-01-02,2025-01-16
Class-wise Autoencoders Measure Classification Difficulty And Detect Label Mistakes,https://www.arxiv.org/abs/2412.02596v1,2024-12-03,2025-01-16
Deep Differentiable Logic Gate Networks,https://www.arxiv.org/abs/2210.08277v1,2022-10-15,2025-01-16
Social Life Simulation for Non-Cognitive Skills Learning,https://www.arxiv.org/abs/2405.00273v2,2024-05-01,2025-01-16
Agents Are Not Enough,https://www.arxiv.org/abs/2412.16241,2024-12-19,2025-01-16
Towards System 2 Reasoning in LLMs - Learning How to Think With Meta Chain-of-Thought,https://www.arxiv.org/abs/2501.04682,2025-01-08,2025-01-16
rStar-Math - Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,https://arxiv.org/abs/2501.04519,2025-01-08,2025-01-16
NVLM - Open Frontier-Class Multimodal LLMs,https://arxiv.org/abs/2409.11402,2024-09-17,2025-01-16
Titans - Learning to Memorize at Test Time,https://arxiv.org/abs/2501.00663,2024-12-31,2025-01-16
LLM-Based Routing in Mixture of Experts - A Novel Framework for Trading,https://arxiv.org/abs/2501.09636,2025-01-16,2025-01-19
Managed-Retention Memory - A New Class of Memory for the AI Era,https://arxiv.org/abs/2501.09605,2025-01-16,2025-01-19
Overshoot - Taking advantage of future gradients in momentum-based stochastic optimization,https://arxiv.org/abs/2501.09556,2025-01-16,2025-01-19
Perspective Transition of Large Language Models for Solving Subjective Tasks,https://arxiv.org/abs/2501.09265,2025-01-16,2025-01-19
Attention is All You Need Until You Need Retention,https://arxiv.org/abs/2501.09166,2025-01-16,2025-01-19
Is magnitude 'generically continuous' for finite metric spaces?,https://arxiv.org/abs/2501.08745,2025-01-16,2025-01-19
Anthropomorphic Features for On-Line Signatures,https://arxiv.org/abs/2501.09048,2025-01-16,2025-01-19
$\texttt{InfoHier}$ - Hierarchical Information Extraction via Encoding and Embedding,https://arxiv.org/abs/2501.08717,2025-01-16,2025-01-19
A Theory of Optimistically Universal Online Learnability for General Concept Classes,https://arxiv.org/abs/2501.08551,2025-01-16,2025-01-19
Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers,https://arxiv.org/abs/2501.08537,2025-01-16,2025-01-19
Do generative video models learn physical principles from watching videos?,https://arxiv.org/abs/2501.09038,2025-01-16,2025-01-19
Causal vs. Anticausal merging of predictors,https://arxiv.org/abs/2501.08426,2025-01-16,2025-01-19
MiniMax-01 - Scaling Foundation Models with Lightning Attention,https://arxiv.org/abs/2501.08313,2025-01-16,2025-01-19
Inference-Time-Compute - More Faithful? A Research Note,https://arxiv.org/abs/2501.08156,2025-01-16,2025-01-19
Hierarchical Autoscaling for Large Language Model Serving with Chiron,https://arxiv.org/abs/2501.08090,2025-01-16,2025-01-19
GRAPHMOE - Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism,https://arxiv.org/abs/2501.07890,2025-01-16,2025-01-19
A Survey of Early Exit Deep Neural Networks in NLP,https://arxiv.org/abs/2501.07670,2025-01-16,2025-01-19
Investigating Large Language Models in Inferring Personality Traits from User Conversations,https://arxiv.org/abs/2501.07532,2025-01-16,2025-01-19
Data and System Perspectives of Sustainable Artificial Intelligence,https://arxiv.org/abs/2501.07487,2025-01-16,2025-01-19
Attention when you need,https://arxiv.org/abs/2501.07440,2025-01-16,2025-01-19
Emergent effects of scaling on the functional hierarchies within large language models,https://arxiv.org/abs/2501.07359,2025-01-16,2025-01-19
TempoGPT - Enhancing Temporal Reasoning via Quantizing Embedding,https://arxiv.org/abs/2501.07335,2025-01-16,2025-01-19
FinerWeb-10BT - Refining Web Data with LLM-Based Line-Level Filtering,https://arxiv.org/abs/2501.07314,2025-01-16,2025-01-19
Principles for Responsible AI Consciousness Research,https://arxiv.org/abs/2501.07290,2025-01-16,2025-01-19
LLM-Net - Democratizing LLMs-as-a-Service through Blockchain-based Expert Networks,https://arxiv.org/abs/2501.07288,2025-01-16,2025-01-19
A User's Guide to $\texttt{KSig}$ - GPU-Accelerated Computation of the Signature Kernel,https://arxiv.org/abs/2501.07145,2025-01-16,2025-01-19
LLM360 K2 - Building a 65B 360-Open-Source Large Language Model from Scratch,https://arxiv.org/abs/2501.07124,2025-01-16,2025-01-19
How GPT learns layer by layer,https://arxiv.org/abs/2501.07108,2025-01-16,2025-01-19
Leveraging ASIC AI Chips for Homomorphic Encryption,https://arxiv.org/abs/2501.07047,2025-01-16,2025-01-19
The Einstein Test - Towards a Practical Test of a Machine's Ability to Exhibit Superintelligence,https://arxiv.org/abs/2501.06948,2025-01-16,2025-01-19
Causal Claims in Economics,https://arxiv.org/abs/2501.06873,2025-01-16,2025-01-19
Common Sense Is All You Need,https://arxiv.org/abs/2501.06642,2025-01-16,2025-01-19
Synthetic Feature Augmentation Improves Generalization Performance of Language Models,https://arxiv.org/abs/2501.06434,2025-01-16,2025-01-19
Tensor Product Attention Is All You Need,https://arxiv.org/abs/2501.06425,2025-01-16,2025-01-19
"Dynamics of ""Spontaneous"" Topic Changes in Next Token Prediction with Self-Attention",https://arxiv.org/abs/2501.06382,2025-01-16,2025-01-19
xLSTM-SENet - xLSTM for Single-Channel Speech Enhancement,https://arxiv.org/abs/2501.06146,2025-01-16,2025-01-19
Merging Feed-Forward Sublayers for Compressed Transformers,https://arxiv.org/abs/2501.06126,2025-01-16,2025-01-19
"All AI Models are Wrong, but Some are Optimal",https://arxiv.org/abs/2501.06086,2025-01-16,2025-01-19
Element-wise Attention Is All You Need,https://arxiv.org/abs/2501.05730,2025-01-16,2025-01-19
Prediction-Assisted Online Distributed Deep Learning Workload Scheduling in GPU Clusters,https://arxiv.org/abs/2501.05563,2025-01-16,2025-01-19
Soup to go - mitigating forgetting during continual learning with model averaging,https://arxiv.org/abs/2501.05559,2025-01-16,2025-01-19
Emergent weight morphologies in deep neural networks,https://arxiv.org/abs/2501.05550,2025-01-16,2025-01-19
Decentralized Diffusion Models,https://arxiv.org/abs/2501.05450,2025-01-16,2025-01-19
The GAN is dead; long live the GAN! A Modern GAN Baseline,https://arxiv.org/abs/2501.05441,2025-01-16,2025-01-19
Shrink the longest - improving latent space isotropy with symplicial geometry,https://arxiv.org/abs/2501.05502,2025-01-16,2025-01-19
Optimizing Distributed Deployment of Mixture-of-Experts Model Inference in Serverless Computing,https://arxiv.org/abs/2501.05313,2025-01-16,2025-01-19
Distributed Learning and Inference Systems - A Networking Perspective,https://arxiv.org/abs/2501.05323,2025-01-16,2025-01-19
Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning,https://arxiv.org/abs/2501.05205,2025-01-16,2025-01-19
Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation,https://arxiv.org/abs/2501.04970,2025-01-16,2025-01-19
Step-by-Step Mastery - Enhancing Soft Constraint Following Ability of Large Language Models,https://arxiv.org/abs/2501.04945,2025-01-16,2025-01-19
$\text{Transformer}^2$ - Self-adaptive LLMs,https://arxiv.org/abs/2501.06252,2025-01-16,2025-01-19
"A Look into How Machine Learning is Reshaping Engineering Models - the Rise of Analysis Paralysis, Optimal yet Infeasible Solutions, and the Inevitable Rashomon Paradox",https://arxiv.org/abs/2501.04894,2025-01-16,2025-01-19
States Hidden in Hidden States - LLMs Emerge Discrete State Representations Implicitly,https://arxiv.org/abs/2407.11421,2024-07-16,2025-01-19
Scaling Laws for Precision,https://arxiv.org/abs/2411.04330,2024-11-07,2025-01-19
Grokking at the Edge of Numerical Stability,https://arxiv.org/abs/2501.04697,2025-01-08,2025-01-19
Titans - Learning to Memorize at Test Time,https://arxiv.org/abs/2501.00663,2024-12-31,2025-01-19
Densely Connected Convolutional Networks,https://arxiv.org/abs/1608.06993,2016-08-25,2025-01-19
On the Algorithmic Bias of Aligning Large Language Models with RLHF - Preference Collapse and Matching Regularization,https://arxiv.org/abs/2405.16455,2024-05-26,2025-01-20
MiniMax-01 - Scaling Foundation Models with Lightning Attention,https://www.arxiv.org/abs/2501.08313,2025-01-14,2025-01-25
Titans - Learning to Memorize at Test Time,https://www.arxiv.org/abs/2501.00663,2024-12-31,2025-01-25
$ -text{Transformer}^2$ - Self-adaptive LLMs,https://www.arxiv.org/abs/2501.06252,2025-01-09,2025-01-25
Do generative video models learn physical principles from watching videos -,https://www.arxiv.org/abs/2501.09038,2025-01-14,2025-01-25
Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps,https://arxiv.org/abs/2501.09732,2025-01-16,2025-01-25
RECALL - Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles,https://arxiv.org/abs/2501.13491,2025-01-23,2025-01-25
Test-time regression - a unifying framework for designing sequence models with associative memory,https://arxiv.org/abs/2501.12352,2025-01-21,2025-01-25
Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design,https://arxiv.org/abs/2501.08603v2,2025-01-15,2025-01-25
O1-Pruner - Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning,https://arxiv.org/abs/2501.12570,2025-01-22,2025-01-25
Reasoning Language Models - A Blueprint,https://arxiv.org/abs/2501.11223,2025-01-20,2025-01-25
"Sigma - Differential Rescaling of Query, Key and Value for Efficient Language Models",https://www.arxiv.org/abs/2501.13629,2025-01-23,2025-01-25
Demons in the Detail - On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models,https://www.arxiv.org/abs/2501.11873,2025-01-21,2025-01-25
Evolving Deeper LLM Thinking,https://www.arxiv.org/abs/2501.09891,2025-01-17,2025-01-25
Physics of Skill Learning,https://www.arxiv.org/abs/2501.12391,2025-01-21,2025-01-25
Autonomy-of-Experts Models,https://www.arxiv.org/abs/2501.13074,2025-01-22,2025-01-25
Recurrent Diffusion for Large-Scale Parameter Generation,https://www.arxiv.org/abs/2501.11587,2025-01-20,2025-01-25
Test-time regression - a unifying framework for designing sequence models with associative memory,https://www.arxiv.org/abs/2501.12352,2025-01-21,2025-01-25
Can We Generate Images with CoT - Let's Verify and Reinforce Image Generation Step by Step,https://www.arxiv.org/abs/2501.13926,2025-01-23,2025-01-25
Reasoning Language Models - A Blueprint,https://www.arxiv.org/abs/2501.11223,2025-01-20,2025-01-25
Overshoot - Taking advantage of future gradients in momentum-based stochastic optimization,https://arxiv.org/abs/2501.09556v1,2025-01-16,2025-01-25
Coarse-to-Fine Process Reward Modeling for Enhanced Mathematical Reasoning,https://arxiv.org/abs/2501.13622v1,2025-01-23,2025-01-25
Evolving Deeper LLM Thinking,https://arxiv.org/abs/2501.09891v1,2025-01-17,2025-01-25
ZOQO - Zero-Order Quantized Optimization,https://arxiv.org/abs/2501.06736v1,2025-01-12,2025-01-25
Grokking at the Edge of Numerical Stability,https://arxiv.org/abs/2501.04697,2025-01-08,2025-01-25
Deliberation in Latent Space via Differentiable Cache Augmentation,https://arxiv.org/abs/2412.17747,2024-12-23,2025-01-25
Everything of Thoughts - Defying the Law of Penrose Triangle for Thought Generation,https://arxiv.org/abs/2311.04254,2023-11-07,2025-01-25
Let SSMs be ConvNets - State-space Modeling with Optimal Tensor Contractions,https://arxiv.org/abs/2501.13230,2025-01-22,2025-01-25
Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step,https://arxiv.org/abs/2501.13926,2025-01-23,2025-01-25
Do Large Language Models Truly Understand Geometric Structures?,https://arxiv.org/abs/2501.13773,2025-01-23,2025-01-25
Integrating Causality with Neurochaos Learning - Proposed Approach and Research Agenda,https://arxiv.org/abs/2501.13763,2025-01-23,2025-01-25
Regularizing cross entropy loss via minimum entropy and K-L divergence,https://arxiv.org/abs/2501.13709,2025-01-23,2025-01-25
"Sigma - Differential Rescaling of Query, Key and Value for Efficient Language Models",https://arxiv.org/abs/2501.13629,2025-01-23,2025-01-25
FedPref - Federated Learning Across Heterogeneous Multi-objective Preferences,https://arxiv.org/abs/2501.13604,2025-01-23,2025-01-25
Communication-Efficient Stochastic Distributed Learning,https://arxiv.org/abs/2501.13516,2025-01-23,2025-01-25
Spurious Forgetting in Continual Learning of Language Models,https://arxiv.org/abs/2501.13453,2025-01-23,2025-01-25
"M3PT - A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention",https://arxiv.org/abs/2501.13416,2025-01-23,2025-01-25
ExLM - Rethinking the Impact of $\texttt{[MASK]}$ Tokens in Masked Language Models,https://arxiv.org/abs/2501.13397,2025-01-23,2025-01-25
Autonomy-of-Experts Models,https://arxiv.org/abs/2501.13074,2025-01-23,2025-01-25
The Finite Element Neural Network Method - One Dimensional Study,https://arxiv.org/abs/2501.12508,2025-01-23,2025-01-25
Physics of Skill Learning,https://arxiv.org/abs/2501.12391,2025-01-23,2025-01-25
Expertise elevates AI usage - experimental evidence comparing laypeople and professional artists,https://arxiv.org/abs/2501.12374,2025-01-23,2025-01-25
Parameters vs FLOPs - Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models,https://arxiv.org/abs/2501.12370,2025-01-23,2025-01-25
CYCle - Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning,https://arxiv.org/abs/2501.12344,2025-01-23,2025-01-25
Preference Curriculum - LLMs Should Always Be Pretrained on Their Preferred Data,https://arxiv.org/abs/2501.13126,2025-01-23,2025-01-25
Debate Helps Weak-to-Strong Generalization,https://arxiv.org/abs/2501.13124,2025-01-23,2025-01-25
Demons in the Detail - On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models,https://arxiv.org/abs/2501.11873,2025-01-23,2025-01-25
Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs,https://arxiv.org/abs/2501.11833,2025-01-23,2025-01-25
Tell me about yourself - LLMs are aware of their learned behaviors,https://arxiv.org/abs/2501.11120,2025-01-23,2025-01-25
Dynamic semantic networks for exploration of creative thinking,https://arxiv.org/abs/2501.11090,2025-01-23,2025-01-25
Control LLM - Controlled Evolution for Intelligence Retention in LLM,https://arxiv.org/abs/2501.10979,2025-01-23,2025-01-25
Dynamic Continual Learning - Harnessing Parameter Uncertainty for Improved Network Adaptation,https://arxiv.org/abs/2501.10861,2025-01-23,2025-01-25
Jailbreaking Large Language Models in Infinitely Many Ways,https://arxiv.org/abs/2501.10800,2025-01-23,2025-01-25
FSMoE - A Flexible and Scalable Training System for Sparse Mixture-of-Experts Models,https://arxiv.org/abs/2501.10714,2025-01-23,2025-01-25
How Should I Build A Benchmark?,https://arxiv.org/abs/2501.10711,2025-01-23,2025-01-25
The Geometry of Tokens in Internal Representations of Large Language Models,https://arxiv.org/abs/2501.10573,2025-01-23,2025-01-25
Good things come in small packages - Should we adopt Lite-GPUs in AI infrastructure?,https://arxiv.org/abs/2501.10187,2025-01-23,2025-01-25
Evolving Deeper LLM Thinking,https://arxiv.org/abs/2501.09891,2025-01-23,2025-01-25
Tree Cross Attention,https://arxiv.org/abs/2309.17388,2023-09-29,2025-01-27
Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and Multiple Level Analysis,https://arxiv.org/abs/2501.12084,2025-01-21,2025-01-31
Transformer-Squared - Self-adaptive LLMs,https://arxiv.org/abs/2501.06252,2025-01-09,2025-01-31
Alice in Wonderland - Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models,https://arxiv.org/abs/2406.02061,2024-06-04,2025-01-31
"SFT Memorizes, RL Generalizes - A Comparative Study of Foundation Model Post-training",https://www.arxiv.org/abs/2501.17161,2025-01-28,2025-01-31
"Hierarchical Autoregressive Transformers - Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models",https://arxiv.org/abs/2501.10322v2,2025-01-17,2025-01-31
LLM360 K2 - Building a 65B 360-Open-Source Large Language Model from Scratch,https://arxiv.org/abs/2501.07124v3,2025-01-13,2025-01-31
Take Caution in Using LLMs as Human Surrogates - Scylla Ex Machina,https://arxiv.org/abs/2410.19599v3,2024-10-25,2025-01-31
Principled model selection for stochastic dynamics,https://arxiv.org/abs/2501.10339v2,2025-01-17,2025-01-31
Evolving Deeper LLM Thinking,https://arxiv.org/abs/2501.09891v1,2025-01-17,2025-01-31
Language Models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,2020-05-28,2025-01-31
BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805,2018-10-11,2025-01-31
Improved Techniques for Training GANs,https://arxiv.org/abs/1606.03498,2016-06-10,2025-01-31
Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,https://arxiv.org/abs/1511.06434,2015-11-19,2025-01-31
Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,2013-12-20,2025-01-31
Generative Adversarial Networks,https://arxiv.org/abs/1406.2661,2014-06-10,2025-01-31
OstQuant - Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting,https://arxiv.org/abs/2501.13987,2025-01-23,2025-01-31
Over-Tokenized Transformer - Vocabulary is Generally Worth Scaling,https://arxiv.org/pdf/2501.16975,2025-01-28,2025-01-31
Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge,https://arxiv.org/abs/2501.18099,2025-01-30,2025-01-31
OmniThink - Expanding Knowledge Boundaries in Machine Writing through Thinking,https://www.arxiv.org/abs/2501.09751v1,2025-01-16,2025-01-31
Recurrent Diffusion for Large-Scale Parameter Generation,https://www.arxiv.org/abs/2501.11587v1,2025-01-20,2025-01-31
DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://www.arxiv.org/abs/2501.12948,2025-01-22,2025-01-31
DeMo - Decoupled Momentum Optimization,https://arxiv.org/pdf/2411.19870,2024-11-29,2025-01-31
Qwen2.5 Technical Report,https://arxiv.org/abs/2412.15115,2024-12-19,2025-01-31
Deliberation in Latent Space via Differentiable Cache Augmentation,https://arxiv.org/abs/2412.17747,2024-12-23,2025-01-31
Everything of Thoughts - Defying the Law of Penrose Triangle for Thought Generation,https://arxiv.org/abs/2311.04254,2023-11-07,2025-01-31
Let SSMs be ConvNets - State-space Modeling with Optimal Tensor Contractions,https://arxiv.org/abs/2501.13230,2025-01-22,2025-01-31
Step-KTO - Optimizing Mathematical Reasoning through Stepwise Binary Feedback,https://arxiv.org/abs/2501.10799,2025-01-18,2025-01-31
"SFT Memorizes, RL Generalizes - A Comparative Study of Foundation Model Post-training",https://arxiv.org/abs/2501.17161,2025-01-28,2025-01-31
Interpretability in Parameter Space - Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition,https://arxiv.org/abs/2501.14926,2025-01-24,2025-01-31
Test-time regression - a unifying framework for designing sequence models with associative memory,https://arxiv.org/abs/2501.12352,2025-01-21,2025-01-31
Tree Cross Attention,https://arxiv.org/abs/2309.17388,2023-09-29,2025-01-31
REST - Retrieval-Based Speculative Decoding,https://arxiv.org/abs/2311.08252,2023-11-14,2025-01-31
"Dense Training, Sparse Inference - Rethinking Training of Mixture-of-Experts Language Models",https://arxiv.org/pdf/2404.05567,2024-04-08,2025-01-31
Scaling Laws for Fine-Grained Mixture of Experts,https://arxiv.org/abs/2402.07871,2024-02-12,2025-01-31
Neural Machine Translation without Embeddings,https://arxiv.org/abs/2008.09396,2020-08-21,2025-01-31
DeltaLLM - Compress LLMs with Low-Rank Deltas between Shared Weights,https://arxiv.org/abs/2501.18596,2025-01-30,2025-01-31
Diffusion Autoencoders are Scalable Image Tokenizers,https://arxiv.org/abs/2501.18593,2025-01-30,2025-01-31
Thoughts Are All Over the Place - On the Underthinking of o1-Like LLMs,https://arxiv.org/abs/2501.18585,2025-01-30,2025-01-31
Streaming DiLoCo with overlapping communication - Towards a Distributed Free Lunch,https://arxiv.org/abs/2501.18512,2025-01-30,2025-01-31
State Stream Transformer (SST)  - Emergent Metacognitive Behaviours Through Latent State Persistence,https://arxiv.org/abs/2501.18356,2025-01-30,2025-01-31
A Unified Perspective on the Dynamics of Deep Transformers,https://arxiv.org/abs/2501.18322,2025-01-30,2025-01-31
Economic Rationality under Specialization - Evidence of Decision Bias in AI Agents,https://arxiv.org/abs/2501.18190,2025-01-30,2025-01-31
Diverse Preference Optimization,https://arxiv.org/abs/2501.18101,2025-01-30,2025-01-31
LLMs can see and hear without any training,https://arxiv.org/abs/2501.18096,2025-01-30,2025-01-31
Perforated Backpropagation - A Neuroscience Inspired Extension to Artificial Neural Networks,https://arxiv.org/abs/2501.18018,2025-01-30,2025-01-31
When less is more - evolving large neural networks from small ones,https://arxiv.org/abs/2501.18012,2025-01-30,2025-01-31
Large Language Models Think Too Fast To Explore Effectively,https://arxiv.org/abs/2501.18009,2025-01-30,2025-01-31
InnerThoughts - Disentangling Representations and Predictions in Large Language Models,https://arxiv.org/abs/2501.17994,2025-01-30,2025-01-31
Janus-Pro - Unified Multimodal Understanding and Generation with Data and Model Scaling,https://arxiv.org/abs/2501.17811,2025-01-30,2025-01-31
AI Governance through Markets,https://arxiv.org/abs/2501.17755,2025-01-30,2025-01-31
Sparse Autoencoders Can Interpret Randomly Initialized Transformers,https://arxiv.org/abs/2501.17727,2025-01-30,2025-01-31
In-Context Meta LoRA Generation,https://arxiv.org/abs/2501.17635,2025-01-30,2025-01-31
Heuristic-Informed Mixture of Experts for Link Prediction in Multilayer Networks,https://arxiv.org/abs/2501.17557,2025-01-30,2025-01-31
RegD - Hierarchical Embeddings via Distances over Geometric Regions,https://arxiv.org/abs/2501.17518,2025-01-30,2025-01-31
DINT Transformer,https://arxiv.org/abs/2501.17486,2025-01-30,2025-01-31
AxBench - Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders,https://arxiv.org/abs/2501.17148,2025-01-30,2025-01-31
Exponential Family Attention,https://arxiv.org/abs/2501.16790,2025-01-30,2025-01-31
Sparse Autoencoders Trained on the Same Data Learn Different Features,https://arxiv.org/abs/2501.16615,2025-01-30,2025-01-31
Open Problems in Mechanistic Interpretability,https://arxiv.org/abs/2501.16496,2025-01-30,2025-01-31
CoCoNUT - Structural Code Understanding does not fall out of a tree,https://arxiv.org/abs/2501.16456,2025-01-30,2025-01-31
Matryoshka Re-Ranker - A Flexible Re-Ranking Architecture With Configurable Depth and Width,https://arxiv.org/abs/2501.16302,2025-01-30,2025-01-31
Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?,https://arxiv.org/abs/2501.15857,2025-01-30,2025-01-31
TopoNets - High Performing Vision and Language Models with Brain-Like Topography,https://arxiv.org/abs/2501.16396,2025-01-30,2025-01-31
TensorLLM - Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs,https://arxiv.org/abs/2501.15674,2025-01-30,2025-01-31
StagFormer - Time Staggering Transformer Decoding for RunningLayers In Parallel,https://arxiv.org/abs/2501.15665,2025-01-30,2025-01-31
People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text,https://arxiv.org/abs/2501.15654,2025-01-30,2025-01-31
A Comprehensive Survey on Self-Interpretable Neural Networks,https://arxiv.org/abs/2501.15638,2025-01-30,2025-01-31
"ARWKV - Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",https://arxiv.org/abs/2501.15570,2025-01-30,2025-01-31
Token Democracy - The Architectural Limits of Alignment in Transformer-Based Language Models,https://arxiv.org/abs/2501.15446,2025-01-30,2025-01-31
Qwen2.5-1M Technical Report,https://arxiv.org/abs/2501.15383,2025-01-30,2025-01-31
Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions,https://arxiv.org/abs/2501.15283,2025-01-30,2025-01-31
Who's Driving? Game Theoretic Path Risk of AGI Development,https://arxiv.org/abs/2501.15280,2025-01-30,2025-01-31
Humanity's Last Exam,https://arxiv.org/abs/2501.14249,2025-01-30,2025-01-31
RL + Transformer = A General-Purpose Problem Solver,https://arxiv.org/abs/2501.14176,2025-01-30,2025-01-31
