Title,ArXiv Link,Paper Date,Date Added
3D-RPE - Enhancing Long-Context Modeling Through 3D Rotary Position Encoding,https://arxiv.org/abs/2406.09897,2024-06-18,2024-06-19
An elementary proof of a universal approximation theorem,https://arxiv.org/abs/2406.10002,2024-06-18,2024-06-19
Breaking the Attention Bottleneck,https://arxiv.org/abs/2406.10906,2024-06-18,2024-06-19
In Tree Structure Should Sentence Be Generated,https://arxiv.org/abs/2406.14189,2024-06-20,2024-06-21
Transcendence - Generative Models Can Outperform The Experts That Train Them,https://arxiv.org/abs/2406.11741,2024-06-18,2024-06-19
"Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",https://arxiv.org/abs/2406.10209,2024-06-18,2024-06-19
Can LLMs Learn Macroeconomic Narratives from Social Media?,https://arxiv.org/abs/2406.12109,2024-06-18,2024-06-19
Amphista - Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style,https://arxiv.org/abs/2406.13170,2024-06-20,2024-06-21
Provable Guarantees for Model Performance via Mechanistic Interpretability,https://arxiv.org/abs/2406.11779,2024-06-18,2024-06-19
Locating and Extracting Relational Concepts in Large Language Models,https://arxiv.org/abs/2406.13184,2024-06-20,2024-06-21
Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,https://arxiv.org/abs/2406.07394,2024-06-11,2024-06-19
In-Context Former - Lightning-fast Compressing Context for Large Language Model,https://arxiv.org/abs/2406.13618,2024-06-20,2024-06-21
Complex fractal trainability boundary can arise from trivial non-convexity,https://arxiv.org/abs/2406.13971,2024-06-20,2024-06-21
Distributional reasoning in LLMs - Parallel reasoning processes in multi-hop reasoning,https://arxiv.org/abs/2406.13858,2024-06-20,2024-06-21
Mixture-of-Agents Enhances Large Language Model Capabilities,https://arxiv.org/abs/2406.04692,2024-06-07,2024-06-20
Optimised Grouped-Query Attention Mechanism for Transformers,https://arxiv.org/abs/2406.14963,2024-06-24,2024-06-25
OTCE - Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser,https://arxiv.org/abs/2406.16495,2024-06-24,2024-06-25
The Responsible Foundation Model Development Cheatsheet - A Review of Tools & Resources,https://arxiv.org/abs/2406.16746,2024-06-24,2024-06-25
Emergence of Hidden Capabilities - Exploring Learning Dynamics in Concept Space,https://arxiv.org/abs/2406.19370,2024-06-27,2024-06-28
Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network,https://arxiv.org/abs/2406.15109,2024-06-24,2024-06-25
Understanding and Mitigating Tokenization Bias in Language Models,https://arxiv.org/abs/2406.16829,2024-06-24,2024-06-25
MoA - Mixture of Sparse Attention for Automatic Large Language Model Compression,https://arxiv.org/abs/2406.14909,2024-06-24,2024-06-25
The Hidden Pitfalls of the Cosine Similarity Loss,https://arxiv.org/abs/2406.16468,2024-06-24,2024-06-25
Finding Transformer Circuits with Edge Pruning,https://arxiv.org/abs/2406.16778,2024-06-24,2024-06-25
A Closer Look into Mixture-of-Experts in Large Language Models,https://arxiv.org/abs/2406.18219,2024-06-26,2024-06-27
Single Parent Family - A Spectrum of Family Members from a Single Pre-Trained Foundation Model,https://arxiv.org/abs/2406.19995,2024-06-28,2024-07-02
Universal Length Generalization with Turing Programs,https://arxiv.org/abs/2407.03310v1,2024-07-03,2024-07-04
Large Language Model Enhanced Knowledge Representation Learning - A Survey,https://arxiv.org/abs/2407.00936,2024-07-02,2024-07-03
Universal Approximation Theory - The basic theory for large language models,https://arxiv.org/abs/2407.00958,2024-07-02,2024-07-03
Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing,https://arxiv.org/abs/2407.03185,2024-07-03,2024-07-04
On the Anatomy of Attention,https://arxiv.org/abs/2407.02423,2024-07-02,2024-07-03
A Review of the Applications of Deep Learning-Based Emergent Communication,https://arxiv.org/abs/2407.03302,2024-07-03,2024-07-04
Hypformer - Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space,https://arxiv.org/abs/2407.01290,2024-07-02,2024-07-03
Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models - Enhancing Performance and Reducing Inference Costs,https://arxiv.org/abs/2407.00945,2024-07-02,2024-07-03
Do language models plan ahead for future tokens -,https://arxiv.org/abs/2404.00859,2024-04-01,2024-07-04
Diffusion Models and Representation Learning - A Survey,https://arxiv.org/abs/2407.00783,2024-07-02,2024-07-03
Predicting vs. Acting - A Trade-off Between World Modeling & Agent Modeling,https://arxiv.org/abs/2407.02446,2024-07-02,2024-07-03
Min P Sampling - Balancing Creativity and Coherence at High Temperature,https://arxiv.org/abs/2407.01082,2024-07-02,2024-07-03
Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs,https://arxiv.org/abs/2406.20086,2024-06-28,2024-07-02
Efficient Training of Language Models with Compact and Consistent Next Token Distributions,https://arxiv.org/abs/2407.02819,2024-07-03,2024-07-04
How Does Overparameterization Affect Features?,https://arxiv.org/abs/2407.00968,2024-07-02,2024-07-03
LLM Internal States Reveal Hallucination Risk Faced With a Query,https://arxiv.org/abs/2407.03282,2024-07-03,2024-07-04
Large Language Models Assume People are More Rational than We Really are,https://arxiv.org/abs/2406.17055,2024-06-25,2024-06-27
MobileLLM - Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,https://arxiv.org/pdf/2402.14905,2024-02-22,2024-07-11
Predictive Coding Networks and Inference Learning - Tutorial and Survey,https://arxiv.org/abs/2407.04117,2024-07-09,2024-07-10
Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial General Intelligence,https://arxiv.org/abs/2407.03652,2024-07-09,2024-07-10
SoftDedup - an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training,https://arxiv.org/abs/2407.06654,2024-07-09,2024-07-10
Anthropocentric bias and the possibility of artificial cognition,https://arxiv.org/abs/2407.03859,2024-07-09,2024-07-10
Mixture of A Million Experts,https://arxiv.org/abs/2407.04153,2024-07-09,2024-07-10
Improving Self Consistency in LLMs through Probabilistic Tokenization,https://arxiv.org/abs/2407.03678,2024-07-09,2024-07-10
Internet of Agents - Weaving a Web of Heterogeneous Agents for Collaborative Intelligence,https://arxiv.org/abs/2407.07061,2024-07-09,2024-07-10
A Theory of Machine Learning,https://arxiv.org/abs/2407.05520,2024-07-09,2024-07-10
Introducing 'Inside' Out of Distribution,https://arxiv.org/abs/2407.04534,2024-07-09,2024-07-10
A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training,https://arxiv.org/abs/2407.07630,2024-07-10,2024-07-11
When LLMs Play the Telephone Game - Cumulative Changes and Attractors in Iterated Cultural Transmissions,https://arxiv.org/abs/2407.04503,2024-07-09,2024-07-10
Enhancing learning in artificial neural networks through cellular heterogeneity and neuromodulatory signaling,https://arxiv.org/abs/2407.04525,2024-07-09,2024-07-10
Learning to (Learn at Test Time) - RNNs with Expressive Hidden States,https://arxiv.org/abs/2407.04620,2024-07-05,2024-07-12
Surpassing Cosine Similarity for Multidimensional Comparisons - Dimension Insensitive Euclidean Metric (DIEM),https://arxiv.org/abs/2407.08623,2024-07-11,2024-07-12
Collective Innovation in Groups of Large Language Models,https://arxiv.org/abs/2407.05377,2024-07-09,2024-07-10
E5-V - Universal Embeddings with Multimodal Large Language Models,https://arxiv.org/abs/2407.12580,2024-07-18,2024-07-19
Beyond KV Caching - Shared Attention for Efficient LLMs,https://arxiv.org/abs/2407.12866,2024-07-18,2024-07-21
SpreadsheetLLM - Encoding Spreadsheets for Large Language Models,https://arxiv.org/abs/2407.09025,2024-07-18,2024-07-21
The Foundations of Tokenization - Statistical and Computational Concerns,https://arxiv.org/abs/2407.11606,2024-07-18,2024-07-20
Graph Transformers - A Survey,https://arxiv.org/abs/2407.09777,2024-07-18,2024-07-21
Human-like Episodic Memory for Infinite Context LLMs,https://arxiv.org/abs/2407.09450,2024-07-12,2024-07-15
Vectoring Languages,https://arxiv.org/abs/2407.11766,2024-07-18,2024-07-20
MaskMoE - Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts,https://arxiv.org/abs/2407.09816,2024-07-18,2024-07-21
A Comprehensive Review of Recommender Systems - Transitioning from Theory to Practice,https://arxiv.org/abs/2407.13699,2024-07-18,2024-07-19
A Survey on Universal Approximation Theorems,https://arxiv.org/abs/2407.12895,2024-07-18,2024-07-19
Future Lens - Anticipating Subsequent Tokens from a Single Hidden State,https://arxiv.org/abs/2311.04897,2023-11-08,2024-07-16
Information-Theoretic Foundations for Machine Learning,https://arxiv.org/abs/2407.12288,2024-07-18,2024-07-19
Flash normalization - fast RMSNorm for LLMs,https://arxiv.org/abs/2407.09577,2024-07-18,2024-07-21
Efficient Visual Transformer by Learnable Token Merging,https://arxiv.org/abs/2407.15219,2024-07-23,2024-07-26
Deep Learning for Economists,https://arxiv.org/abs/2407.15339,2024-07-23,2024-07-26
Matryoshka Diffusion Models,https://arxiv.org/abs/2310.15111,2023-10-23,2024-07-26
Vectoring Languages,https://arxiv.org/abs/2407.11766,2024-07-18,2024-07-20
On the Benefits of Rank in Attention Layers,https://arxiv.org/abs/2407.16153,2024-07-23,2024-07-25
Demystifying Verbatim Memorization in Large Language Models,https://arxiv.org/abs/2407.17817,2024-07-25,2024-07-26
"Farewell to Length Extrapolation, a Training-Free Infinite Context with Finite Attention Scope",https://arxiv.org/abs/2407.15176,2024-07-23,2024-07-26
Dissecting Multiplication in Transformers - Insights into LLMs,https://arxiv.org/abs/2407.15360,2024-07-23,2024-07-26
Keep the Cost Down - A Review on Methods to Optimize LLM' s KV-Cache Consumption,https://arxiv.org/abs/2407.18003,2024-07-25,2024-07-26
Shared Imagination - LLMs Hallucinate Alike,https://arxiv.org/abs/2407.16604,2024-07-23,2024-07-25
Investigating learning-independent abstract reasoning in artificial neural networks,https://arxiv.org/abs/2407.17791,2024-07-25,2024-07-26
Learning mental states estimation through self-observation - a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind,https://arxiv.org/abs/2407.18022,2024-07-25,2024-07-26
"A Comprehensive Survey of LLM Alignment Techniques - RLHF, RLAIF, PPO, DPO and More",https://arxiv.org/abs/2407.16216,2024-07-23,2024-07-25
PMoE - Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning,https://arxiv.org/abs/2407.21571,2024-07-31,2024-08-01
Deep Companion Learning - Enhancing Generalization Through Historical Consistency,https://arxiv.org/abs/2407.18821,2024-07-29,2024-07-30
Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens,https://arxiv.org/abs/2407.21248,2024-07-31,2024-08-01
Machine Learning for predicting chaotic systems,https://arxiv.org/abs/2407.20158,2024-07-29,2024-07-30
Universal Approximation Theory - Foundations for Parallelism in Neural Networks,https://arxiv.org/abs/2407.21670,2024-07-31,2024-08-01
Mixture of Nested Experts - Adaptive Processing of Visual Tokens,https://arxiv.org/abs/2407.19985,2024-07-29,2024-07-30
Big Cooperative Learning,https://arxiv.org/abs/2407.21319,2024-07-31,2024-08-01
An introduction to reinforcement learning for neuroscience,https://arxiv.org/abs/2311.07315v2,2023-11-13,2024-08-02
MoMa - Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts,https://arxiv.org/abs/2407.21770,2024-07-31,2024-08-01
Social Learning through Interactions with Other Agents - A Survey,https://arxiv.org/abs/2407.21713,2024-07-31,2024-08-01
Comprehensive Survey of Complex-Valued Neural Networks - Insights into Backpropagation and Activation Functions,https://arxiv.org/abs/2407.19258,2024-07-29,2024-07-30
Supertrust - Evolution-based superalignment strategy for safe coexistence,https://arxiv.org/abs/2407.20208,2024-07-29,2024-07-30
"Entropy, Thermodynamics and the Geometrization of the Language Model",https://arxiv.org/abs/2407.21092,2024-07-31,2024-08-01
Do Language Models Have a Critical Period for Language Acquisition -,https://arxiv.org/abs/2407.19325v1,2024-07-27,2024-08-02
Exploring Loss Landscapes through the Lens of Spin Glass Theory,https://arxiv.org/abs/2407.20724,2024-07-31,2024-08-01
The Llama 3 Herd of Models,https://arxiv.org/abs/2407.21783,2024-07-31,2024-08-01
Diffusion Guided Language Modeling,https://arxiv.org/abs/2408.04220,2024-08-08,2024-08-09
POA - Pre-training Once for Models of All Sizes,https://arxiv.org/abs/2408.01031,2024-08-02,2024-08-09
Language Model Can Listen While Speaking,https://arxiv.org/abs/2408.02622,2024-08-05,2024-08-06
Unexpected Benefits of Self-Modeling in Neural Systems,https://arxiv.org/abs/2407.10188,2024-07-14,2024-08-05
Climbing the Complexity Ladder with Expressive Attention,https://arxiv.org/abs/2407.18601v1,2024-07-26,2024-08-09
Mixture-of-Depths - Dynamically allocating compute in transformer-based language models,https://arxiv.org/abs/2404.02258,2024-04-02,2024-08-09
What comes after transformers? -- A selective survey connecting ideas in deep learning,https://arxiv.org/abs/2408.00386,2024-08-02,2024-08-05
Gemma 2 - Improving Open Language Models at a Practical Size,https://arxiv.org/abs/2408.00118,2024-07-31,2024-08-05
Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models,https://arxiv.org/abs/2408.01308,2024-08-02,2024-08-05
"SentenceVAE - Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models",https://arxiv.org/abs/2408.00655,2024-08-02,2024-08-05
OpenEP - Open-Ended Future Event Prediction,https://arxiv.org/abs/2408.06578,2024-08-13,2024-08-14
Liquid Time-constant Networks,https://arxiv.org/abs/2006.04439,2020-06-08,2024-08-13
The Clever Hans Effect in Unsupervised Learning,https://arxiv.org/abs/2408.08041,2024-08-15,2024-08-16
Capturing the Complexity of Human Strategic Decision-Making with Machine Learning,https://arxiv.org/abs/2408.07865,2024-08-15,2024-08-16
Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning,https://arxiv.org/abs/2408.00690v2,2024-08-01,2024-08-16
"A Single Goal is All You Need - Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals",https://arxiv.org/abs/2408.05804,2024-08-12,2024-08-13
Can Turing machine be curious about its Turing test results - Three informal lectures on physics of intelligence,https://arxiv.org/abs/1606.08109,2016-06-27,2024-08-12
"Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks",https://arxiv.org/abs/2408.04965,2024-08-10,2024-08-12
The ShareLM Collection and Plugin - Contributing Human-Model Chats for the Benefit of the Community,https://arxiv.org/abs/2408.08291,2024-08-15,2024-08-16
Liquid Structural State-Space Models,https://arxiv.org/abs/2209.12951,2022-09-26,2024-08-14
BAM! Just Like That - Simple and Efficient Parameter Upcycling for Mixture of Experts,https://arxiv.org/abs/2408.08274,2024-08-15,2024-08-16
Patch-Level Training for Large Language Models,https://arxiv.org/abs/2407.12665,2024-07-17,2024-08-16
The Physics of Learning - From Autoencoders to Truly Autonomous Learning Machines,https://arxiv.org/abs/2407.04700,2024-02-12,2024-08-12
ReST-MCTS - - LLM Self-Training via Process Reward Guided Tree Search,https://arxiv.org/abs/2406.03816,2024-06-06,2024-08-12
Layerwise Recurrent Router for Mixture-of-Experts,https://arxiv.org/abs/2408.06793,2024-08-13,2024-08-14
Understanding Learning through the Lens of Dynamical Invariants,https://arxiv.org/abs/2401.10428,2024-01-19,2024-08-12
Reciprocal Learning,https://arxiv.org/abs/2408.06257,2024-08-12,2024-08-13
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-11
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-11
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-08-23
Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations,https://arxiv.org/abs/2408.10920,2024-08-20,2024-08-22
From pixels to planning - scale-free active inference,https://arxiv.org/abs/2407.20292,2024-07-27,2024-08-07
Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model,https://arxiv.org/abs/2408.11039,2024-08-20,2024-08-21
Beyond English-Centric LLMs - What Language Do Multilingual Language Models Think in?,https://arxiv.org/abs/2408.10811,2024-08-20,2024-08-22
Scaling Laws with Vocabulary - Larger Models Deserve Larger Vocabularies,https://arxiv.org/abs/2407.13623,2024-07-18,2024-08-23
Machine Learning with Physics Knowledge for Prediction - A Survey,https://arxiv.org/abs/2408.09840,2024-08-20,2024-08-22
Scaling Law with Learning Rate Annealing,https://arxiv.org/abs/2408.11029,2024-08-20,2024-08-21
The Exploration-Exploitation Dilemma Revisited - An Entropy Perspective,https://arxiv.org/abs/2408.09974,2024-08-20,2024-08-22
HMoE - Heterogeneous Mixture of Experts for Language Modeling,https://arxiv.org/abs/2408.10681,2024-08-20,2024-08-22
Learning Randomized Algorithms with Transformers,https://arxiv.org/abs/2408.10818,2024-08-20,2024-08-22
SMILE - Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models,https://arxiv.org/abs/2408.10174,2024-08-20,2024-08-22
KAN 2.0 - Kolmogorov-Arnold Networks Meet Science,https://arxiv.org/abs/2408.10205,2024-08-19,2024-08-20
EPO - Hierarchical LLM Agents with Environment Preference Optimization,https://arxiv.org/abs/2408.16090,2024-08-29,2024-08-31
An Overview on Machine Learning Methods for Partial Differential Equations - from Physics Informed Neural Networks to Deep Operator Learning,https://arxiv.org/abs/2408.13222,2024-08-23,2024-08-31
Category-Theoretical and Topos-Theoretical Frameworks in Machine Learning - A Survey,https://arxiv.org/abs/2408.14014,2024-08-27,2024-08-30
Dolphin - Long Context as a New Modality for Energy-Efficient On-Device Language Models,https://arxiv.org/abs/2408.15518,2024-08-29,2024-08-31
Diffusion Models Are Real-Time Game Engines,https://arxiv.org/abs/2408.14837,2024-08-27,2024-08-29
Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts,https://arxiv.org/abs/2408.15664,2024-08-29,2024-08-31
Self-Improving Diffusion Models with Synthetic Data,https://arxiv.org/abs/2408.16333,2024-08-29,2024-08-30
In-Context Learning with Representations - Contextual Generalization of Trained Transformers,https://arxiv.org/abs/2408.10147v1,2024-08-19,2024-08-31
A Law of Next-Token Prediction in Large Language Models,https://arxiv.org/abs/2408.13442,2024-08-27,2024-08-30
Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need,https://arxiv.org/abs/2408.15997,2024-08-29,2024-08-31
Federated Learning - A Cutting-Edge Survey of the Latest Advancements and Applications,https://arxiv.org/abs/2310.05269,2023-10-08,2024-09-02
How does the brain compute with probabilities?,https://arxiv.org/abs/2409.02709,2024-09-04,2024-09-05
Quantifying Emergence in Neural Networks - Insights from Pruning and Training Dynamics,https://arxiv.org/abs/2409.01568,2024-09-04,2024-09-05
Learning (With) Distributed Optimization,https://arxiv.org/abs/2308.05548,2023-08-10,2024-09-02
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-08-23
Adversarial Training Using Feedback Loops,https://arxiv.org/abs/2308.11881,2023-08-23,2024-09-02
The Unbearable Slowness of Being,https://www.arxiv.org/abs/2408.10234v1,2024-08-03,2024-09-05
A Survey on Emergent Language,https://arxiv.org/abs/2409.02645,2024-09-04,2024-09-05
Dreaming is All You Need,https://arxiv.org/abs/2409.01633,2024-09-04,2024-09-05
Configurable Foundation Models - Building LLMs from a Modular Perspective,https://arxiv.org/abs/2409.02877,2024-09-04,2024-09-05
Neural timescales from a computational perspective,https://arxiv.org/abs/2409.02684,2024-09-04,2024-09-05
Scaling Laws for Economic Productivity - Experimental Evidence in LLM-Assisted Translation,https://arxiv.org/abs/2409.02391,2024-09-04,2024-09-05
Attention Heads of Large Language Models - A Survey,https://arxiv.org/abs/2409.03752,2024-09-05,2024-09-06
Distributed Inference and Fine-tuning of Large Language Models Over The Internet,https://arxiv.org/abs/2312.08361,2023-12-13,2024-09-02
Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer,https://arxiv.org/abs/2408.16978,2024-08-30,2024-09-02
Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering,https://arxiv.org/abs/2409.02426,2024-09-04,2024-09-05
"Attend First, Consolidate Later - On the Importance of Attention in Different LLM Layers",https://arxiv.org/abs/2409.03621,2024-09-05,2024-09-06
Pooling And Attention - What Are Effective Designs For LLm-Based Embedding Models?,https://arxiv.org/abs/2409.02727,2024-09-04,2024-09-05
LLaMA-Omni - Seamless Speech Interaction with Large Language Models,https://arxiv.org/abs/2409.06666,2024-09-10,2024-09-11
DA-MoE - Towards Dynamic Expert Allocation for Mixture-of-Experts Models,https://arxiv.org/abs/2409.06669,2024-09-10,2024-09-11
"Theory, Analysis, and Best Practices for Sigmoid Self-Attention",https://arxiv.org/abs/2409.04431,2024-09-06,2024-09-10
Implicit Chain of Thought Reasoning via Knowledge Distillation,https://arxiv.org/abs/2311.01460,2023-11-02,2024-09-09
BPE Gets Picky - Efficient Vocabulary Refinement During Tokenizer Training,https://arxiv.org/abs/2409.04599,2024-09-06,2024-09-10
Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,https://arxiv.org/abs/2409.04109,2024-09-06,2024-09-10
Breaking Neural Network Scaling Laws with Modularity,https://arxiv.org/abs/2409.05780,2024-09-10,2024-09-11
"Interpolation, Extrapolation, Hyperpolation - Generalising into new dimensions",https://arxiv.org/abs/2409.05513,2024-09-10,2024-09-11
"The AdEMAMix Optimizer - Better, Faster, Older",https://arxiv.org/abs/2409.03137,2024-09-05,2024-09-09
Adaptive Large Language Models By Layerwise Attention Shortcuts,https://arxiv.org/abs/2409.10870,2024-09-17,2024-09-18
Autoregressive + Chain of Thought (CoT) $\simeq$ Recurrent - Recurrence's Role in Language Models and a Revist of Recurrent Transformer,https://arxiv.org/abs/2409.09239,2024-09-16,2024-09-17
Rolling Diffusion Models,https://arxiv.org/abs/2402.09470v3,2024-02-12,2024-09-20
Kolmogorov-Arnold Transformer,https://arxiv.org/abs/2409.10594,2024-09-16,2024-09-20
"Language Models ""Grok"" to Copy",https://arxiv.org/abs/2409.09281,2024-09-16,2024-09-17
A mathematical framework of intelligence and consciousness based on Riemannian Geometry,https://arxiv.org/abs/2407.11024v3,2024-07-02,2024-09-13
Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors,https://arxiv.org/abs/2409.09905,2024-09-16,2024-09-17
Linear Recency Bias During Training Improves Transformers' Fit to Reading Times,https://arxiv.org/abs/2409.11250,2024-09-17,2024-09-18
Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization,https://arxiv.org/abs/2409.11212,2024-09-17,2024-09-18
Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking,https://arxiv.org/abs/2409.12059,2024-09-18,2024-09-19
Large Language Monkeys - Scaling Inference Compute with Repeated Sampling,https://arxiv.org/abs/2407.21787,2024-07-31,2024-08-02
Why Is Anything Conscious?,https://arxiv.org/abs/2409.14545,2024-09-25,2024-09-26
Scaling Smart - Accelerating Large Language Model Pre-training with Small Model Initialization,https://arxiv.org/abs/2409.12903,2024-09-25,2024-09-26
From Explicit CoT to Implicit CoT - Learning to Internalize CoT Step by Step,https://arxiv.org/abs/2405.14838,2024-05-23,2024-09-09
MoEUT - Mixture-of-Experts Universal Transformers,https://arxiv.org/abs/2405.16039,2024-05-25,2024-09-24
Self-attention as an attractor network - transient memories without backpropagation,https://arxiv.org/abs/2409.16112,2024-09-25,2024-09-26
Training Language Models to Self-Correct via Reinforcement Learning,https://arxiv.org/abs/2409.12917,2024-09-19,2024-09-23
Personalized Federated Learning via Backbone Self-Distillation,https://arxiv.org/abs/2409.15636,2024-09-25,2024-09-26
Characterizing stable regions in the residual stream of LLMs,https://arxiv.org/abs/2409.17113,2024-09-25,2024-09-26
Large Language Model Predicts Above Normal All India Summer Monsoon Rainfall in 2024,https://arxiv.org/abs/2409.16799,2024-09-25,2024-09-26
Is All Learning (Natural) Gradient Descent?,https://arxiv.org/abs/2409.16422,2024-09-25,2024-09-26
Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts,https://arxiv.org/abs/2409.16658,2024-09-25,2024-09-26
Lines of Thought in Large Language Models,https://arxiv.org/abs/2410.01545,2024-10-03,2024-10-05
nGPT - Normalized Transformer with Representation Learning on the Hypersphere,https://arxiv.org/abs/2410.01131,2024-10-03,2024-10-05
The Perfect Blend - Redefining RLHF with Mixture of Judges,https://arxiv.org/abs/2409.20370,2024-09-30,2024-10-01
Selective Attention Improves Transformer,https://arxiv.org/abs/2410.02703,2024-10-03,2024-10-05
ENTP - Encoder-only Next Token Prediction,https://arxiv.org/abs/2410.01600,2024-10-03,2024-10-05
U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,https://arxiv.org/abs/2410.01692,2024-10-03,2024-10-05
Diffusion Models are Evolutionary Algorithms,https://arxiv.org/abs/2410.02543,2024-10-03,2024-10-05
House of Cards - Massive Weights in LLMs,https://arxiv.org/abs/2410.01866,2024-10-03,2024-10-05
Intelligence at the Edge of Chaos,https://arxiv.org/abs/2410.02536,2024-10-03,2024-10-05
Geometric Signatures of Compositionality Across a Language Model's Lifetime,https://arxiv.org/abs/2410.01444,2024-10-03,2024-10-05
FAN - Fourier Analysis Networks,https://arxiv.org/abs/2410.02675,2024-10-03,2024-10-05
Round and Round We Go! What makes Rotary Positional Encodings useful?,https://arxiv.org/abs/2410.06205,2024-10-09,2024-10-10
"Blocks Architecture (BloArk) - Efficient, Cost-Effective, and Incremental Dataset Architecture for Wikipedia Revision History",https://arxiv.org/abs/2410.04410,2024-10-09,2024-10-10
Towards a Categorical Foundation of Deep Learning - A Survey,https://arxiv.org/abs/2410.05353,2024-10-09,2024-10-10
Differential Transformer,https://arxiv.org/abs/2410.05258,2024-10-09,2024-10-10
OpenDiLoCo - An Open-Source Framework for Globally Distributed Low-Communication Training,https://arxiv.org/abs/2407.07852,2024-07-10,2024-10-12
From Tokens to Words - on the inner lexicon of LLMs,https://arxiv.org/abs/2410.05864,2024-10-09,2024-10-10
Hyper-Connections,https://arxiv.org/abs/2409.19606v1,2024-09-29,2024-10-12
FusionLLM - A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression,https://arxiv.org/abs/2410.12707,2024-10-17,2024-10-19
Model Swarms - Collaborative Search to Adapt LLM Experts via Swarm Intelligence,https://arxiv.org/abs/2410.11163,2024-10-15,2024-10-17
One Step Diffusion via Shortcut Models,https://arxiv.org/abs/2410.12557,2024-10-17,2024-10-19
Cross-Dataset Generalization in Deep Learning,https://arxiv.org/abs/2410.11207,2024-10-15,2024-10-17
Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing,https://arxiv.org/abs/2410.11462,2024-10-15,2024-10-17
HART - Efficient Visual Generation with Hybrid Autoregressive Transformer,https://arxiv.org/abs/2410.10812,2024-10-14,2024-10-15
Role of Delay in Brain Dynamics,https://arxiv.org/abs/2410.11384,2024-10-15,2024-10-17
MoR - Mixture of Ranks for Low-Rank Adaptation Tuning,https://arxiv.org/abs/2410.13408,2024-10-17,2024-10-19
A Hitchhiker's Guide to Scaling Law Estimation,https://arxiv.org/abs/2410.11840,2024-10-15,2024-10-17
Thinking LLMs - General Instruction Following with Thought Generation,https://arxiv.org/abs/2410.10630,2024-10-14,2024-10-15
Diffusion Forcing - Next-token Prediction Meets Full-Sequence Diffusion,https://arxiv.org/abs/2407.01392,2024-07-01,2024-11-02
Beyond Autoregression - Discrete Diffusion for Complex Reasoning and Planning,https://arxiv.org/abs/2410.14157,2024-10-22,2024-10-24
Sparse Crosscoders for Cross-Layer Features and Model Diffing,https://transformer-circuits.pub/2024/crosscoders/index.html,2024-10-30,2024-10-30
Circuits Updates - August 2024,https://transformer-circuits.pub/2024/august-update/index.html,2024-10-30,2024-10-30
Looking Inward - Language Models Can Learn About Themselves by Introspection,https://arxiv.org/abs/2410.13787,2024-10-17,2024-10-19
MrT5 - Dynamic Token Merging for Efficient Byte-level Language Models,https://arxiv.org/abs/2410.20771,2024-10-31,2024-11-02
Using Dictionary Learning Features as Classifiers,https://transformer-circuits.pub/2024/features-as-classifiers/index.html,2024-10-30,2024-10-30
Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence,https://arxiv.org/abs/2410.17161,2024-10-22,2024-10-24
Decomposing The Dark Matter of Sparse Autoencoders,https://arxiv.org/abs/2410.14670,2024-10-22,2024-10-24
TokenFormer - Rethinking Transformer Scaling with Tokenized Model Parameters,https://arxiv.org/abs/2410.23168,2024-10-31,2024-11-01
Scaling Diffusion Language Models via Adaptation from Autoregressive Models,https://arxiv.org/abs/2410.17891,2024-10-24,2024-10-29
A Visual Case Study of the Training Dynamics in Neural Networks,https://arxiv.org/abs/2410.24050,2024-10-31,2024-11-01
Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition,https://arxiv.org/abs/2410.17765,2024-10-24,2024-10-29
Beyond position - how rotary embeddings shape representations and memory in autoregressive transfomers,https://arxiv.org/abs/2410.18067,2024-10-24,2024-10-29
Circuits Updates - September 2024,https://transformer-circuits.pub/2024/september-update/index.html,2024-10-30,2024-10-30
LayerSkip - Enabling Early Exit Inference and Self-Speculative Decoding,https://arxiv.org/pdf/2404.16710,2024-04-25,2024-10-30
Adversarial Training - A Survey,https://arxiv.org/abs/2410.15042,2024-10-22,2024-10-24
The Geometry of Concepts - Sparse Autoencoder Feature Structure,https://arxiv.org/abs/2410.19750,2024-10-10,2024-11-01
O1 Replication Journey - A Strategic Progress Report -- Part 1,https://arxiv.org/abs/2410.18982,2024-10-08,2024-10-30
Towards a Similarity-adjusted Surprisal Theory,https://arxiv.org/abs/2410.17676,2024-10-24,2024-10-29
A Survey of Small Language Models,https://arxiv.org/abs/2410.20011,2024-10-25,2024-11-07
Photon - Federated LLM Pre-Training,https://arxiv.org/abs/2411.02908,2024-11-06,2024-11-06
Hymba - A Hybrid-head Architecture for Small Language Models,https://arxiv.org/abs/2411.13676,2024-11-20,2024-12-05
TreeCoders - Trees of Transformers,https://arxiv.org/abs/2411.07218,2024-11-12,2024-11-13
DiffLoRA - Generating Personalized Low-Rank Adaptation Weights with Diffusion,https://arxiv.org/abs/2408.06740,2024-08-13,2024-12-05
Gini Coefficient as a Unified Metric for Evaluating Many-versus-Many Similarity in Vector Spaces,https://arxiv.org/abs/2411.07983,2024-11-12,2024-11-13
LAUREL - Learned Augmented Residual Layer,https://arxiv.org/abs/2411.07501,2024-11-12,2024-11-13
Comparative Analysis of Pooling Mechanisms in LLMs - A Sentiment Analysis Perspective,https://arxiv.org/abs/2411.14654,2024-11-22,2024-12-05
Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis,https://arxiv.org/abs/2411.07132,2024-11-12,2024-11-13
Bio-inspired AI - Integrating Biological Complexity into Artificial Intelligence,https://arxiv.org/abs/2411.15243,2024-11-22,2024-12-05
Enhancing Transformer Training Efficiency with Dynamic Dropout,https://arxiv.org/abs/2411.03236,2024-11-06,2024-11-06
Cut Your Losses in Large-Vocabulary Language Models,https://arxiv.org/abs/2411.09009,2024-11-14,2024-11-17
Attention Entropy is a Key Factor - An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models,https://arxiv.org/abs/2412.16545,2024-12-24,2024-12-26
Memory Layers at Scale,https://arxiv.org/abs/2412.09764,2024-12-19,2024-12-20
You Only Cache Once - Decoder-Decoder Architectures for Language Models,https://arxiv.org/abs/2405.05254,2024-05-08,2024-12-23
Causal Diffusion Transformers for Generative Modeling,https://www.arxiv.org/abs/2412.12095,2024-12-16,2024-12-20
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-11
Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat,https://arxiv.org/abs/2412.14205,2024-12-19,2024-12-20
Experience of Training a 1.7B-Parameter LLaMa Model From Scratch,https://arxiv.org/abs/2412.13335,2024-12-19,2024-12-20
Byte Latent Transformer - Patches Scale Better Than Tokens,https://arxiv.org/abs/2412.09871,2024-12-19,2024-12-20
A Survey of RWKV,https://arxiv.org/abs/2412.14847,2024-12-19,2024-12-20
The Hyperfitting Phenomenon - Sharpening and Stabilizing LLMs for Open-Ended Text Generation,https://arxiv.org/abs/2412.04318,2024-12-06,2024-12-09
Does Self-Attention Need Separate Weights in Transformers?,https://arxiv.org/abs/2412.00359,2024-12-06,2024-12-09
Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations,https://arxiv.org/abs/2408.10920,2024-08-20,2024-08-22
Hierarchical VAE with a Diffusion-based VampPrior,https://arxiv.org/abs/2412.01373,2024-12-06,2024-12-09
Introduction to Graph Neural Networks - A Starting Point for Machine Learning Engineers,https://arxiv.org/abs/2412.19419,2024-12-30,2025-01-01
Not All Language Model Features Are Linear,https://arxiv.org/abs/2405.14860,2024-05-23,2024-12-11
Flow Matching Guide and Code,https://www.arxiv.org/abs/2412.06264,2024-12-09,2024-12-20
Visual Autoregressive Modeling - Scalable Image Generation via Next-Scale Prediction,https://arxiv.org/abs/2404.02905,2024-04-03,2024-12-11
Multimodal Latent Language Modeling with Next-Token Diffusion,https://www.arxiv.org/abs/2412.08635,2024-12-11,2024-12-20
Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://arxiv.org/abs/1503.00075,2015-02-28,2024-12-11
Reinforcement Learning - An Overview,https://arxiv.org/abs/2412.05265,2024-12-06,2024-12-09
Concept Boundary Vectors,https://arxiv.org/abs/2412.15698,2024-12-24,2024-12-26
FlashAttention on a Napkin - A Diagrammatic Approach to Deep Learning IO-Awareness,https://arxiv.org/abs/2412.03317,2024-12-06,2024-12-09
Large Concept Models - Language Modeling in a Sentence Representation Space,https://arxiv.org/abs/2412.08821,2024-12-11,2024-12-18
Repository Structure-Aware Training Makes SLMs Better Issue Resolver,https://arxiv.org/abs/2412.19031,2024-12-30,2025-01-01
Training Large Language Models to Reason in a Continuous Latent Space,https://www.arxiv.org/abs/2412.06769,2024-12-09,2024-12-20
