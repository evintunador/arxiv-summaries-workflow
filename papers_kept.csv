Title,ArXiv Link,Paper Date,Date Added
3D-RPE - Enhancing Long-Context Modeling Through 3D Rotary Position Encoding,https://arxiv.org/abs/2406.09897,2024-06-18,2024-06-19
An elementary proof of a universal approximation theorem,https://arxiv.org/abs/2406.10002,2024-06-18,2024-06-19
Breaking the Attention Bottleneck,https://arxiv.org/abs/2406.10906,2024-06-18,2024-06-19
In Tree Structure Should Sentence Be Generated,https://arxiv.org/abs/2406.14189,2024-06-20,2024-06-21
Transcendence - Generative Models Can Outperform The Experts That Train Them,https://arxiv.org/abs/2406.11741,2024-06-18,2024-06-19
"Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",https://arxiv.org/abs/2406.10209,2024-06-18,2024-06-19
Can LLMs Learn Macroeconomic Narratives from Social Media?,https://arxiv.org/abs/2406.12109,2024-06-18,2024-06-19
Amphista - Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style,https://arxiv.org/abs/2406.13170,2024-06-20,2024-06-21
Provable Guarantees for Model Performance via Mechanistic Interpretability,https://arxiv.org/abs/2406.11779,2024-06-18,2024-06-19
Locating and Extracting Relational Concepts in Large Language Models,https://arxiv.org/abs/2406.13184,2024-06-20,2024-06-21
Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,https://arxiv.org/abs/2406.07394,2024-06-11,2024-06-19
In-Context Former - Lightning-fast Compressing Context for Large Language Model,https://arxiv.org/abs/2406.13618,2024-06-20,2024-06-21
Complex fractal trainability boundary can arise from trivial non-convexity,https://arxiv.org/abs/2406.13971,2024-06-20,2024-06-21
Distributional reasoning in LLMs - Parallel reasoning processes in multi-hop reasoning,https://arxiv.org/abs/2406.13858,2024-06-20,2024-06-21
Mixture-of-Agents Enhances Large Language Model Capabilities,https://arxiv.org/abs/2406.04692,2024-06-07,2024-06-20
Optimised Grouped-Query Attention Mechanism for Transformers,https://arxiv.org/abs/2406.14963,2024-06-24,2024-06-25
OTCE - Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser,https://arxiv.org/abs/2406.16495,2024-06-24,2024-06-25
The Responsible Foundation Model Development Cheatsheet - A Review of Tools & Resources,https://arxiv.org/abs/2406.16746,2024-06-24,2024-06-25
Emergence of Hidden Capabilities - Exploring Learning Dynamics in Concept Space,https://arxiv.org/abs/2406.19370,2024-06-27,2024-06-28
Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network,https://arxiv.org/abs/2406.15109,2024-06-24,2024-06-25
Understanding and Mitigating Tokenization Bias in Language Models,https://arxiv.org/abs/2406.16829,2024-06-24,2024-06-25
MoA - Mixture of Sparse Attention for Automatic Large Language Model Compression,https://arxiv.org/abs/2406.14909,2024-06-24,2024-06-25
The Hidden Pitfalls of the Cosine Similarity Loss,https://arxiv.org/abs/2406.16468,2024-06-24,2024-06-25
Finding Transformer Circuits with Edge Pruning,https://arxiv.org/abs/2406.16778,2024-06-24,2024-06-25
A Closer Look into Mixture-of-Experts in Large Language Models,https://arxiv.org/abs/2406.18219,2024-06-26,2024-06-27
Single Parent Family - A Spectrum of Family Members from a Single Pre-Trained Foundation Model,https://arxiv.org/abs/2406.19995,2024-06-28,2024-07-02
Universal Length Generalization with Turing Programs,https://arxiv.org/abs/2407.03310v1,2024-07-03,2024-07-04
Large Language Model Enhanced Knowledge Representation Learning - A Survey,https://arxiv.org/abs/2407.00936,2024-07-02,2024-07-03
Universal Approximation Theory - The basic theory for large language models,https://arxiv.org/abs/2407.00958,2024-07-02,2024-07-03
Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing,https://arxiv.org/abs/2407.03185,2024-07-03,2024-07-04
On the Anatomy of Attention,https://arxiv.org/abs/2407.02423,2024-07-02,2024-07-03
A Review of the Applications of Deep Learning-Based Emergent Communication,https://arxiv.org/abs/2407.03302,2024-07-03,2024-07-04
Hypformer - Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space,https://arxiv.org/abs/2407.01290,2024-07-02,2024-07-03
Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models - Enhancing Performance and Reducing Inference Costs,https://arxiv.org/abs/2407.00945,2024-07-02,2024-07-03
Do language models plan ahead for future tokens -,https://arxiv.org/abs/2404.00859,2024-04-01,2024-07-04
Diffusion Models and Representation Learning - A Survey,https://arxiv.org/abs/2407.00783,2024-07-02,2024-07-03
Predicting vs. Acting - A Trade-off Between World Modeling & Agent Modeling,https://arxiv.org/abs/2407.02446,2024-07-02,2024-07-03
Min P Sampling - Balancing Creativity and Coherence at High Temperature,https://arxiv.org/abs/2407.01082,2024-07-02,2024-07-03
Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs,https://arxiv.org/abs/2406.20086,2024-06-28,2024-07-02
Efficient Training of Language Models with Compact and Consistent Next Token Distributions,https://arxiv.org/abs/2407.02819,2024-07-03,2024-07-04
How Does Overparameterization Affect Features?,https://arxiv.org/abs/2407.00968,2024-07-02,2024-07-03
LLM Internal States Reveal Hallucination Risk Faced With a Query,https://arxiv.org/abs/2407.03282,2024-07-03,2024-07-04
Large Language Models Assume People are More Rational than We Really are,https://arxiv.org/abs/2406.17055,2024-06-25,2024-06-27
MobileLLM - Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,https://arxiv.org/pdf/2402.14905,2024-02-22,2024-07-11
Predictive Coding Networks and Inference Learning - Tutorial and Survey,https://arxiv.org/abs/2407.04117,2024-07-09,2024-07-10
Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial General Intelligence,https://arxiv.org/abs/2407.03652,2024-07-09,2024-07-10
SoftDedup - an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training,https://arxiv.org/abs/2407.06654,2024-07-09,2024-07-10
Anthropocentric bias and the possibility of artificial cognition,https://arxiv.org/abs/2407.03859,2024-07-09,2024-07-10
Mixture of A Million Experts,https://arxiv.org/abs/2407.04153,2024-07-09,2024-07-10
Improving Self Consistency in LLMs through Probabilistic Tokenization,https://arxiv.org/abs/2407.03678,2024-07-09,2024-07-10
Internet of Agents - Weaving a Web of Heterogeneous Agents for Collaborative Intelligence,https://arxiv.org/abs/2407.07061,2024-07-09,2024-07-10
A Theory of Machine Learning,https://arxiv.org/abs/2407.05520,2024-07-09,2024-07-10
Introducing 'Inside' Out of Distribution,https://arxiv.org/abs/2407.04534,2024-07-09,2024-07-10
A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training,https://arxiv.org/abs/2407.07630,2024-07-10,2024-07-11
When LLMs Play the Telephone Game - Cumulative Changes and Attractors in Iterated Cultural Transmissions,https://arxiv.org/abs/2407.04503,2024-07-09,2024-07-10
Enhancing learning in artificial neural networks through cellular heterogeneity and neuromodulatory signaling,https://arxiv.org/abs/2407.04525,2024-07-09,2024-07-10
Learning to (Learn at Test Time) - RNNs with Expressive Hidden States,https://arxiv.org/abs/2407.04620,2024-07-05,2024-07-12
Surpassing Cosine Similarity for Multidimensional Comparisons - Dimension Insensitive Euclidean Metric (DIEM),https://arxiv.org/abs/2407.08623,2024-07-11,2024-07-12
Collective Innovation in Groups of Large Language Models,https://arxiv.org/abs/2407.05377,2024-07-09,2024-07-10
E5-V - Universal Embeddings with Multimodal Large Language Models,https://arxiv.org/abs/2407.12580,2024-07-18,2024-07-19
Beyond KV Caching - Shared Attention for Efficient LLMs,https://arxiv.org/abs/2407.12866,2024-07-18,2024-07-21
SpreadsheetLLM - Encoding Spreadsheets for Large Language Models,https://arxiv.org/abs/2407.09025,2024-07-18,2024-07-21
The Foundations of Tokenization - Statistical and Computational Concerns,https://arxiv.org/abs/2407.11606,2024-07-18,2024-07-20
Graph Transformers - A Survey,https://arxiv.org/abs/2407.09777,2024-07-18,2024-07-21
Human-like Episodic Memory for Infinite Context LLMs,https://arxiv.org/abs/2407.09450,2024-07-12,2024-07-15
Vectoring Languages,https://arxiv.org/abs/2407.11766,2024-07-18,2024-07-20
MaskMoE - Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts,https://arxiv.org/abs/2407.09816,2024-07-18,2024-07-21
A Comprehensive Review of Recommender Systems - Transitioning from Theory to Practice,https://arxiv.org/abs/2407.13699,2024-07-18,2024-07-19
A Survey on Universal Approximation Theorems,https://arxiv.org/abs/2407.12895,2024-07-18,2024-07-19
Future Lens - Anticipating Subsequent Tokens from a Single Hidden State,https://arxiv.org/abs/2311.04897,2023-11-08,2024-07-16
Information-Theoretic Foundations for Machine Learning,https://arxiv.org/abs/2407.12288,2024-07-18,2024-07-19
Flash normalization - fast RMSNorm for LLMs,https://arxiv.org/abs/2407.09577,2024-07-18,2024-07-21
Efficient Visual Transformer by Learnable Token Merging,https://arxiv.org/abs/2407.15219,2024-07-23,2024-07-26
Deep Learning for Economists,https://arxiv.org/abs/2407.15339,2024-07-23,2024-07-26
Matryoshka Diffusion Models,https://arxiv.org/abs/2310.15111,2023-10-23,2024-07-26
Vectoring Languages,https://arxiv.org/abs/2407.11766,2024-07-18,2024-07-20
On the Benefits of Rank in Attention Layers,https://arxiv.org/abs/2407.16153,2024-07-23,2024-07-25
Demystifying Verbatim Memorization in Large Language Models,https://arxiv.org/abs/2407.17817,2024-07-25,2024-07-26
"Farewell to Length Extrapolation, a Training-Free Infinite Context with Finite Attention Scope",https://arxiv.org/abs/2407.15176,2024-07-23,2024-07-26
Dissecting Multiplication in Transformers - Insights into LLMs,https://arxiv.org/abs/2407.15360,2024-07-23,2024-07-26
Keep the Cost Down - A Review on Methods to Optimize LLM' s KV-Cache Consumption,https://arxiv.org/abs/2407.18003,2024-07-25,2024-07-26
Shared Imagination - LLMs Hallucinate Alike,https://arxiv.org/abs/2407.16604,2024-07-23,2024-07-25
Investigating learning-independent abstract reasoning in artificial neural networks,https://arxiv.org/abs/2407.17791,2024-07-25,2024-07-26
Learning mental states estimation through self-observation - a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind,https://arxiv.org/abs/2407.18022,2024-07-25,2024-07-26
"A Comprehensive Survey of LLM Alignment Techniques - RLHF, RLAIF, PPO, DPO and More",https://arxiv.org/abs/2407.16216,2024-07-23,2024-07-25
PMoE - Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning,https://arxiv.org/abs/2407.21571,2024-07-31,2024-08-01
Deep Companion Learning - Enhancing Generalization Through Historical Consistency,https://arxiv.org/abs/2407.18821,2024-07-29,2024-07-30
Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens,https://arxiv.org/abs/2407.21248,2024-07-31,2024-08-01
Machine Learning for predicting chaotic systems,https://arxiv.org/abs/2407.20158,2024-07-29,2024-07-30
Universal Approximation Theory - Foundations for Parallelism in Neural Networks,https://arxiv.org/abs/2407.21670,2024-07-31,2024-08-01
Mixture of Nested Experts - Adaptive Processing of Visual Tokens,https://arxiv.org/abs/2407.19985,2024-07-29,2024-07-30
Big Cooperative Learning,https://arxiv.org/abs/2407.21319,2024-07-31,2024-08-01
An introduction to reinforcement learning for neuroscience,https://arxiv.org/abs/2311.07315v2,2023-11-13,2024-08-02
MoMa - Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts,https://arxiv.org/abs/2407.21770,2024-07-31,2024-08-01
Social Learning through Interactions with Other Agents - A Survey,https://arxiv.org/abs/2407.21713,2024-07-31,2024-08-01
Comprehensive Survey of Complex-Valued Neural Networks - Insights into Backpropagation and Activation Functions,https://arxiv.org/abs/2407.19258,2024-07-29,2024-07-30
Supertrust - Evolution-based superalignment strategy for safe coexistence,https://arxiv.org/abs/2407.20208,2024-07-29,2024-07-30
"Entropy, Thermodynamics and the Geometrization of the Language Model",https://arxiv.org/abs/2407.21092,2024-07-31,2024-08-01
Do Language Models Have a Critical Period for Language Acquisition -,https://arxiv.org/abs/2407.19325v1,2024-07-27,2024-08-02
Exploring Loss Landscapes through the Lens of Spin Glass Theory,https://arxiv.org/abs/2407.20724,2024-07-31,2024-08-01
The Llama 3 Herd of Models,https://arxiv.org/abs/2407.21783,2024-07-31,2024-08-01
Diffusion Guided Language Modeling,https://arxiv.org/abs/2408.04220,2024-08-08,2024-08-09
POA - Pre-training Once for Models of All Sizes,https://arxiv.org/abs/2408.01031,2024-08-02,2024-08-09
Language Model Can Listen While Speaking,https://arxiv.org/abs/2408.02622,2024-08-05,2024-08-06
Unexpected Benefits of Self-Modeling in Neural Systems,https://arxiv.org/abs/2407.10188,2024-07-14,2024-08-05
Climbing the Complexity Ladder with Expressive Attention,https://arxiv.org/abs/2407.18601v1,2024-07-26,2024-08-09
Mixture-of-Depths - Dynamically allocating compute in transformer-based language models,https://arxiv.org/abs/2404.02258,2024-04-02,2024-08-09
What comes after transformers? -- A selective survey connecting ideas in deep learning,https://arxiv.org/abs/2408.00386,2024-08-02,2024-08-05
Gemma 2 - Improving Open Language Models at a Practical Size,https://arxiv.org/abs/2408.00118,2024-07-31,2024-08-05
Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models,https://arxiv.org/abs/2408.01308,2024-08-02,2024-08-05
"SentenceVAE - Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models",https://arxiv.org/abs/2408.00655,2024-08-02,2024-08-05
OpenEP - Open-Ended Future Event Prediction,https://arxiv.org/abs/2408.06578,2024-08-13,2024-08-14
Liquid Time-constant Networks,https://arxiv.org/abs/2006.04439,2020-06-08,2024-08-13
The Clever Hans Effect in Unsupervised Learning,https://arxiv.org/abs/2408.08041,2024-08-15,2024-08-16
Capturing the Complexity of Human Strategic Decision-Making with Machine Learning,https://arxiv.org/abs/2408.07865,2024-08-15,2024-08-16
Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning,https://arxiv.org/abs/2408.00690v2,2024-08-01,2024-08-16
"A Single Goal is All You Need - Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals",https://arxiv.org/abs/2408.05804,2024-08-12,2024-08-13
Can Turing machine be curious about its Turing test results - Three informal lectures on physics of intelligence,https://arxiv.org/abs/1606.08109,2016-06-27,2024-08-12
"Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks",https://arxiv.org/abs/2408.04965,2024-08-10,2024-08-12
The ShareLM Collection and Plugin - Contributing Human-Model Chats for the Benefit of the Community,https://arxiv.org/abs/2408.08291,2024-08-15,2024-08-16
Liquid Structural State-Space Models,https://arxiv.org/abs/2209.12951,2022-09-26,2024-08-14
BAM! Just Like That - Simple and Efficient Parameter Upcycling for Mixture of Experts,https://arxiv.org/abs/2408.08274,2024-08-15,2024-08-16
Patch-Level Training for Large Language Models,https://arxiv.org/abs/2407.12665,2024-07-17,2024-08-16
The Physics of Learning - From Autoencoders to Truly Autonomous Learning Machines,https://arxiv.org/abs/2407.04700,2024-02-12,2024-08-12
ReST-MCTS - - LLM Self-Training via Process Reward Guided Tree Search,https://arxiv.org/abs/2406.03816,2024-06-06,2024-08-12
Layerwise Recurrent Router for Mixture-of-Experts,https://arxiv.org/abs/2408.06793,2024-08-13,2024-08-14
Understanding Learning through the Lens of Dynamical Invariants,https://arxiv.org/abs/2401.10428,2024-01-19,2024-08-12
Reciprocal Learning,https://arxiv.org/abs/2408.06257,2024-08-12,2024-08-13
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-11
Tree Attention - Topology-aware Decoding for Long-Context Attention on GPU clusters,https://arxiv.org/abs/2408.04093,2024-08-07,2024-08-11
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-08-23
Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations,https://arxiv.org/abs/2408.10920,2024-08-20,2024-08-22
From pixels to planning - scale-free active inference,https://arxiv.org/abs/2407.20292,2024-07-27,2024-08-07
Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model,https://arxiv.org/abs/2408.11039,2024-08-20,2024-08-21
Beyond English-Centric LLMs - What Language Do Multilingual Language Models Think in?,https://arxiv.org/abs/2408.10811,2024-08-20,2024-08-22
Scaling Laws with Vocabulary - Larger Models Deserve Larger Vocabularies,https://arxiv.org/abs/2407.13623,2024-07-18,2024-08-23
Machine Learning with Physics Knowledge for Prediction - A Survey,https://arxiv.org/abs/2408.09840,2024-08-20,2024-08-22
Scaling Law with Learning Rate Annealing,https://arxiv.org/abs/2408.11029,2024-08-20,2024-08-21
The Exploration-Exploitation Dilemma Revisited - An Entropy Perspective,https://arxiv.org/abs/2408.09974,2024-08-20,2024-08-22
HMoE - Heterogeneous Mixture of Experts for Language Modeling,https://arxiv.org/abs/2408.10681,2024-08-20,2024-08-22
Learning Randomized Algorithms with Transformers,https://arxiv.org/abs/2408.10818,2024-08-20,2024-08-22
SMILE - Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models,https://arxiv.org/abs/2408.10174,2024-08-20,2024-08-22
KAN 2.0 - Kolmogorov-Arnold Networks Meet Science,https://arxiv.org/abs/2408.10205,2024-08-19,2024-08-20
EPO - Hierarchical LLM Agents with Environment Preference Optimization,https://arxiv.org/abs/2408.16090,2024-08-29,2024-08-31
An Overview on Machine Learning Methods for Partial Differential Equations - from Physics Informed Neural Networks to Deep Operator Learning,https://arxiv.org/abs/2408.13222,2024-08-23,2024-08-31
Category-Theoretical and Topos-Theoretical Frameworks in Machine Learning - A Survey,https://arxiv.org/abs/2408.14014,2024-08-27,2024-08-30
Dolphin - Long Context as a New Modality for Energy-Efficient On-Device Language Models,https://arxiv.org/abs/2408.15518,2024-08-29,2024-08-31
Diffusion Models Are Real-Time Game Engines,https://arxiv.org/abs/2408.14837,2024-08-27,2024-08-29
Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts,https://arxiv.org/abs/2408.15664,2024-08-29,2024-08-31
Self-Improving Diffusion Models with Synthetic Data,https://arxiv.org/abs/2408.16333,2024-08-29,2024-08-30
In-Context Learning with Representations - Contextual Generalization of Trained Transformers,https://arxiv.org/abs/2408.10147v1,2024-08-19,2024-08-31
A Law of Next-Token Prediction in Large Language Models,https://arxiv.org/abs/2408.13442,2024-08-27,2024-08-30
Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need,https://arxiv.org/abs/2408.15997,2024-08-29,2024-08-31
Federated Learning - A Cutting-Edge Survey of the Latest Advancements and Applications,https://arxiv.org/abs/2310.05269,2023-10-08,2024-09-02
How does the brain compute with probabilities?,https://arxiv.org/abs/2409.02709,2024-09-04,2024-09-05
Quantifying Emergence in Neural Networks - Insights from Pruning and Training Dynamics,https://arxiv.org/abs/2409.01568,2024-09-04,2024-09-05
Learning (With) Distributed Optimization,https://arxiv.org/abs/2308.05548,2023-08-10,2024-09-02
Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,https://arxiv.org/abs/2310.16834,2023-10-25,2024-08-23
Adversarial Training Using Feedback Loops,https://arxiv.org/abs/2308.11881,2023-08-23,2024-09-02
The Unbearable Slowness of Being,https://www.arxiv.org/abs/2408.10234v1,2024-08-03,2024-09-05
A Survey on Emergent Language,https://arxiv.org/abs/2409.02645,2024-09-04,2024-09-05
Dreaming is All You Need,https://arxiv.org/abs/2409.01633,2024-09-04,2024-09-05
Configurable Foundation Models - Building LLMs from a Modular Perspective,https://arxiv.org/abs/2409.02877,2024-09-04,2024-09-05
Neural timescales from a computational perspective,https://arxiv.org/abs/2409.02684,2024-09-04,2024-09-05
Scaling Laws for Economic Productivity - Experimental Evidence in LLM-Assisted Translation,https://arxiv.org/abs/2409.02391,2024-09-04,2024-09-05
Attention Heads of Large Language Models - A Survey,https://arxiv.org/abs/2409.03752,2024-09-05,2024-09-06
Distributed Inference and Fine-tuning of Large Language Models Over The Internet,https://arxiv.org/abs/2312.08361,2023-12-13,2024-09-02
Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer,https://arxiv.org/abs/2408.16978,2024-08-30,2024-09-02
Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering,https://arxiv.org/abs/2409.02426,2024-09-04,2024-09-05
"Attend First, Consolidate Later - On the Importance of Attention in Different LLM Layers",https://arxiv.org/abs/2409.03621,2024-09-05,2024-09-06
Pooling And Attention - What Are Effective Designs For LLm-Based Embedding Models?,https://arxiv.org/abs/2409.02727,2024-09-04,2024-09-05
LLaMA-Omni - Seamless Speech Interaction with Large Language Models,https://arxiv.org/abs/2409.06666,2024-09-10,2024-09-11
DA-MoE - Towards Dynamic Expert Allocation for Mixture-of-Experts Models,https://arxiv.org/abs/2409.06669,2024-09-10,2024-09-11
"Theory, Analysis, and Best Practices for Sigmoid Self-Attention",https://arxiv.org/abs/2409.04431,2024-09-06,2024-09-10
Implicit Chain of Thought Reasoning via Knowledge Distillation,https://arxiv.org/abs/2311.01460,2023-11-02,2024-09-09
BPE Gets Picky - Efficient Vocabulary Refinement During Tokenizer Training,https://arxiv.org/abs/2409.04599,2024-09-06,2024-09-10
Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,https://arxiv.org/abs/2409.04109,2024-09-06,2024-09-10
Breaking Neural Network Scaling Laws with Modularity,https://arxiv.org/abs/2409.05780,2024-09-10,2024-09-11
"Interpolation, Extrapolation, Hyperpolation - Generalising into new dimensions",https://arxiv.org/abs/2409.05513,2024-09-10,2024-09-11
"The AdEMAMix Optimizer - Better, Faster, Older",https://arxiv.org/abs/2409.03137,2024-09-05,2024-09-09
Adaptive Large Language Models By Layerwise Attention Shortcuts,https://arxiv.org/abs/2409.10870,2024-09-17,2024-09-18
Autoregressive + Chain of Thought (CoT) $\simeq$ Recurrent - Recurrence's Role in Language Models and a Revist of Recurrent Transformer,https://arxiv.org/abs/2409.09239,2024-09-16,2024-09-17
Rolling Diffusion Models,https://arxiv.org/abs/2402.09470v3,2024-02-12,2024-09-20
Kolmogorov-Arnold Transformer,https://arxiv.org/abs/2409.10594,2024-09-16,2024-09-20
"Language Models ""Grok"" to Copy",https://arxiv.org/abs/2409.09281,2024-09-16,2024-09-17
A mathematical framework of intelligence and consciousness based on Riemannian Geometry,https://arxiv.org/abs/2407.11024v3,2024-07-02,2024-09-13
Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors,https://arxiv.org/abs/2409.09905,2024-09-16,2024-09-17
Linear Recency Bias During Training Improves Transformers' Fit to Reading Times,https://arxiv.org/abs/2409.11250,2024-09-17,2024-09-18
Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization,https://arxiv.org/abs/2409.11212,2024-09-17,2024-09-18
Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking,https://arxiv.org/abs/2409.12059,2024-09-18,2024-09-19
Large Language Monkeys - Scaling Inference Compute with Repeated Sampling,https://arxiv.org/abs/2407.21787,2024-07-31,2024-08-02
Why Is Anything Conscious?,https://arxiv.org/abs/2409.14545,2024-09-25,2024-09-26
Scaling Smart - Accelerating Large Language Model Pre-training with Small Model Initialization,https://arxiv.org/abs/2409.12903,2024-09-25,2024-09-26
From Explicit CoT to Implicit CoT - Learning to Internalize CoT Step by Step,https://arxiv.org/abs/2405.14838,2024-05-23,2024-09-09
MoEUT - Mixture-of-Experts Universal Transformers,https://arxiv.org/abs/2405.16039,2024-05-25,2024-09-24
Self-attention as an attractor network - transient memories without backpropagation,https://arxiv.org/abs/2409.16112,2024-09-25,2024-09-26
Training Language Models to Self-Correct via Reinforcement Learning,https://arxiv.org/abs/2409.12917,2024-09-19,2024-09-23
Personalized Federated Learning via Backbone Self-Distillation,https://arxiv.org/abs/2409.15636,2024-09-25,2024-09-26
Characterizing stable regions in the residual stream of LLMs,https://arxiv.org/abs/2409.17113,2024-09-25,2024-09-26
Large Language Model Predicts Above Normal All India Summer Monsoon Rainfall in 2024,https://arxiv.org/abs/2409.16799,2024-09-25,2024-09-26
Is All Learning (Natural) Gradient Descent?,https://arxiv.org/abs/2409.16422,2024-09-25,2024-09-26
Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts,https://arxiv.org/abs/2409.16658,2024-09-25,2024-09-26
